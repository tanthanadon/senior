import sys import os import pandas as pd import numpy as np from jaratoolbox import celldatabase reload ( celldatabase ) from jaratoolbox import ephyscore from jaratoolbox import spikesorting from jaratoolbox import spikesanalysis from jaratoolbox import behavioranalysis from jaratoolbox import settings reload ( settings ) from scipy import stats AVERAGE_JITTER = { <str> : 0.0093 , <str> : 0.0094 , <str> : 0.0095 , <str> : 0.0091 } def get_sound_onset_times ( ephysData , sessionType ) :      eventOnsetTimes = ephysData [ <str> ] [ <str> ] if len ( eventOnsetTimes ) == 0 :          eventOnsetTimes = ephysData [ <str> ] [ <str> ] + AVERAGE_JITTER [ sessionType ]  eventOnsetTimes = spikesanalysis . minimum_event_onset_diff ( eventOnsetTimes , minEventOnsetDiff = 0.2 ) return eventOnsetTimes  def stim_response ( ephysData , baseRange = [ - 0.05 , - 0.04 ] , responseRange = [ 0.0 , 0.01 ] , stimType = <str> ) :      fullTimeRange = [ baseRange [ 0 ] , responseRange [ 1 ] ] if stimType == <str> :          eventOnsetTimes = ephysData [ <str> ] [ <str> ]  elif stimType == <str> :          eventOnsetTimes = get_sound_onset_times ( ephysData , <str> )  spikeTimestamps = ephysData [ <str> ] spikeTimesFromEventOnset , trialIndexForEachSpike , indexLimitsEachTrial = spikesanalysis . eventlocked_spiketimes ( spikeTimestamps , eventOnsetTimes , fullTimeRange ) baseSpikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , baseRange ) laserSpikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , responseRange ) [ testStatistic , pVal ] = stats . ranksums ( laserSpikeCountMat , baseSpikeCountMat ) return testStatistic , pVal  def best_window_freq_tuning ( spikeTimesFromEventOnset , indexLimitsEachTrial , trialsEachFreq , windowsToTry = [ [ 0.0 , 0.1 ] , [ 0.0 , 0.05 ] , [ 0.1 , 0.15 ] ] ) :      zscores = np . zeros ( ( len ( windowsToTry ) , trialsEachFreq . shape [ 1 ] ) ) for ind , window in enumerate ( windowsToTry ) :          duration = window [ 1 ] - window [ 0 ] baseTimeRange = [ - 0.1 - duration , - 0.1 ] spikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , window ) baseSpikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , baseTimeRange ) for ind2 in range ( len ( numFreqs ) ) :              trialsThisFreq = trialsEachFreq [ : , ind2 ] spikeCountsThisFreq = spikeCountMat [ trialsThisFreq ] baseCountsThisFreq = baseSpikeCountMat [ trialsThisFreq ] zScore , pVal = stats . ranksums ( spikeCountsThisFreq , baseCountsThisFreq ) zscores [ ind , ind2 ] = zScore   maxInd = np . unravel_index ( zscores . argmax ( ) , zscores . shape ) windowToUse = windowsToTry [ maxInd [ 0 ] ] return windowToUse  def gaussian_tuning_fit ( stimArray , responseArray ) :      from scipy . optimize import curve_fit try :          maxInd = np . argmax ( responseArray ) p0 = [ stimArray [ maxInd ] , responseArray [ maxInd ] , 1. , 0. ] curveFit = curve_fit ( gaussian , stimArray , responseArray , p0 = p0 , maxfev = 10000 ) [ 0 ]  except RuntimeError :          print <str> . format ( type ) return None , None  fitResponseArray = gaussian ( stimArray , curveFit [ 0 ] , curveFit [ 1 ] , curveFit [ 2 ] , curveFit [ 3 ] ) residuals = responseArray - fitResponseArray SSresidual = np . sum ( residuals ** 2 ) SStotal = np . sum ( ( responseArray - np . mean ( responseArray ) ) ** 2 ) Rsquared = 1 - ( SSresidual / SStotal ) return curveFit , Rsquared  def gaussian ( x , mu , amp , sigma , offset ) :      return offset + amp * np . exp ( - ( ( x - mu ) / sigma ) ** 2 )  def best_index ( cellObj , bestFreq , behavType = <str> ) :      behavIndex = cellObj . get_session_inds ( behavType ) charFreqs = [ ] for ind in behavIndex :          bdata = cellObj . load_behavior_by_index ( ind ) charFreq = np . unique ( bdata [ <str> ] ) [ 0 ] charFreqs . append ( charFreq )  if bestFreq is not None and len ( charFreqs ) > 0 :          octaveDiff = np . zeros ( len ( charFreqs ) ) for ind , charFreq in enumerate ( charFreqs ) :              octaveDiff [ ind ] = np . log2 ( bestFreq / charFreq )  octaveDiff = np . abs ( octaveDiff ) bestBehavIndex = behavIndex [ np . argmin ( octaveDiff ) ] octavesFromBest = min ( octaveDiff )  else :          bestBehavIndex = None octavesFromBest = None  return bestBehavIndex , octavesFromBest  if __name__ == <str> :      if len ( sys . argv [ 1 : ] ) :          subjects = sys . argv [ 1 : ]  else :          subjects = subjects_info . PV_ARCHT_MICE + subjects_info . SOM_ARCHT_MICE  for subject in subjects :          inforec = <str> . format ( subject ) db = celldatabase . generate_cell_database ( inforec ) dbFilenameNew = <str> . format ( subject ) celldatabase . save_hdf ( db , dbFilenameNew ) print ( <str> . format ( dbFilenameNew ) )  allSubjects = [ ] fulldb = pd . DataFrame ( ) for subject in allSubjects :          db = celldatabase . load_hdf ( <str> . format ( subject ) ) fulldb = fulldb . append ( db , ignore_index = True )  fulldbFilename = os . path . join ( settings . DATABASE_PATH , <str> ) celldatabase . save_hdf ( fulldb , fulldbFilename )     