 import log from log import debug , print_tb import feedparser , filesystem from bs4 import BeautifulSoup import urllib2 import json , os from settings import Settings from base import * from movieapi import * from nfowriter import * from strmwriter import * def real_url ( url ) : 	 import urlparse res = urlparse . urlparse ( url ) res = urlparse . ParseResult ( <str> , <str> , res . path , res . params , res . query , res . fragment ) res = urlparse . urlunparse ( res ) debug ( <str> % ( url , res ) ) return res  def origin_url ( url ) : 	 import urlparse res = urlparse . urlparse ( url ) res = urlparse . ParseResult ( <str> , <str> , res . path , res . params , res . query , res . fragment ) res = urlparse . urlunparse ( res ) debug ( <str> % ( url , res ) ) return res  class DescriptionParser ( DescriptionParserBase ) : 	 def __init__ ( self , full_title , content , link , settings , imdb = None ) : 		 self . _link = link DescriptionParserBase . __init__ ( self , full_title , content , settings ) if imdb : 			 self . _dict [ <str> ] = imdb   def link ( self ) : 		 return origin_url ( self . _link )  def get_tag ( self , x ) : 		 return { <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> } . get ( x , <str> )  def parse ( self ) : 		 tag = <str> self . _dict [ <str> ] = False for span in self . soup . select ( <str> ) : 			 try : 				 text = span . get_text ( ) if text == <str> : 					 return False  if text == <str> : 					 self . _dict [ <str> ] = True  if tag == <str> : 					 tag = self . get_tag ( text . strip ( <str> ) )  else : 					 self . _dict [ tag ] = text . strip ( <str> ) tag = <str>   except : 				 pass   self . parse_country_studio ( ) count_id = 0 for a in self . soup . select ( <str> ) : 			 try : 				 href = a [ <str> ] components = href . split ( <str> ) if components [ 2 ] == <str> and components [ 3 ] == <str> : 					 self . _dict [ <str> ] = components [ 4 ] count_id += 1  if self . settings : 					 if self . settings . use_kinopoisk and components [ 2 ] == <str> : 						 self . _dict [ <str> ] = href    except : 				 pass   if count_id > 1 : 			 return False  for img in self . soup . select ( <str> ) : 			 try : 				 self . _dict [ <str> ] = img [ <str> ] debug ( self . _dict [ <str> ] )  except : 				 pass   self . make_movie_api ( self . get_value ( <str> ) , self . get_value ( <str> ) , settings = self . settings ) return True   def make_full_url ( link ) : 	 import urlparse res = urlparse . urlparse ( link ) res = urlparse . ParseResult ( res . scheme if res . scheme else <str> , <str> , res . path , res . params , res . query , res . fragment ) res = urlparse . urlunparse ( res ) return res  def write_movie ( item , settings ) : 	 full_title = item . title debug ( <str> + full_title . encode ( <str> ) ) parser = DescriptionParser ( full_title , item . description , item . link , settings ) debug ( <str> ) if parser . need_skipped ( full_title ) : 		 return  if parser . parsed ( ) : 		 filename = parser . make_filename ( ) if not filename : 			 return  debug ( <str> + filename . encode ( <str> ) ) STRMWriter ( origin_url ( item . link ) ) . write ( filename , parser = parser , settings = settings ) NFOWriter ( parser , movie_api = parser . movie_api ( ) ) . write_movie ( filename ) from downloader import TorrentDownloader TorrentDownloader ( item . link , settings . torrents_path ( ) , settings ) . download ( )  else : 		 skipped ( item )  del parser  def write_movies ( rss_url , path , settings ) : 	 with filesystem . save_make_chdir_context ( path ) : 		 d = feedparser . parse ( real_url ( rss_url ) ) cnt = 0 settings . progress_dialog . update ( 0 , <str> , path ) for item in d . entries : 			 item . link = origin_url ( item . link ) write_movie ( item , settings ) cnt += 1 settings . progress_dialog . update ( cnt * 100 / len ( d . entries ) , <str> , path )    def write_tvshow ( item , settings ) : 	 full_title = item . title debug ( <str> + full_title . encode ( <str> ) ) parser = DescriptionParser ( full_title , item . description , item . link , settings ) debug ( <str> ) if parser . need_skipped ( full_title ) : 		 return  if parser . parsed ( ) : 		 import tvshowapi tvshowapi . write_tvshow ( full_title , item . link , settings , parser )  del parser  def write_tvshows ( rss_url , path , settings ) : 	 with filesystem . save_make_chdir_context ( path ) : 		 d = feedparser . parse ( real_url ( rss_url ) ) cnt = 0 settings . progress_dialog . update ( 0 , <str> , path ) for item in d . entries : 			 item . link = origin_url ( item . link ) write_tvshow ( item , settings ) cnt += 1 settings . progress_dialog . update ( cnt * 100 / len ( d . entries ) , <str> , path )    def get_rss_url ( f_id , passkey ) : 	 return <str> + str ( f_id ) + <str> + passkey  def run ( settings ) : 	 if settings . animation_save : 		 write_movies ( settings . animation_url , settings . animation_path ( ) , settings )  if settings . documentary_save : 		 write_movies ( settings . documentary_url , settings . documentary_path ( ) , settings )  if settings . movies_save : 		 write_movies ( settings . movies_url , settings . movies_path ( ) , settings )  if settings . tvshows_save : 		 write_tvshows ( get_rss_url ( 64 , settings . hdclub_passkey ) , settings . tvshow_path ( ) , settings )   def make_search_url ( what , IDs , imdb , settings ) : 	 url = <str> url += <str> + str ( IDs ) url += <str> + settings . hdclub_passkey if imdb is None : 		 url += <str> + urllib2 . quote ( what . encode ( <str> ) )  url += <str> + imdb return url  def search_generate ( what , imdb , settings , path_out ) : 	 return 0 count = 0 session = requests . session ( ) if settings . movies_save : 		 url = make_search_url ( what , 71 , imdb , settings ) result1 = search_results ( imdb , session , settings , url , 71 ) count += make_search_strms ( result1 , settings , <str> , settings . movies_path ( ) , path_out )  if settings . animation_save and count == 0 : 		 url = make_search_url ( what , 70 , imdb , settings ) result2 = search_results ( imdb , session , settings , url , 70 ) count += make_search_strms ( result2 , settings , <str> , settings . animation_path ( ) , path_out )  if settings . documentary_save and count == 0 : 		 url = make_search_url ( what , 78 , imdb , settings ) result3 = search_results ( imdb , session , settings , url , 78 ) count += make_search_strms ( result3 , settings , <str> , settings . documentary_path ( ) , path_out )  if settings . tvshows_save and count == 0 : 		 url = make_search_url ( what , 64 , imdb , settings ) result4 = search_results ( imdb , session , settings , url , 64 ) count += make_search_strms ( result4 , settings , <str> , settings . tvshow_path ( ) , path_out )  return count  def make_search_strms ( result , settings , type , path , path_out ) : 	 count = 0 for item in result : 		 link = item [ <str> ] parser = item [ <str> ] if link : 			 settings . progress_dialog . update ( count * 100 / len ( result ) , <str> , parser . get_value ( <str> ) ) if type == <str> : 				 import movieapi path = movieapi . write_movie ( parser . get_value ( <str> ) , link , settings , parser , path , skip_nfo_exists = True ) path_out . append ( path ) count += 1  if type == <str> : 				 import tvshowapi path = tvshowapi . write_tvshow ( parser . get_value ( <str> ) , link , settings , parser , path , skip_nfo_exists = True ) path_out . append ( path ) count += 1    return count  class TrackerPostsEnumerator ( object ) : 	 _items = [ ] def __init__ ( self , session ) : 		 self . _s = session self . _items [ : ] = [ ]  def items ( self ) : 		 return self . _items  def process_page ( self , url ) : 		 request = self . _s . get ( real_url ( url ) ) self . soup = BeautifulSoup ( clean_html ( request . text ) , <str> ) debug ( url ) tbody = self . soup . find ( <str> , attrs = { <str> : <str> } ) if tbody : 			 for tr in tbody : 				 try : 					 from bs4 import NavigableString if isinstance ( tr , NavigableString ) : 						 continue  item = { } TDs = tr . find_all ( <str> , recursive = False ) item [ <str> ] = TDs [ 2 ] . find ( <str> ) [ <str> ] item [ <str> ] = TDs [ 2 ] . find ( <str> ) . get_text ( ) . strip ( <str> ) item [ <str> ] = item [ <str> ] . replace ( <str> , <str> ) item [ <str> ] = TDs [ 4 ] . get_text ( ) . strip ( <str> ) item [ <str> ] = TDs [ 0 ] . find ( <str> ) [ <str> ] . split ( <str> ) [ - 1 ] self . _items . append ( item . copy ( ) )  except BaseException as e : 					 log . print_tb ( e )      def search_results ( imdb , session , settings , url , cat ) : 	 debug ( <str> + url ) enumerator = TrackerPostsEnumerator ( session ) enumerator . process_page ( url ) result = [ ] for post in enumerator . items ( ) : 		 if <str> in post and int ( post [ <str> ] ) < 5 : 			 continue  if str ( post . get ( <str> , <str> ) ) != str ( cat ) : 			 continue  page = requests . get ( real_url ( make_full_url ( post [ <str> ] ) ) ) soup = BeautifulSoup ( page . text , <str> ) content = <str> tbl = soup . find ( <str> , class_ = <str> ) for td in tbl . find_all ( <str> , class_ = <str> ) : 			 content += td . prettify ( )  img = soup . find ( <str> , attrs = { <str> : <str> } ) if img : 			 content += img . parent . prettify ( )  img = soup . find ( <str> , attrs = { <str> : <str> } ) if img : 			 content += img . parent . prettify ( )  parser = DescriptionParser ( post [ <str> ] , content , make_full_url ( post [ <str> ] ) , settings = settings , imdb = imdb ) debug ( <str> % ( post [ <str> ] , str ( parser . parsed ( ) ) , parser . get_value ( <str> ) ) ) if parser . parsed ( ) : 			 result . append ( { <str> : parser , <str> : make_full_url ( post [ <str> ] ) } )   return result  def download_torrent ( url , path , settings ) : 	 from base import save_hashes save_hashes ( path ) url = url . replace ( <str> , <str> ) if not <str> in url : 		 url += <str> + settings . hdclub_passkey  try : 		 import shutil response = urllib2 . urlopen ( real_url ( url ) ) with filesystem . fopen ( path , <str> ) as f : 			 shutil . copyfileobj ( response , f )  save_hashes ( path ) return True  except BaseException as e : 		 print_tb ( e ) return False    