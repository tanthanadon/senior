import log from log import debug import re import urllib2 , urlparse from bs4 import BeautifulSoup import base import feedparser import requests import filesystem from base import DescriptionParserBase , clean_html , Informer from nfowriter import NFOWriter from settings import Settings from strmwriter import STRMWriter import tvshowapi def real_url ( url , settings ) : 	 res = urlparse . urlparse ( url ) res = urlparse . ParseResult ( res . scheme if res . scheme else <str> , settings . rutor_domain , res . path , res . params , res . query , res . fragment ) res = urlparse . urlunparse ( res ) debug ( <str> % ( url , res ) ) return res  def origin_url ( url , settings ) : 	 res = urlparse . urlparse ( url ) res = urlparse . ParseResult ( res . scheme if res . scheme else <str> , <str> , res . path , res . params , res . query , res . fragment ) res = urlparse . urlunparse ( res ) debug ( <str> % ( url , res ) ) return res  class DescriptionParser ( DescriptionParserBase ) : 	 def __init__ ( self , content , settings = None ) : 		 Informer . __init__ ( self ) self . _dict = dict ( ) self . content = content self . settings = settings self . OK = self . parse ( )  def get_tag ( self , x ) : 		 return { <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , } . get ( x . strip ( ) , <str> )  def clean ( self , title ) : 		 title = re . sub ( <str> , <str> , title ) return title . strip ( <str> )  def get_title ( self , full_title ) : 		 try : 			 sep = <str> if not <str> in full_title : 				 sep = <str>  found = re . search ( <str> + sep , full_title ) . group ( 1 ) return self . clean ( found )  except AttributeError : 			 return full_title   def get_original_title ( self , full_title ) : 		 if not <str> in full_title : 			 return self . get_title ( full_title )  try : 			 found = re . search ( <str> , full_title ) . group ( 1 ) return self . clean ( found )  except AttributeError : 			 return full_title   def get_year ( self , full_title ) : 		 try : 			 found = re . search ( <str> , full_title ) . group ( 1 ) return unicode ( found )  except AttributeError : 			 return 0   def parse_title ( self , full_title ) : 		 self . _dict [ <str> ] = full_title self . _dict [ <str> ] = self . get_title ( full_title ) self . _dict [ <str> ] = self . get_original_title ( full_title ) self . _dict [ <str> ] = self . get_year ( full_title )  def parse_title_tvshow ( self , full_title ) : 		 self . parse_title ( full_title )  def parse ( self ) : 		 if True : 			 try : 				 self . _link = self . content debug ( self . _link )  except : 				 return False  full_title = self . _dict [ <str> ] debug ( <str> + full_title . encode ( <str> ) ) self . parse_title ( full_title ) if self . need_skipped ( full_title ) : 				 return False  if self . need_skipped_by_filter ( full_title , self . settings . rutor_filter ) : 				 return False  r = requests . get ( real_url ( self . _link , self . settings ) ) if r . status_code == requests . codes . ok : 				 return self . parse_description ( r . content )   return False  def parse_description ( self , html_text ) : 		 from HTMLParser import HTMLParseError html_text = clean_html ( html_text ) try : 			 self . soup = BeautifulSoup ( html_text , <str> )  except HTMLParseError as e : 			 log . print_tb ( e ) log . debug ( html_text ) return False  tag = <str> for b in self . soup . select ( <str> ) : 			 try : 				 text = b . get_text ( ) tag = self . get_tag ( text ) if tag == <str> : 					 plot = base . striphtml ( unicode ( b . next_sibling . next_sibling ) . strip ( ) ) if plot : 						 self . _dict [ tag ] = plot debug ( <str> % ( text . encode ( <str> ) , tag . encode ( <str> ) , self . _dict [ tag ] . encode ( <str> ) ) )   elif tag == <str> : 					 genres = [ ] elements = b . findNextSiblings ( <str> ) for a in elements : 						 if <str> in a [ <str> ] : 							 genres . append ( a . get_text ( ) )   self . _dict [ tag ] = <str> . join ( genres )  elif tag != <str> : 					 self . _dict [ tag ] = base . striphtml ( unicode ( b . next_sibling ) . strip ( ) ) debug ( <str> % ( text . encode ( <str> ) , tag . encode ( <str> ) , self . _dict [ tag ] . encode ( <str> ) ) )   except : 				 pass   tags = [ ] for tag in [ <str> , <str> , <str> , <str> , <str> , <str> ] : 			 if tag not in self . _dict : 				 tags . append ( tag )   if tags : 			 try : 				 details = self . soup . select_one ( <str> ) . get_text ( ) lines = details . split ( <str> ) for l in lines : 					 if <str> in l : 						 key , desc = l . split ( <str> , 1 ) key = key . strip ( <str> ) desc = desc . strip ( <str> ) tag = self . get_tag ( key + <str> ) if tag and desc and tag not in self . _dict : 							 self . _dict [ tag ] = desc     except BaseException as e : 				 debug ( <str> ) debug ( e ) pass   if <str> in self . _dict : 			 self . _dict [ <str> ] = self . _dict [ <str> ] . lower ( ) . replace ( <str> , <str> )  if <str> in self . _dict : 			 self . _dict [ <str> ] = self . _dict [ <str> ] . replace ( <str> , <str> ) if self . settings . rutor_nosd : 				 video = self . _dict [ <str> ] parts = video . split ( <str> ) for part in parts : 					 part = part . strip ( ) if <str> in part : 						 return False  m = re . search ( ur <str> , part ) if m : 						 w = int ( m . group ( 1 ) ) if w < 1280 : 							 return False      else : 			 pass  count_id = 0 for a in self . soup . select ( <str> ) : 			 try : 				 href = a [ <str> ] components = href . split ( <str> ) if components [ 2 ] == <str> and components [ 3 ] == <str> : 					 self . _dict [ <str> ] = components [ 4 ] count_id += 1   except : 				 pass   if count_id == 0 : 			 div_index = self . soup . select ( <str> ) if div_index : 				 for a in div_index [ 0 ] . findAll ( <str> , recursive = True ) : 					 if <str> in a [ <str> ] : 						 parts = a [ <str> ] . split ( <str> ) href = parts [ 0 ] + <str> + parts [ 1 ] + <str> + parts [ 2 ] r = requests . get ( real_url ( href , self . settings ) ) if r . status_code != requests . codes . ok : 							 break  html = r . content soup = BeautifulSoup ( clean_html ( html ) , <str> ) for a in soup . select ( <str> ) : 							 try : 								 href = a [ <str> ] components = href . split ( <str> ) if components [ 2 ] == <str> and components [ 3 ] == <str> : 									 self . _dict [ <str> ] = components [ 4 ] count_id += 1   except : 								 pass    if <str> in self . _dict : 						 break     if count_id > 1 : 			 return False  if <str> not in self . _dict : 			 if not hasattr ( self . settings , <str> ) : 				 return False   for det in self . soup . select ( <str> ) : 			 tr = det . find ( <str> , recursive = False ) if tr : 				 tds = tr . findAll ( <str> , recursive = False ) if len ( tds ) > 1 : 					 td = tds [ 1 ] img = td . find ( <str> ) try : 						 self . _dict [ <str> ] = img [ <str> ] debug ( <str> + self . _dict [ <str> ] ) break  except : 						 pass     for kp_id in self . soup . select ( <str> ) : 			 self . _dict [ <str> ] = kp_id [ <str> ]  self . make_movie_api ( self . get_value ( <str> ) , self . get_value ( <str> ) , settings = self . settings ) return True  def link ( self ) : 		 return origin_url ( self . _link , self . settings )  def need_skipped_by_filter ( self , full_title , rutor_filter ) : 		 keywords = rutor_filter . split ( ) m = re . search ( <str> , full_title ) if m : 			 quality_str = m . group ( 1 ) for key in keywords : 				 if key in quality_str : 					 return True   return False  return True   class DescriptionParserTVShows ( DescriptionParser ) : 	 def need_skipped ( self , full_title ) : 		 for phrase in [ <str> , <str> , <str> , <str> , <str> , <str> , <str> ] : 			 if phrase in full_title : 				 debug ( <str> + phrase . encode ( <str> ) ) return True   return False   class DescriptionParserRSS ( DescriptionParser ) : 	 def __init__ ( self , title , link , settings = None ) : 		 Informer . __init__ ( self ) self . _dict = dict ( ) self . content = link self . settings = settings self . _dict [ <str> ] = title . strip ( <str> ) self . OK = self . parse ( )   class DescriptionParserRSSTVShows ( DescriptionParserRSS , DescriptionParserTVShows ) : 	 pass  def write_movie_rss ( fulltitle , description , link , settings , path ) : 	 parser = DescriptionParserRSS ( fulltitle , link , settings ) if parser . parsed ( ) : 		 import movieapi movieapi . write_movie ( fulltitle , link , settings , parser , path = path , skip_nfo_exists = True )   def write_tvshow ( fulltitle , description , link , settings , path ) : 	 parser = DescriptionParserRSSTVShows ( fulltitle , link , settings ) if parser . parsed ( ) : 		 tvshowapi . write_tvshow ( fulltitle , link , settings , parser , path , skip_nfo_exists = True )   def title ( rss_url ) : 	 return <str>  def is_tvshow ( title ) : 	 m = re . match ( <str> , title ) if m : 		 return True  m = re . search ( <str> , title ) if m : 		 return True  return False  def get_source_url ( link ) : 	 m = re . match ( <str> , link ) if m is None : 		 return None  return <str> % m . group ( 1 )  def write_tvshows ( rss_url , path , settings ) : 	 debug ( <str> % rss_url ) with filesystem . save_make_chdir_context ( path , <str> ) : 		 d = feedparser . parse ( real_url ( rss_url , settings ) ) cnt = 0 settings . progress_dialog . update ( 0 , title ( rss_url ) , path ) for item in d . entries : 			 if not is_tvshow ( item . title ) : 				 continue  try : 				 debug ( item . title . encode ( <str> ) )  except : 				 continue  write_tvshow ( fulltitle = item . title , description = item . description , link = origin_url ( get_source_url ( item . link ) , settings ) , settings = settings , path = path ) cnt += 1 settings . progress_dialog . update ( cnt * 100 / len ( d . entries ) , title ( rss_url ) , path )    def write_movies_rss ( rss_url , path , settings ) : 	 debug ( <str> % rss_url ) with filesystem . save_make_chdir_context ( path , <str> ) : 		 d = feedparser . parse ( real_url ( rss_url , settings ) ) cnt = 0 settings . progress_dialog . update ( 0 , title ( rss_url ) , path ) for item in d . entries : 			 if is_tvshow ( item . title ) : 				 continue  try : 				 debug ( item . title . encode ( <str> ) )  except : 				 continue  write_movie_rss ( fulltitle = item . title , description = item . description , link = origin_url ( get_source_url ( item . link ) , settings ) , settings = settings , path = path ) cnt += 1 settings . progress_dialog . update ( cnt * 100 / len ( d . entries ) , title ( rss_url ) , path )    def get_rss_url ( f_id ) : 	 return <str> + str ( f_id )  def run ( settings ) : 	 if settings . movies_save : 		 write_movies_rss ( get_rss_url ( 1 ) , settings . movies_path ( ) , settings )  if settings . animation_save : 		 write_movies_rss ( get_rss_url ( 7 ) , settings . animation_path ( ) , settings )  if settings . animation_tvshows_save : 		 write_tvshows ( get_rss_url ( 7 ) , settings . animation_tvshow_path ( ) , settings )  if settings . tvshows_save : 		 write_tvshows ( get_rss_url ( 4 ) , settings . tvshow_path ( ) , settings )   def get_magnet_link ( url ) : 	 r = requests . get ( real_url ( url , settings ) ) if r . status_code == requests . codes . ok : 		 soup = BeautifulSoup ( clean_html ( r . content ) , <str> ) for a in soup . select ( <str> ) : 			 debug ( a [ <str> ] ) return a [ <str> ]   return None  def download_torrent ( url , path , settings ) : 	 from base import save_hashes save_hashes ( path ) url = urllib2 . unquote ( url ) debug ( <str> + url ) page = requests . get ( real_url ( url , settings ) ) soup = BeautifulSoup ( clean_html ( page . content ) , <str> ) a = soup . select ( <str> ) if len ( a ) > 1 : 		 link = a [ 1 ] [ <str> ]  else : 		 link = None  if link : 		 r = requests . get ( real_url ( link , settings ) ) debug ( r . headers ) if <str> in r . headers : 			 if not <str> in r . headers [ <str> ] : 				 return False   try : 			 with filesystem . fopen ( path , <str> ) as torr : 				 for chunk in r . iter_content ( 100000 ) : 					 torr . write ( chunk )   save_hashes ( path ) return True  except : 			 pass   return False  def make_search_strms ( result , settings , type , path , path_out ) : 	 count = 0 for item in result : 		 link = item [ <str> ] parser = item [ <str> ] settings . progress_dialog . update ( count * 100 / len ( result ) , <str> , parser . get_value ( <str> ) ) if link : 			 if type == <str> : 				 import movieapi _path = movieapi . write_movie ( parser . get_value ( <str> ) , link , settings , parser , path , skip_nfo_exists = True ) if _path : 					 path_out . append ( _path ) count += 1   if type == <str> : 				 import tvshowapi _path = tvshowapi . write_tvshow ( parser . get_value ( <str> ) , link , settings , parser , path , skip_nfo_exists = True ) if _path : 					 path_out . append ( _path ) count += 1     return count  class PostsEnumerator ( object ) : 	 _items = [ ] def __init__ ( self , settings ) : 		 self . _s = requests . Session ( ) self . settings = settings self . _items [ : ] = [ ]  def process_page ( self , url ) : 		 try : 			 request = self . _s . get ( real_url ( url , self . settings ) )  except requests . exceptions . ConnectionError : 			 return  self . soup = BeautifulSoup ( clean_html ( request . content ) , <str> ) debug ( url ) indx = self . soup . find ( <str> , attrs = { <str> : <str> } ) if indx : 			 bgnd = indx . find ( <str> , class_ = <str> ) if bgnd : 				 for row in bgnd . next_siblings : 					 td2 = row . contents [ 1 ] td5 = row . contents [ - 1 ] item = { } topic_a = td2 . find_all ( <str> ) if topic_a : 						 topic_a = topic_a [ - 1 ] item [ <str> ] = topic_a  dl_a = td2 . find ( <str> , class_ = <str> ) if dl_a : 						 item [ <str> ] = dl_a [ <str> ]  span_green = td5 . find ( <str> , class_ = <str> ) if span_green : 						 item [ <str> ] = span_green . get_text ( ) . strip ( )  self . _items . append ( item . copy ( ) )     def items ( self ) : 		 return self . _items   def search_results ( imdb , settings , url , what = None ) : 	 result = [ ] enumerator = PostsEnumerator ( settings ) from log import dump_context with dump_context ( <str> ) : 		 enumerator . process_page ( url )  for post in enumerator . items ( ) : 		 try : 			 if <str> in post and int ( post [ <str> ] ) < 1 : 				 continue   except ValueError : 			 pass  title = post [ <str> ] . get_text ( ) dl_link = str ( <str> + post [ <str> ] ) link = get_source_url ( dl_link ) import copy s = copy . copy ( settings ) if not imdb : 			 s . no_skip_by_imdb = True  if is_tvshow ( title ) : 			 parser = DescriptionParserRSSTVShows ( title , link , s )  else : 			 parser = DescriptionParserRSS ( title , link , s )  if parser . parsed ( ) : 			 if ( imdb and parser . get_value ( <str> ) == imdb ) : 				 result . append ( { <str> : parser , <str> : dl_link } )  elif what and parser . Dict ( ) . get ( <str> ) == what : 				 result . append ( { <str> : parser , <str> : dl_link } )    return result  def search_generate ( what , imdb , settings , path_out ) : 	 count = 0 if settings . movies_save : 		 url = <str> + imdb result1 = search_results ( imdb , settings , url ) count += make_search_strms ( result1 , settings , <str> , settings . movies_path ( ) , path_out )  if settings . animation_save and count == 0 : 		 url = <str> + imdb result2 = search_results ( imdb , settings , url ) count += make_search_strms ( result2 , settings , <str> , settings . animation_path ( ) , path_out )  if settings . animation_tvshows_save and count == 0 : 		 url = <str> + imdb result3 = search_results ( imdb , settings , url ) count += make_search_strms ( result3 , settings , <str> , settings . animation_tvshow_path ( ) , path_out )  if settings . tvshows_save and count == 0 : 		 url = <str> + imdb result4 = search_results ( imdb , settings , url ) count += make_search_strms ( result4 , settings , <str> , settings . tvshow_path ( ) , path_out )  if settings . movies_save and count == 0 : 		 if not result1 : 			 url = <str> + urllib2 . quote ( what . encode ( <str> ) ) result1 = search_results ( None , settings , url , what ) count += make_search_strms ( result1 , settings , <str> , settings . movies_path ( ) , path_out )   return count  if __name__ == <str> : 	 import filesystem test_dir = filesystem . join ( filesystem . dirname ( __file__ ) , <str> ) settings = Settings ( filesystem . join ( test_dir , <str> ) ) settings . addon_data_path = filesystem . join ( test_dir , <str> ) settings . torrent_path = filesystem . join ( test_dir , <str> ) settings . torrent_player = <str> settings . kp_googlecache = False settings . kp_usezaborona = True settings . use_kinopoisk = True settings . use_worldart = True settings . rutor_domain = <str> path_out = [ ] from log import dump_context with dump_context ( <str> ) : 		 run ( settings )  search_generate ( None , <str> , settings , path_out ) pass    