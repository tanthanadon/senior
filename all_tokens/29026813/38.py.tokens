from base import DescriptionParserBase , Informer from soup_base import soup_base from log import debug protocol = <str> domain = <str> class DescriptionParser ( DescriptionParserBase , soup_base ) : 	 def __init__ ( self , url , fulltitle , settings = None ) : 		 Informer . __init__ ( self ) soup_base . __init__ ( self , url ) self . _dict = dict ( ) self . _dict [ <str> ] = url self . _dict [ <str> ] = fulltitle self . settings = settings self . OK = self . parse ( )  def link ( self ) : 		 return self . _dict [ <str> ]  def parse ( self ) : 		 import re imdb = self . soup . find ( <str> , class_ = <str> ) if imdb and imdb . get ( <str> ) : 			 m = re . search ( <str> , imdb [ <str> ] ) if m : 				 self . _dict [ <str> ] = m . group ( 1 )   kp_a = self . soup . select ( <str> ) if kp_a : 			 try : 				 self . _dict [ <str> ] = kp_a [ 0 ] [ <str> ] . split ( <str> ) [ - 1 ]  except : 				 pass   from bs4 import NavigableString tag = None for div in self . soup . find_all ( <str> , class_ = <str> ) : 			 txt = div . get_text ( ) if <str> in txt : 				 txt = <str> for part in div . children : 					 if isinstance ( part , NavigableString ) : 						 txt += unicode ( part )  else : 						 if part . name == <str> : 							 txt += <str>  else : 							 txt += part . get_text ( )    def write_tag ( tag , value ) : 					 self . _dict [ tag ] = value . split ( <str> ) [ - 1 ] . lstrip ( )  for line in txt . split ( <str> ) : 					 if line . startswith ( <str> ) : 						 write_tag ( <str> , line )  if line . startswith ( <str> ) : 						 write_tag ( <str> , line )     if self . get_value ( <str> ) : 			 self . make_movie_api ( self . get_value ( <str> ) , self . get_value ( <str> ) , settings = self . settings ) return True    class BaseEnumerator : 	 def __init__ ( self , content ) : 		 from bs4 import BeautifulSoup self . soup = BeautifulSoup ( content , <str> )  def items ( self ) : 		 div = self . soup . find ( <str> , class_ = <str> ) sz = self . size ( ) root = div if sz == 30 else self . soup if root : 			 count = 0 for a in root . find_all ( <str> ) : 				 href = a . get ( <str> , <str> ) if href . startswith ( <str> ) and href . endswith ( <str> ) : 					 count += 1 if count > sz : 						 return  box = a . parent . parent . parent fulltitle = box . find ( <str> ) . get_text ( ) if box else <str> yield a [ <str> ] , fulltitle     def size ( self ) : 		 span = self . soup . find ( <str> , class_ = <str> ) if span : 			 import re m = re . search ( <str> , span . get_text ( ) ) if m : 				 return int ( m . group ( 1 ) )   return 30   class PostEnumerator ( soup_base , BaseEnumerator ) : 	 def __init__ ( self , url ) : 		 soup_base . __init__ ( self , url )   def url ( type ) : 	 return <str> . format ( protocol , domain , type )  class Process ( object ) : 	 def __init__ ( self , settings ) : 		 self . settings = settings  def process_movie ( self , url , parser ) : 		 import movieapi import filesystem api = parser . movie_api ( ) try : 			 genre = api [ <str> ]  except : 			 genre = [ ]  if <str> in genre : 			 if not self . settings . animation_save : 				 return  base_path = self . settings . animation_path ( )  elif <str> in genre : 			 if not self . settings . documentary_save : 				 return  base_path = self . settings . documentary_path ( )  else : 			 if not self . settings . movies_save : 				 return  base_path = self . settings . movies_path ( )  with filesystem . save_make_chdir_context ( base_path , <str> ) : 			 return movieapi . write_movie ( parser . get_value ( <str> ) , url , self . settings , parser , path = base_path )   def process_tvshow ( self , url , parser ) : 		 import tvshowapi import filesystem api = parser . movie_api ( ) try : 			 genre = api [ <str> ]  except : 			 genre = [ ]  if <str> in genre : 			 if not self . settings . animation_tvshows_save : 				 return  base_path = self . settings . animation_tvshow_path ( )  else : 			 if not self . settings . tvshows_save : 				 return  base_path = self . settings . tvshow_path ( )  with filesystem . save_make_chdir_context ( base_path , <str> ) : 			 return tvshowapi . write_tvshow ( parser . get_value ( <str> ) , url , self . settings , parser , path = base_path )   def process ( self , url , fulltitle ) : 		 parser = DescriptionParser ( url , fulltitle , settings = self . settings ) try : 			 parser . Dict ( ) [ <str> ] = parser . movie_api ( ) [ <str> ] parser . Dict ( ) [ <str> ] = parser . movie_api ( ) [ <str> ]  except : 			 None  if parser . parsed ( ) : 			 if <str> in url or parser . movie_api ( ) . get ( <str> ) == <str> : 				 return self . process_tvshow ( url , parser )  else : 				 return self . process_movie ( url , parser )    def test ( self ) : 		 url = <str> fulltitle = <str> self . process ( url , fulltitle )   def run ( settings ) : 	 import filesystem types = [ <str> , <str> , <str> , <str> , <str> ] items_on_page = 30 processed_urls = [ ] def urls ( ) : 		 for t in types : 			 if not getattr ( settings , <str> + t , False ) : 				 continue  indx = 0 for item in PostEnumerator ( url ( t ) ) . items ( ) : 				 progress = int ( indx * 100 / items_on_page ) settings . progress_dialog . update ( progress , <str> . format ( t . upper ( ) ) , item [ 1 ] ) indx += 1 yield item    process = Process ( settings ) for href , fulltitle in urls ( ) : 		 if href not in processed_urls : 			 process . process ( href , fulltitle ) processed_urls . append ( href )    def download_torrent ( url , path , settings ) : 	 from base import save_hashes save_hashes ( path ) import urllib2 url = urllib2 . unquote ( url ) debug ( <str> + url ) soup = soup_base ( url ) . soup if soup : 		 btn = soup . find ( <str> , class_ = <str> ) if btn : 			 try : 				 dnl_url = btn [ <str> ] dnl_url = dnl_url . split ( <str> ) [ - 1 ] dnl_url = dnl_url . replace ( <str> , <str> ) dnl_url = <str> . format ( protocol , domain , dnl_url )  except BaseException as e : 				 pass  import requests r = requests . get ( dnl_url ) try : 				 import filesystem with filesystem . fopen ( path , <str> ) as torr : 					 for chunk in r . iter_content ( 100000 ) : 						 torr . write ( chunk )   save_hashes ( path ) return True  except : 				 pass    return False  def search_generate ( what , imdb , settings , path_out ) : 	 url = <str> . format ( protocol , domain ) headers = { <str> : domain , <str> : url , <str> : url + <str> , <str> : <str> } data = { <str> : <str> , <str> : <str> , <str> : str ( imdb ) } import requests res = requests . post ( url + <str> , headers = headers , data = data ) enumerator = BaseEnumerator ( res . content ) count = enumerator . size ( ) fails = 0 def urls ( ) : 		 indx = 0 for item in enumerator . items ( ) : 			 progress = int ( indx * 100 / count ) settings . progress_dialog . update ( progress , <str> , item [ 1 ] ) indx += 1 yield item   process = Process ( settings ) for href , fulltitle in urls ( ) : 		 result = process . process ( href , fulltitle ) if result : 			 path_out . append ( result )  else : 			 fails += 1   return count - fails  if __name__ == <str> : 	 from settings import Settings import filesystem test_dir = filesystem . join ( filesystem . dirname ( __file__ ) , <str> ) settings = Settings ( filesystem . join ( test_dir , <str> ) ) settings . addon_data_path = filesystem . join ( test_dir , <str> ) settings . torrent_path = filesystem . join ( test_dir , <str> ) settings . torrent_player = <str> settings . kp_googlecache = False settings . kp_usezaborona = True settings . use_kinopoisk = True settings . use_worldart = True settings . kinohd_4k = False settings . kinohd_3d = False path_out = [ ] res = search_generate ( None , <str> , settings , path_out ) run ( settings ) pass    