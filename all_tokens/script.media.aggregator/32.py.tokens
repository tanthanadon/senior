import log from log import debug , print_tb import feedparser , filesystem from bs4 import BeautifulSoup import urllib2 import json , os from settings import Settings from base import * from movieapi import * from nfowriter import * from strmwriter import * def real_url ( url ) : 	 import urlparse res = urlparse . urlparse ( url ) res = urlparse . ParseResult ( <str> , <str> , res . path , res . params , res . query , res . fragment ) res = urlparse . urlunparse ( res ) return res  def origin_url ( url ) : 	 import urlparse res = urlparse . urlparse ( url ) res = urlparse . ParseResult ( <str> , <str> , res . path , res . params , res . query , res . fragment ) res = urlparse . urlunparse ( res ) return res  class DescriptionParser ( DescriptionParserBase ) : 	 def __init__ ( self , full_title , content , link , settings , imdb = None ) : 		 self . _link = link DescriptionParserBase . __init__ ( self , full_title , content , settings ) if imdb : 			 self . _dict [ <str> ] = imdb   def need_skipped ( self , full_title ) : 		 if self . settings . bluebird_nouhd and <str> in full_title . split ( <str> ) : 			 return True  return DescriptionParserBase . need_skipped ( self , full_title )  def link ( self ) : 		 return origin_url ( self . _link )  def get_tag ( self , x ) : 		 return { <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> } . get ( x . rstrip ( <str> ) , <str> )  def parse ( self ) : 		 tag = <str> def get_actors ( b ) : 			 div = b . find_next ( <str> ) actors = [ ] for img in div . find_all ( <str> ) : 				 if <str> in img [ <str> ] : 					 actors . append ( img [ <str> ] )   if actors : 				 return <str> . join ( actors )  txt = b . next_sibling from bs4 import NavigableString if isinstance ( txt , NavigableString ) : 				 txt = unicode ( txt ) if txt . startswith ( <str> ) : 					 return txt . lstrip ( <str> ) . strip ( )  return txt if txt else <str>  return <str>  def get_other ( b ) : 			 return unicode ( b . next_sibling ) . lstrip ( <str> ) . strip ( )  for b in self . soup . find_all ( <str> ) : 			 tag = self . get_tag ( b . get_text ( ) ) if tag == <str> : 				 self . Dict ( ) [ tag ] = get_actors ( b )  elif tag : 				 self . Dict ( ) [ tag ] = get_other ( b )   self . parse_country_studio ( ) from sets import Set imdb_ids = Set ( ) for a in self . soup . select ( <str> ) : 			 try : 				 href = a [ <str> ] components = href . split ( <str> ) if components [ 2 ] == <str> and components [ 3 ] == <str> : 					 imdb_ids . add ( components [ 4 ] )  if components [ 2 ] == <str> and <str> in components : 					 self . _dict [ <str> ] = href   except : 				 pass   if len ( imdb_ids ) > 1 : 			 return False  elif len ( imdb_ids ) == 1 : 			 self . _dict [ <str> ] = imdb_ids . pop ( )  s = origin_url ( <str> ) import re res = re . search ( <str> , self . _link ) if res : 			 s = s . replace ( <str> , res . group ( 1 ) ) self . _dict [ <str> ] = s debug ( self . _dict [ <str> ] )  self . make_movie_api ( self . get_value ( <str> ) , self . get_value ( <str> ) , settings = self . settings ) return True   def write_movie ( item , settings , path ) : 	 full_title = item . title debug ( <str> + full_title . encode ( <str> ) ) parser = DescriptionParser ( full_title , item . description , item . link , settings ) debug ( <str> ) if parser . need_skipped ( full_title ) : 		 return  if parser . parsed ( ) : 		 filename = parser . make_filename ( ) if not filename : 			 return  from movieapi import make_imdb_path , copy_files imdb = parser . get_value ( <str> ) new_path = make_imdb_path ( path , imdb ) if not filesystem . exists ( new_path ) : 			 with filesystem . save_make_chdir_context ( new_path , <str> ) : 				 pass   if new_path != path : 			 copy_files ( path , new_path , filename )  debug ( <str> + filename . encode ( <str> ) ) STRMWriter ( origin_url ( item . link ) ) . write ( filename , new_path , parser = parser , settings = settings ) NFOWriter ( parser , movie_api = parser . movie_api ( ) ) . write_movie ( filename , new_path ) if settings . bluebird_preload_torrents : 			 from downloader import TorrentDownloader TorrentDownloader ( item . link , settings . torrents_path ( ) , settings ) . download ( )  settings . update_paths . add ( new_path )  else : 		 skipped ( item )  del parser  def write_movies ( rss_url , path , settings ) : 	 with filesystem . save_make_chdir_context ( path , <str> ) : 		 d = feedparser . parse ( real_url ( rss_url ) ) cnt = 0 settings . progress_dialog . update ( 0 , <str> , path ) for item in d . entries : 			 item . link = origin_url ( item . link ) write_movie ( item , settings , path ) cnt += 1 settings . progress_dialog . update ( cnt * 100 / len ( d . entries ) , <str> , path )    def write_tvshow ( item , settings ) : 	 full_title = item . title debug ( <str> + full_title . encode ( <str> ) ) parser = DescriptionParser ( full_title , item . description , item . link , settings ) debug ( <str> ) if parser . need_skipped ( full_title ) : 		 return  if parser . parsed ( ) : 		 import tvshowapi tvshowapi . write_tvshow ( full_title , item . link , settings , parser )  del parser  def write_tvshows ( rss_url , path , settings ) : 	 return with filesystem . save_make_chdir_context ( path , <str> ) : 		 d = feedparser . parse ( real_url ( rss_url ) ) cnt = 0 settings . progress_dialog . update ( 0 , <str> , path ) for item in d . entries : 			 item . link = origin_url ( item . link ) write_tvshow ( item , settings ) cnt += 1 settings . progress_dialog . update ( cnt * 100 / len ( d . entries ) , <str> , path )    def get_rss_url ( f_id , passkey ) : 	 return origin_url ( <str> + str ( f_id ) + <str> + passkey )  def create_session ( settings ) : 	 if create_session . session : 		 return create_session . session  session = requests . session ( ) if not settings . bluebird_login or not settings . bluebird_password : 		 return None  data = { <str> : settings . bluebird_login , <str> : settings . bluebird_password } headers = { <str> : <str> , <str> : real_url ( <str> ) , <str> : real_url ( <str> ) , <str> : <str> } r = session . post ( real_url ( <str> ) , headers = headers , data = data ) if r . ok and <str> not in r . text : 		 create_session . session = session return session  return None  create_session . session = None def get_passkey ( settings ) : 	 s = create_session ( settings ) if not s : 		 return None  r = s . get ( real_url ( <str> ) ) if r . ok : 		 txt = r . text indx = txt . index ( <str> ) if indx >= 0 : 			 txt = txt [ indx : ] i1 = txt . index ( <str> ) i2 = txt . index ( <str> ) txt = txt [ i1 + 3 : i2 ] return txt   return None  def run ( settings ) : 	 if not settings . bluebird_passkey : 		 settings . bluebird_passkey = get_passkey ( settings )  if not settings . bluebird_passkey : 		 return  if settings . animation_save : 		 write_movies ( get_rss_url ( 2 , settings . bluebird_passkey ) , settings . animation_path ( ) , settings )  if settings . documentary_save : 		 write_movies ( get_rss_url ( 3 , settings . bluebird_passkey ) , settings . documentary_path ( ) , settings )  if settings . movies_save : 		 write_movies ( get_rss_url ( 1 , settings . bluebird_passkey ) , settings . movies_path ( ) , settings )  if settings . tvshows_save : 		 write_tvshows ( get_rss_url ( 6 , settings . bluebird_passkey ) , settings . tvshow_path ( ) , settings )   def make_search_url ( what , IDs , imdb , settings ) : 	 url = <str> url += <str> + str ( IDs ) if imdb is None : 		 url += <str> + urllib2 . quote ( what . encode ( <str> ) )  url += <str> + imdb return origin_url ( url )  def get_cookies ( settings ) : 	 s = settings . bluebird_cookies ss = s . split ( <str> ) ss = [ i . split ( <str> ) for i in ss ] return { i [ 0 ] : i [ 1 ] for i in ss }  def search_generate ( what , imdb , settings , path_out ) : 	 count = 0 session = create_session ( settings ) if not session : 		 return 0  if settings . movies_save : 		 url = make_search_url ( what , 1 , imdb , settings ) result1 = search_results ( imdb , session , settings , url , 1 ) count += make_search_strms ( result1 , settings , <str> , settings . movies_path ( ) , path_out )  if settings . animation_save and count == 0 : 		 url = make_search_url ( what , 2 , imdb , settings ) result2 = search_results ( imdb , session , settings , url , 2 ) count += make_search_strms ( result2 , settings , <str> , settings . animation_path ( ) , path_out )  if settings . documentary_save and count == 0 : 		 url = make_search_url ( what , 3 , imdb , settings ) result3 = search_results ( imdb , session , settings , url , 3 ) count += make_search_strms ( result3 , settings , <str> , settings . documentary_path ( ) , path_out )  if settings . tvshows_save and count == 0 : 		 url = make_search_url ( what , 6 , imdb , settings ) result4 = search_results ( imdb , session , settings , url , 6 ) count += make_search_strms ( result4 , settings , <str> , settings . tvshow_path ( ) , path_out )  return count  def make_search_strms ( result , settings , type , path , path_out ) : 	 count = 0 for item in result : 		 link = item [ <str> ] parser = item [ <str> ] if link : 			 settings . progress_dialog . update ( count * 100 / len ( result ) , <str> , parser . get_value ( <str> ) ) if type == <str> : 				 import movieapi _path = movieapi . write_movie ( parser . get_value ( <str> ) , link , settings , parser , path , skip_nfo_exists = True , download_torrent = False ) if _path : 					 path_out . append ( _path ) count += 1   if type == <str> : 				 import tvshowapi _path = tvshowapi . write_tvshow ( parser . get_value ( <str> ) , link , settings , parser , path , skip_nfo_exists = True ) if _path : 					 path_out . append ( _path ) count += 1     return count  class TrackerPostsEnumerator ( object ) : 	 _items = [ ] def __init__ ( self , session , cookies = None ) : 		 self . _s = session self . _items [ : ] = [ ] self . cookies = cookies  def items ( self ) : 		 return self . _items  def process_page ( self , url ) : 		 request = self . _s . get ( real_url ( url ) , cookies = self . cookies ) self . soup = BeautifulSoup ( clean_html ( request . text ) , <str> ) debug ( url ) tbody = self . soup . find ( <str> , attrs = { <str> : <str> } ) if tbody : 			 for tr in tbody : 				 try : 					 from bs4 import NavigableString if isinstance ( tr , NavigableString ) : 						 continue  item = { } TDs = tr . find_all ( <str> , recursive = False ) item [ <str> ] = TDs [ 2 ] . find ( <str> ) [ <str> ] item [ <str> ] = TDs [ 2 ] . find ( <str> ) . get_text ( ) . strip ( <str> ) item [ <str> ] = item [ <str> ] . replace ( <str> , <str> ) item [ <str> ] = TDs [ 4 ] . get_text ( ) . strip ( <str> ) item [ <str> ] = TDs [ 0 ] . find ( <str> ) [ <str> ] . split ( <str> ) [ - 1 ] self . _items . append ( item . copy ( ) )  except BaseException as e : 					 log . print_tb ( e )      def search_results ( imdb , session , settings , url , cat ) : 	 debug ( <str> + url ) enumerator = TrackerPostsEnumerator ( session ) from log import dump_context with dump_context ( <str> ) : 		 enumerator . process_page ( url )  result = [ ] for post in enumerator . items ( ) : 		 if <str> in post and int ( post [ <str> ] ) < 5 : 			 continue  if str ( post . get ( <str> , <str> ) ) != str ( cat ) : 			 continue  url = real_url ( post [ <str> ] ) page = session . get ( url , headers = { <str> : real_url ( <str> ) } ) soup = BeautifulSoup ( page . text , <str> ) content = <str> tbl = soup . find ( <str> , attrs = { <str> : <str> } ) for td in tbl . find_all ( <str> , class_ = <str> ) : 			 tdn = td . next_sibling content += unicode ( tdn )  img = soup . find ( <str> , attrs = { <str> : <str> } ) if img : 			 content += unicode ( img . parent )  img = soup . find ( <str> , attrs = { <str> : <str> } ) if img : 			 content += unicode ( img . parent )  parser = DescriptionParser ( post [ <str> ] , content , origin_url ( post [ <str> ] ) , settings = settings , imdb = imdb ) debug ( <str> % ( post [ <str> ] , str ( parser . parsed ( ) ) , parser . get_value ( <str> ) ) ) if parser . parsed ( ) : 			 result . append ( { <str> : parser , <str> : origin_url ( post [ <str> ] ) } )   return result  def download_torrent ( url , path , settings ) : 	 if not settings . bluebird_passkey : 		 settings . bluebird_passkey = get_passkey ( settings )  if not settings . bluebird_passkey : 		 return False  from base import save_hashes save_hashes ( path ) url = url . replace ( <str> , <str> ) if not <str> in url : 		 url += <str> + settings . bluebird_passkey  try : 		 response = urllib2 . urlopen ( real_url ( url ) ) data = response . read ( ) if not data . startswith ( <str> ) : 			 return False  with filesystem . fopen ( path , <str> ) as f : 			 f . write ( data )  save_hashes ( path ) return True  except BaseException as e : 		 print_tb ( e ) return False   def login ( user , passw , code ) : 	 pass    