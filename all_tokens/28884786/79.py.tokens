from cltk . utils . cltk_logger import logger __author__ = [ <str> ] __license__ = <str> class Scansion :      def __init__ ( self ) :          self . vowels = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] self . sing_cons = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] self . doub_cons = [ <str> , <str> , <str> ] self . long_vowels = [ <str> , <str> , <str> , <str> , <str> ] self . diphthongs = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] self . stops = [ <str> , <str> , <str> , <str> , <str> , <str> ] self . liquids = [ <str> , <str> ] self . punc = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] self . punc_stops = [ <str> , <str> , <str> ]  def _clean_text ( self , text ) :          clean = [ ] for char in text :              if char in self . punc_stops :                  clean += <str>  elif char not in self . punc :                  clean += char  else :                  pass   return ( <str> . join ( clean ) ) . lower ( )  def _clean_accents ( self , text ) :          accents = { <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , } text = self . _clean_text ( text ) for char in text :              for key in accents . keys ( ) :                  if char in key :                      text = text . replace ( char , accents . get ( key ) )  else :                      pass    return text  def _tokenize ( self , text ) :          sentences = [ ] tokens = [ ] for word in self . _clean_accents ( text ) . split ( <str> ) :              tokens . append ( word ) if <str> in word :                  sentences . append ( tokens ) tokens = [ ]   return sentences  def _syllable_condenser ( self , words_syllables ) :          sentences_syllables = [ ] for sentence in words_syllables :              syllables_sentence = [ ] for word in sentence :                  syllables_sentence += word  sentences_syllables . append ( syllables_sentence )  return sentences_syllables  def _long_by_nature ( self , syllable ) :          vowel_group = [ ] for char in syllable :              print if char in self . long_vowels :                  return True  elif char not in self . sing_cons and char not in self . doub_cons :                  vowel_group += char   if <str> . join ( vowel_group ) in self . diphthongs :              return True   def _long_by_position ( self , syllable , sentence ) :          try :              next_syll = sentence [ sentence . index ( syllable ) + 1 ] if ( next_syll [ 0 ] in self . sing_cons and next_syll [ 1 ] in self . sing_cons ) and ( next_syll [ 0 ] not in self . stops and next_syll [ 1 ] not in self . liquids ) :                  return True  elif syllable [ - 1 ] in self . vowels and next_syll [ 0 ] in self . doub_cons :                  return True  elif syllable [ - 1 ] in self . sing_cons and ( next_syll [ 0 ] in self . sing_cons ) :                  return True  else :                  pass   except IndexError :              logger . info ( <str> , syllable )   def _scansion ( self , sentence_syllables ) :          scanned_text = [ ] for sentence in sentence_syllables :              scanned_sent = [ ] for syllable in sentence :                  if self . _long_by_position ( syllable , sentence ) or self . _long_by_nature ( syllable ) :                      scanned_sent . append ( <str> )  else :                      scanned_sent . append ( <str> )   if len ( scanned_sent ) > 1 :                  del scanned_sent [ - 1 ] scanned_sent . append ( <str> )  scanned_text . append ( <str> . join ( scanned_sent ) )  return scanned_text  def _make_syllables ( self , sentences_words ) :          text = self . _tokenize ( sentences_words ) all_syllables = [ ] for sentence in text :              syll_per_sent = [ ] for word in sentence :                  syll_start = 0 syll_per_word = [ ] cur_letter_in = 0 while cur_letter_in < len ( word ) :                      letter = word [ cur_letter_in ] if ( cur_letter_in != len ( word ) - 1 ) and ( word [ cur_letter_in ] + word [ cur_letter_in + 1 ] ) in self . diphthongs :                          cur_letter_in += 1 syll_per_word . append ( word [ syll_start : cur_letter_in + 1 ] ) syll_start = cur_letter_in + 1  elif ( letter in self . vowels ) or ( letter in self . long_vowels ) :                          syll_per_word . append ( word [ syll_start : cur_letter_in + 1 ] ) syll_start = cur_letter_in + 1  cur_letter_in += 1  try :                      last_vowel = syll_per_word [ - 1 ] [ - 1 ] cur_letter_in = len ( word ) - 1 leftovers = <str> while word [ cur_letter_in ] != last_vowel :                          if word [ cur_letter_in ] != <str> :                              leftovers = word [ cur_letter_in ] + leftovers  cur_letter_in -= 1  syll_per_word [ - 1 ] += leftovers syll_per_sent . append ( syll_per_word )  except IndexError :                      logger . info ( <str> , word )   all_syllables . append ( syll_per_sent )  return all_syllables  def scan_text ( self , input_string ) :          syllables = self . _make_syllables ( input_string ) sentence_syllables = self . _syllable_condenser ( syllables ) meter = self . _scansion ( sentence_syllables ) return meter    