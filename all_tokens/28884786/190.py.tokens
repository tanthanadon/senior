import logging import os import sys import time from cltk . utils . cltk_logger import logger try :      from gensim . models import Word2Vec  except AttributeError :      logger . error ( <str> )  from cltk . corpus . utils . formatter import phi5_plaintext_cleanup from cltk . corpus . utils . formatter import tlg_plaintext_cleanup from cltk . corpus . utils . formatter import assemble_phi5_author_filepaths from cltk . corpus . utils . formatter import assemble_tlg_author_filepaths from cltk . stem . latin . j_v import JVReplacer from cltk . stem . lemma import LemmaReplacer from cltk . stop . latin import STOPS_LIST as latin_stops from cltk . tokenize . word import WordTokenizer from cltk . tokenize . sentence import TokenizeSentence from cltk . tokenize . word import WordTokenizer def gen_docs ( corpus , lemmatize , rm_stops ) :      assert corpus in [ <str> , <str> ] if corpus == <str> :          language = <str> filepaths = assemble_phi5_author_filepaths ( ) jv_replacer = JVReplacer ( ) text_cleaner = phi5_plaintext_cleanup word_tokenizer = WordTokenizer ( <str> ) if rm_stops :              stops = latin_stops  else :              stops = None   elif corpus == <str> :          language = <str> filepaths = assemble_tlg_author_filepaths ( ) text_cleaner = tlg_plaintext_cleanup word_tokenizer = WordTokenizer ( <str> ) if rm_stops :              stops = latin_stops  else :              stops = None   if lemmatize :          lemmatizer = LemmaReplacer ( language )  sent_tokenizer = TokenizeSentence ( language ) for filepath in filepaths :          with open ( filepath ) as f :              text = f . read ( )  text = text_cleaner ( text , rm_punctuation = False , rm_periods = False ) sent_tokens = sent_tokenizer . tokenize_sentences ( text ) for sentence in sent_tokens :              sentence = text_cleaner ( sentence , rm_punctuation = True , rm_periods = True ) sentence = word_tokenizer ( sentence ) sentence = [ s . lower ( ) for s in sentence ] sentence = [ w for w in sentence if w ] if language == <str> :                  sentence = [ w [ 1 : ] if w . startswith ( <str> ) else w for w in sentence ]  if stops :                  sentence = [ w for w in sentence if w not in stops ]  sentence = [ w for w in sentence if len ( w ) > 1 ] if sentence :                  sentence = sentence  if lemmatize :                  sentence = lemmatizer . lemmatize ( sentence )  if sentence and language == <str> :                  sentence = [ jv_replacer . replace ( word ) for word in sentence ]  if sentence :                  yield sentence     def make_model ( corpus , lemmatize = False , rm_stops = False , size = 100 , window = 10 , min_count = 5 , workers = 4 , sg = 1 , save_path = None ) :      t0 = time . time ( ) sentences_stream = gen_docs ( corpus , lemmatize = lemmatize , rm_stops = rm_stops ) model = Word2Vec ( sentences = list ( sentences_stream ) , size = size , window = window , min_count = min_count , workers = workers , sg = sg ) model . init_sims ( replace = True ) if save_path :          save_path = os . path . expanduser ( save_path ) model . save ( save_path )  print ( <str> . format ( save_path , ( time . time ( ) - t0 ) / 60 ) )  def get_sims ( word , language , lemmatized = False , threshold = 0.70 ) :      jv_replacer = JVReplacer ( ) if language == <str> :          word = jv_replacer . replace ( word ) . casefold ( )  model_dirs = { <str> : get_cltk_data_dir ( ) + <str> , <str> : get_cltk_data_dir ( ) + <str> } assert language in model_dirs . keys ( ) , <str> . format ( model_dirs . keys ( ) ) if lemmatized :          lemma_str = <str>  else :          lemma_str = <str>  model_name = <str> . format ( language , lemma_str ) model_dir_abs = os . path . expanduser ( model_dirs [ language ] ) model_path = os . path . join ( model_dir_abs , model_name ) try :          model = Word2Vec . load ( model_path )  except FileNotFoundError as fnf_error :          print ( fnf_error ) print ( <str> . format ( language ) ) raise  try :          similars = model . most_similar ( word )  except KeyError as key_err :          print ( key_err ) possible_matches = [ ] for term in model . vocab :              if term . startswith ( word [ : 3 ] ) :                  possible_matches . append ( term )   print ( <str> . format ( possible_matches ) ) return None  returned_sims = [ ] for similar in similars :          if similar [ 1 ] > threshold :              returned_sims . append ( similar [ 0 ] )   if not returned_sims :          print ( <str> . format ( threshold ) )  return returned_sims   