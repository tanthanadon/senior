import json import os import re import codecs import time import logging from typing import List , Dict , Tuple , Set , Any , Generator from nltk . corpus . reader . api import CorpusReader from nltk . corpus . reader import PlaintextCorpusReader from nltk . probability import FreqDist from nltk . tokenize import sent_tokenize , word_tokenize from nltk import pos_tag from cltk . prosody . latin . string_utils import flatten from cltk . tokenize . sentence import TokenizeSentence from cltk . tokenize . word import WordTokenizer LOG = logging . getLogger ( __name__ ) LOG . addHandler ( logging . NullHandler ( ) ) SUPPORTED_CORPORA = { <str> : [ <str> , <str> , <str> , ] , <str> : [ <str> , <str> , ] } def get_corpus_reader ( corpus_name : str = None , language : str = None ) -> CorpusReader :      BASE = get_cltk_data_dir ( ) + <str> . format ( language ) root = os . path . join ( os . path . expanduser ( BASE ) , corpus_name ) if not os . path . exists ( root ) or corpus_name not in SUPPORTED_CORPORA . get ( language ) :          raise ValueError ( <str> . format ( corpus_name , language ) )  sentence_tokenizer = TokenizeSentence ( language ) the_word_tokenizer = WordTokenizer ( language ) doc_pattern = <str> if language == <str> :          if corpus_name == <str> :              skip_keywords = [ <str> , <str> ] return FilteredPlaintextCorpusReader ( root = root , fileids = doc_pattern , sent_tokenizer = sentence_tokenizer , word_tokenizer = the_word_tokenizer , skip_keywords = skip_keywords )  if corpus_name == <str> :              valid_json_root = os . path . join ( root , <str> ) return JsonfileCorpusReader ( root = valid_json_root , sent_tokenizer = sentence_tokenizer , word_tokenizer = the_word_tokenizer , target_language = <str> )  if corpus_name == <str> :              return TesseraeCorpusReader ( root = root , fileids = <str> , sent_tokenizer = sentence_tokenizer , word_tokenizer = the_word_tokenizer , )   if language == <str> :          if corpus_name == <str> :              valid_json_root = os . path . join ( root , <str> ) return JsonfileCorpusReader ( root = valid_json_root , sent_tokenizer = sentence_tokenizer , word_tokenizer = the_word_tokenizer , target_language = <str> )  if corpus_name == <str> :              return TesseraeCorpusReader ( root = root , fileids = <str> , sent_tokenizer = sent_tokenize , word_tokenizer = word_tokenize , pos_tagger = pos_tag , target_language = <str> )    def assemble_corpus ( corpus_reader : CorpusReader , types_requested : List [ str ] , type_dirs : Dict [ str , List [ str ] ] = None , type_files : Dict [ str , List [ str ] ] = None ) -> CorpusReader :      fileid_names = [ ] try :          all_file_ids = list ( corpus_reader . fileids ( ) ) clean_ids_types = [ ] if type_files :              for key , valuelist in type_files . items ( ) :                  if key in types_requested :                      for value in valuelist :                          if value in all_file_ids :                              if key :                                  clean_ids_types . append ( ( value , key ) )       if type_dirs :              for key , valuelist in type_dirs . items ( ) :                  if key in types_requested :                      for value in valuelist :                          corrected_dir = value . replace ( <str> , <str> ) corrected_dir = <str> . format ( corrected_dir ) for name in all_file_ids :                              if name and name . startswith ( corrected_dir ) :                                  clean_ids_types . append ( ( name , key ) )       clean_ids_types . sort ( key = lambda x : x [ 0 ] ) fileid_names , categories = zip ( * clean_ids_types ) corpus_reader . _fileids = fileid_names return corpus_reader  except Exception :          LOG . exception ( <str> )   class FilteredPlaintextCorpusReader ( PlaintextCorpusReader , CorpusReader ) :      def __init__ ( self , root , fileids = None , encoding = <str> , skip_keywords = None , ** kwargs ) :          if not fileids :              fileids = <str>  PlaintextCorpusReader . __init__ ( self , root , fileids , encoding ) CorpusReader . __init__ ( self , root , fileids , encoding ) if <str> in kwargs :              self . _sent_tokenizer = kwargs [ <str> ]  if <str> in kwargs :              self . _word_tokenizer = kwargs [ <str> ]  self . skip_keywords = skip_keywords  def words ( self , fileids = None ) -> Generator [ str , str , None ] :          if not fileids :              fileids = self . fileids ( )  for para in self . paras ( fileids ) :              flat_para = flatten ( para ) skip = False if self . skip_keywords :                  for keyword in self . skip_keywords :                      if keyword in flat_para :                          skip = True    if not skip :                  for word in flat_para :                      yield word     def paras ( self , fileids = None ) -> Generator [ str , str , None ] :          if not fileids :              fileids = self . fileids ( )  for para in super ( ) . paras ( fileids ) :              flat_para = flatten ( para ) skip = False if self . skip_keywords :                  for keyword in self . skip_keywords :                      if keyword in flat_para :                          skip = True    if not skip :                  yield para    def sents ( self , fileids = None ) -> Generator [ str , str , None ] :          if not fileids :              fileids = self . fileids ( )  for sent in super ( ) . sents ( fileids ) :              skip = False if self . skip_keywords :                  for keyword in self . skip_keywords :                      if keyword in sent :                          skip = True    if not skip :                  yield sent    def docs ( self , fileids = None ) -> Generator [ str , str , None ] :          if not fileids :              fileids = self . fileids ( )  for path , encoding in self . abspaths ( fileids , include_encoding = True ) :              with codecs . open ( path , <str> , encoding = encoding ) as reader :                  if self . skip_keywords :                      tmp_data = [ ] for line in reader :                          skip = False for keyword in self . skip_keywords :                              if keyword in line :                                  skip = True   if not skip :                              tmp_data . append ( line )   yield <str> . join ( tmp_data )  else :                      yield reader . read ( )     def sizes ( self , fileids = None ) -> Generator [ int , int , None ] :          if not fileids :              fileids = self . fileids ( )  for path in self . abspaths ( fileids ) :              yield os . path . getsize ( path )   def __iter__ ( self ) :          for sent in self . sents ( ) :              yield sent    class JsonfileCorpusReader ( CorpusReader ) :      def __init__ ( self , root , fileids = None , encoding = <str> , skip_keywords = None , target_language = None , paragraph_separator = <str> , ** kwargs ) :          if not target_language :              target_language = <str>  if not fileids :              fileids = <str> . format ( target_language )  CorpusReader . __init__ ( self , root , fileids , encoding ) if <str> in kwargs :              self . _sent_tokenizer = kwargs [ <str> ]  if <str> in kwargs :              self . _word_tokenizer = kwargs [ <str> ]  self . skip_keywords = skip_keywords self . paragraph_separator = paragraph_separator  def words ( self , fileids = None ) -> Generator [ str , str , None ] :          for sentence in self . sents ( fileids ) :              words = self . _word_tokenizer . tokenize ( sentence ) for word in words :                  yield word    def sents ( self , fileids = None ) -> Generator [ str , str , None ] :          for para in self . paras ( fileids ) :              sentences = self . _sent_tokenizer . tokenize ( para ) for sentence in sentences :                  yield sentence    def paras ( self , fileids = None ) -> Generator [ str , str , None ] :          def _recurse_to_strings ( my_dict : Dict [ str , Any ] ) -> List [ str ] :              vals = [ ] m_keys = sorted ( list ( my_dict . keys ( ) ) ) for mkey in m_keys :                  if isinstance ( my_dict [ mkey ] , dict ) :                      vals += _recurse_to_strings ( my_dict [ mkey ] )  else :                      vals += [ my_dict [ mkey ] ]   return vals  for doc in self . docs ( fileids ) :              text_data = _recurse_to_strings ( doc [ <str> ] ) text_sections = [ ] for text_part in text_data :                  skip = False if self . skip_keywords :                      for keyword in self . skip_keywords :                          if keyword in text_part :                              skip = True    if not skip :                      text_sections . append ( text_part )   for para in text_sections :                  yield para . strip ( )    def docs ( self , fileids = None ) -> Generator [ Dict [ str , Any ] , Dict [ str , Any ] , None ] :          for path , encoding in self . abspaths ( fileids , include_encoding = True ) :              with codecs . open ( path , <str> , encoding = encoding ) as reader :                  the_doc = json . loads ( reader . read ( ) ) if <str> not in the_doc :                      the_doc [ <str> ] = path  yield the_doc    def sizes ( self , fileids = None ) -> Generator [ int , int , None ] :          if not fileids :              fileids = self . fileids ( )  for path in self . abspaths ( fileids ) :              yield os . path . getsize ( path )   def __iter__ ( self ) -> Generator [ str , str , None ] :          for sent in self . sents ( ) :              yield sent    class TesseraeCorpusReader ( PlaintextCorpusReader ) :      def __init__ ( self , root , fileids = None , encoding = <str> , skip_keywords = None , ** kwargs ) :          PlaintextCorpusReader . __init__ ( self , root , fileids , encoding ) if <str> in kwargs :              self . _sent_tokenizer = kwargs [ <str> ]  if <str> in kwargs :              self . _word_tokenizer = kwargs [ <str> ]  if <str> in kwargs :              self . pos_tagger = kwargs [ <str> ]   def docs ( self : object , fileids : str ) :          for path , encoding in self . abspaths ( fileids , include_encoding = True ) :              with codecs . open ( path , <str> , encoding = encoding ) as f :                  yield f . read ( )    def texts ( self : object , fileids : str , plaintext : bool = True ) :          for doc in self . docs ( fileids ) :              if plaintext == True :                  doc = re . sub ( <str> , <str> , doc )  doc = doc . rstrip ( ) yield doc   def paras ( self : object , fileids : str ) :          for text in self . texts ( fileids ) :              for para in text . split ( <str> ) :                  yield para    def lines ( self : object , fileids : str , plaintext : bool = True ) :          for text in self . texts ( fileids , plaintext ) :              text = re . sub ( <str> , <str> , text , re . MULTILINE ) for line in text . split ( <str> ) :                  yield line    def sents ( self : object , fileids : str ) :          for para in self . paras ( fileids ) :              for sent in sent_tokenize ( para ) :                  yield sent    def words ( self : object , fileids : str ) :          for sent in self . sents ( fileids ) :              for token in word_tokenize ( sent ) :                  yield token    def pos_tokenize ( self : object , fileids : str ) :          for para in self . paras ( fileids ) :              yield [ self . pos_tagger ( word_tokenize ( sent ) ) for sent in sent_tokenize ( para ) ]   def describe ( self : object , fileids : str = None ) :          started = time . time ( ) counts = FreqDist ( ) tokens = FreqDist ( ) for para in self . paras ( fileids ) :              counts [ <str> ] += 1 for sent in para :                  counts [ <str> ] += 1 for word in sent :                      counts [ <str> ] += 1 tokens [ word ] += 1    n_fileids = len ( self . fileids ( ) ) return { <str> : n_fileids , <str> : counts [ <str> ] , <str> : counts [ <str> ] , <str> : counts [ <str> ] , <str> : len ( tokens ) , <str> : round ( ( counts [ <str> ] / len ( tokens ) ) , 3 ) , <str> : round ( ( counts [ <str> ] / n_fileids ) , 3 ) , <str> : round ( ( counts [ <str> ] / counts [ <str> ] ) , 3 ) , <str> : round ( ( time . time ( ) - started ) , 3 ) , }    