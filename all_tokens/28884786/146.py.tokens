__author__ = [ <str> , <str> ] __license__ = <str> import re from typing import List from nltk . tokenize . punkt import PunktSentenceTokenizer , PunktParameters from cltk . tokenize . latin . params import ABBREVIATIONS from cltk . tokenize . latin . params import latin_exceptions from cltk . tokenize . latin . params import latin_replacements as REPLACEMENTS from cltk . tokenize . latin . sentence import SentenceTokenizer from cltk . tokenize . latin . params import LatinLanguageVars class WordTokenizer :      ENCLITICS = [ <str> , <str> , <str> , <str> , <str> , <str> ] EXCEPTIONS = list ( set ( ENCLITICS + latin_exceptions ) ) def __init__ ( self ) :          self . language = <str> self . punkt_param = PunktParameters ( ) self . punkt_param . abbrev_types = set ( ABBREVIATIONS ) self . sent_tokenizer = PunktSentenceTokenizer ( self . punkt_param ) self . word_tokenizer = LatinLanguageVars ( )  def tokenize ( self , text : str ) -> List [ str ] :          def matchcase ( word ) :              def replace ( matching ) :                  text = matching . group ( ) if text . isupper ( ) :                      return word . upper ( )  elif text . islower ( ) :                      return word . lower ( )  elif text [ 0 ] . isupper ( ) :                      return word . capitalize ( )  return word  return replace  for replacement in REPLACEMENTS :              text = re . sub ( replacement [ 0 ] , matchcase ( replacement [ 1 ] ) , text , flags = re . IGNORECASE )  sents = self . sent_tokenizer . tokenize ( text ) tokens = [ ] for sent in sents :              temp_tokens = self . word_tokenizer . word_tokenize ( sent ) if temp_tokens :                  if temp_tokens [ 0 ] . endswith ( <str> ) :                      if temp_tokens [ 0 ] . lower ( ) not in WordTokenizer . EXCEPTIONS :                          temp = [ temp_tokens [ 0 ] [ : - 2 ] , <str> ] temp_tokens = temp + temp_tokens [ 1 : ]   if temp_tokens [ - 1 ] . endswith ( <str> ) :                      final_word = temp_tokens [ - 1 ] [ : - 1 ] del temp_tokens [ - 1 ] temp_tokens += [ final_word , <str> ]  for token in temp_tokens :                      tokens . append ( token )    specific_tokens = [ ] for token in tokens :              is_enclitic = False if token . lower ( ) not in WordTokenizer . EXCEPTIONS :                  for enclitic in WordTokenizer . ENCLITICS :                      if token . endswith ( enclitic ) :                          if enclitic == <str> :                              specific_tokens += [ token [ : - len ( enclitic ) ] ] + [ <str> ]  elif enclitic == <str> :                              if token . endswith ( <str> ) :                                  specific_tokens += [ token [ : - len ( enclitic ) + 1 ] ] + [ <str> ]  else :                                  specific_tokens += [ token [ : - len ( enclitic ) ] ] + [ <str> ]   else :                              specific_tokens += [ token [ : - len ( enclitic ) ] ] + [ <str> + enclitic ]  is_enclitic = True break    if not is_enclitic :                  specific_tokens . append ( token )   return specific_tokens    