from typing import List , Dict from cltk . prosody . latin . syllabifier import Syllabifier class Scansion :      SHORT_VOWELS = [ <str> , <str> , <str> , <str> , <str> , <str> ] LONG_VOWELS = [ <str> , <str> , <str> , <str> , <str> ] VOWELS = SHORT_VOWELS + LONG_VOWELS DIPHTHONGS = [ <str> , <str> , <str> , <str> , <str> ] SINGLE_CONSONANTS = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] DOUBLE_CONSONANTS = [ <str> , <str> ] CONSONANTS = SINGLE_CONSONANTS + DOUBLE_CONSONANTS DIGRAPHS = [ <str> , <str> , <str> , <str> ] LIQUIDS = [ <str> , <str> ] MUTES = [ <str> , <str> , <str> , <str> , <str> , <str> ] MUTE_LIQUID_EXCEPTIONS = [ <str> , <str> ] NASALS = [ <str> , <str> ] SESTS = [ <str> , <str> , <str> , <str> , <str> ] def __init__ ( self , punctuation = [ <str> , <str> , <str> , <str> , <str> ] , clausula_length = 13 , elide = True ) :          self . punctuation = punctuation self . clausula_length = clausula_length self . elide = elide self . syllabifier = Syllabifier ( )  def _tokenize_syllables ( self , word : str ) -> List [ Dict ] :          syllable_tokens = [ ] syllables = self . syllabifier . syllabify ( word ) longs = self . LONG_VOWELS + self . DIPHTHONGS for i , _ in enumerate ( syllables ) :              syllable_dict = { <str> : syllables [ i ] , <str> : i , <str> : ( False , None ) } if any ( long in syllables [ i ] for long in longs ) :                  if syllables [ i ] [ : 3 ] != <str> :                      syllable_dict [ <str> ] = True  else :                      syllable_dict [ <str> ] = False   else :                  syllable_dict [ <str> ] = False  if i < len ( syllables ) - 1 and syllable_dict [ <str> ] [ - 1 ] in self . CONSONANTS :                  if syllable_dict [ <str> ] [ - 1 ] in self . MUTES and syllables [ i + 1 ] [ 0 ] in self . LIQUIDS and syllable_dict [ <str> ] [ - 1 ] + syllables [ i + 1 ] [ 0 ] not in self . MUTE_LIQUID_EXCEPTIONS :                      syllable_dict [ <str> ] = ( False , <str> )  elif syllable_dict [ <str> ] [ - 1 ] in self . DOUBLE_CONSONANTS or syllables [ i + 1 ] [ 0 ] in self . CONSONANTS :                      syllable_dict [ <str> ] = ( True , None )  else :                      syllable_dict [ <str> ] = ( False , None )   elif i < len ( syllables ) - 1 and syllable_dict [ <str> ] [ - 1 ] in self . VOWELS and len ( syllables [ i + 1 ] ) > 1 :                  if syllables [ i + 1 ] [ 0 ] in self . MUTES and syllables [ i + 1 ] [ 1 ] in self . LIQUIDS and syllables [ i + 1 ] [ 0 ] + syllables [ i + 1 ] [ 1 ] not in self . MUTE_LIQUID_EXCEPTIONS :                      syllable_dict [ <str> ] = ( False , <str> )  elif syllables [ i + 1 ] [ 0 ] in self . CONSONANTS and syllables [ i + 1 ] [ 1 ] in self . CONSONANTS or syllables [ i + 1 ] [ 0 ] in self . DOUBLE_CONSONANTS :                      syllable_dict [ <str> ] = ( True , None )  else :                      syllable_dict [ <str> ] = ( False , None )   elif len ( syllable_dict [ <str> ] ) > 2 and syllable_dict [ <str> ] [ - 1 ] in self . CONSONANTS and syllable_dict [ <str> ] [ - 2 ] in self . CONSONANTS and syllable_dict [ <str> ] [ - 3 ] in self . VOWELS :                  syllable_dict [ <str> ] = ( True , None )  else :                  syllable_dict [ <str> ] = ( False , None )  syllable_tokens . append ( syllable_dict ) if len ( syllables ) > 2 and i == len ( syllables ) - 2 :                  if syllable_dict [ <str> ] or syllable_dict [ <str> ] [ 0 ] :                      syllable_dict [ <str> ] = True  else :                      syllable_tokens [ i - 1 ] [ <str> ] = True   elif len ( syllables ) == 2 and i == 0 or len ( syllables ) == 1 :                  syllable_dict [ <str> ] = True  syllable_dict [ <str> ] = False if <str> not in syllable_dict else True  return syllable_tokens  def _tokenize_words ( self , sentence : str ) -> List [ Dict ] :          tokens = [ ] split_sent = [ word for word in sentence . split ( <str> ) if word != <str> ] for i , word in enumerate ( split_sent ) :              if len ( word ) == 1 and word not in self . VOWELS :                  break  word_dict = { <str> : split_sent [ i ] , <str> : i } word_dict [ <str> ] = self . _tokenize_syllables ( split_sent [ i ] ) word_dict [ <str> ] = len ( word_dict [ <str> ] ) if i != 0 and word_dict [ <str> ] [ 0 ] [ <str> ] [ 0 ] in self . VOWELS or i != 0 and word_dict [ <str> ] [ 0 ] [ <str> ] [ 0 ] == <str> :                  last_syll_prev_word = tokens [ i - 1 ] [ <str> ] [ - 1 ] if last_syll_prev_word [ <str> ] [ - 1 ] in self . LONG_VOWELS or last_syll_prev_word [ <str> ] [ - 1 ] == <str> :                      last_syll_prev_word [ <str> ] = ( True , <str> )  elif len ( last_syll_prev_word [ <str> ] ) > 1 and last_syll_prev_word [ <str> ] [ - 2 : ] in self . DIPHTHONGS :                      last_syll_prev_word [ <str> ] = ( True , <str> )  elif last_syll_prev_word [ <str> ] [ - 1 ] in self . SHORT_VOWELS :                      last_syll_prev_word [ <str> ] = ( True , <str> )   if i > 0 and tokens [ i - 1 ] [ <str> ] [ - 1 ] [ <str> ] [ - 1 ] in self . CONSONANTS and word_dict [ <str> ] [ 0 ] [ <str> ] [ 0 ] in self . CONSONANTS :                  tokens [ i - 1 ] [ <str> ] [ - 1 ] [ <str> ] = ( True , None )  elif i > 0 and tokens [ i - 1 ] [ <str> ] [ - 1 ] [ <str> ] [ - 1 ] in self . VOWELS and word_dict [ <str> ] [ 0 ] [ <str> ] [ 0 ] in self . CONSONANTS :                  if any ( sest in word_dict [ <str> ] [ 0 ] [ <str> ] for sest in self . SESTS ) :                      tokens [ i - 1 ] [ <str> ] [ - 1 ] [ <str> ] = ( False , <str> )  elif word_dict [ <str> ] [ 0 ] [ <str> ] [ 0 ] in self . MUTES and word_dict [ <str> ] [ 0 ] [ <str> ] [ 1 ] in self . LIQUIDS :                      tokens [ i - 1 ] [ <str> ] [ - 1 ] [ <str> ] = ( False , <str> )  elif word_dict [ <str> ] [ 0 ] [ <str> ] [ 0 ] in self . DOUBLE_CONSONANTS or word_dict [ <str> ] [ 0 ] [ <str> ] [ 1 ] in self . CONSONANTS :                      tokens [ i - 1 ] [ <str> ] [ - 1 ] [ <str> ] = ( True , None )   tokens . append ( word_dict )  return tokens  def tokenize ( self , text : str ) -> List [ Dict ] :          tokenized_sentences = text . split ( <str> ) tokenized_text = [ ] for sentence in tokenized_sentences :              sentence_dict = { } sentence_dict [ <str> ] = sentence sentence_dict [ <str> ] = self . _tokenize_words ( sentence ) tokenized_text . append ( sentence_dict )  return tokenized_text  def scan_text ( self , text : str ) -> List [ str ] :          tokens = self . tokenize ( text ) clausulae = [ ] for sentence in tokens :              sentence_clausula = [ ] syllables = [ word [ <str> ] for word in sentence [ <str> ] ] flat_syllables = [ syllable for word in syllables for syllable in word ] if self . elide :                  flat_syllables = [ syll for syll in flat_syllables if not syll [ <str> ] [ 0 ] ] [ : - 1 ] [ : : - 1 ]  for syllable in flat_syllables :                  if len ( sentence_clausula ) < self . clausula_length - 1 :                      if syllable [ <str> ] or syllable [ <str> ] [ 0 ] :                          sentence_clausula . append ( <str> )  else :                          sentence_clausula . append ( <str> )    sentence_clausula = sentence_clausula [ : : - 1 ] sentence_clausula . append ( <str> ) clausulae . append ( <str> . join ( sentence_clausula ) )  clausulae = clausulae [ : - 1 ] return clausulae    