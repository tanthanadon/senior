__author__ = [ <str> , <str> , <str> ] __license__ = <str> from builtins import bytes from cltk . corpus . greek . tlg_index import TLG_INDEX from cltk . corpus . greek . tlg_index import TLG_WORKS_INDEX from cltk . corpus . latin . phi5_index import PHI5_INDEX from cltk . corpus . latin . phi5_index import PHI5_WORKS_INDEX from cltk . utils . cltk_logger import logger from unicodedata import normalize from cltk . tokenize . word import WordTokenizer import re import os import regex TLG_PHI_REPLACEMENTS = { <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , } TONOS_OXIA = { <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , } def tonos_oxia_converter ( text , reverse = False ) :      for char_tonos , char_oxia in TONOS_OXIA . items ( ) :          if not reverse :              text = text . replace ( char_tonos , char_oxia )  else :              text = text . replace ( char_oxia , char_tonos )   return text  def remove_non_ascii ( input_string ) :      no_ascii = <str> . join ( i for i in input_string if ord ( i ) < 128 ) return no_ascii  def remove_non_latin ( input_string , also_keep = None ) :      if also_keep :          also_keep += [ <str> ]  else :          also_keep = [ <str> ]  latin_chars = <str> latin_chars += latin_chars . lower ( ) latin_chars += <str> . join ( also_keep ) no_latin = <str> . join ( [ char for char in input_string if char in latin_chars ] ) return no_latin  def tlg_plaintext_cleanup ( text , rm_punctuation = False , rm_periods = False ) :      remove_comp = regex . compile ( <str> , flags = regex . VERSION1 ) text = remove_comp . sub ( <str> , text ) new_text = None if rm_punctuation :          new_text = <str> punctuation = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] if rm_periods :              punctuation += [ <str> , <str> ]  for char in text :              if char in punctuation :                  pass  else :                  new_text += char    if new_text :          text = new_text  replace_comp = regex . compile ( <str> ) text = replace_comp . sub ( <str> , text ) comp_space = regex . compile ( <str> ) text = comp_space . sub ( <str> , text ) return text  def cltk_normalize ( text , compatibility = True ) :      if compatibility :          return normalize ( <str> , text )  else :          return normalize ( <str> , text )   def phi5_plaintext_cleanup ( text , rm_punctuation = False , rm_periods = False ) :      remove_comp = regex . compile ( <str> ) text = remove_comp . sub ( <str> , text ) new_text = None if rm_punctuation :          new_text = <str> punctuation = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] if rm_periods :              punctuation += [ <str> ]  for char in text :              if bytes ( char , <str> ) == <str> :                  pass  elif char in punctuation :                  pass  else :                  new_text += char    if new_text :          text = new_text  replace_comp = regex . compile ( <str> ) text = replace_comp . sub ( <str> , text ) comp_space = regex . compile ( <str> ) text = comp_space . sub ( <str> , text ) return text  def assemble_tlg_author_filepaths ( ) :      plaintext_dir_rel = get_cltk_data_dir ( ) + <str> plaintext_dir = os . path . expanduser ( plaintext_dir_rel ) filepaths = [ os . path . join ( plaintext_dir , x + <str> ) for x in TLG_INDEX ] return filepaths  def assemble_phi5_author_filepaths ( ) :      plaintext_dir_rel = get_cltk_data_dir ( ) + <str> plaintext_dir = os . path . expanduser ( plaintext_dir_rel ) filepaths = [ os . path . join ( plaintext_dir , x + <str> ) for x in PHI5_INDEX ] return filepaths  def assemble_tlg_works_filepaths ( ) :      plaintext_dir_rel = get_cltk_data_dir ( ) + <str> plaintext_dir = os . path . expanduser ( plaintext_dir_rel ) all_filepaths = [ ] for author_code in TLG_WORKS_INDEX :          author_data = TLG_WORKS_INDEX [ author_code ] works = author_data [ <str> ] for work in works :              f = os . path . join ( plaintext_dir , author_code + <str> + <str> + work + <str> ) all_filepaths . append ( f )   return all_filepaths  def assemble_phi5_works_filepaths ( ) :      plaintext_dir_rel = get_cltk_data_dir ( ) + <str> plaintext_dir = os . path . expanduser ( plaintext_dir_rel ) all_filepaths = [ ] for author_code in PHI5_WORKS_INDEX :          author_data = PHI5_WORKS_INDEX [ author_code ] works = author_data [ <str> ] for work in works :              f = os . path . join ( plaintext_dir , author_code + <str> + <str> + work + <str> ) all_filepaths . append ( f )   return all_filepaths  patterns = [ ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) ] def build_match_and_apply_functions ( pattern , replace ) :      def matches_rule ( word ) :          return re . search ( pattern , word )  def apply_rule ( word ) :          return re . sub ( pattern , replace , word )  return ( matches_rule , apply_rule )  rules = [ build_match_and_apply_functions ( pattern , replace ) for ( pattern , replace ) in patterns ] def normalize_fr ( string ) :      string = string . lower ( ) word_tokenizer = WordTokenizer ( <str> ) tokens = word_tokenizer . tokenize ( string ) normalized_text = [ ] for token in tokens :          for matches_rule , apply_rule in rules :              if matches_rule ( token ) :                  normalized = apply_rule ( token ) normalized_text . append ( normalized )    return normalized_text   