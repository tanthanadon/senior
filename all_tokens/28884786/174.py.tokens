import os import unittest from unittest . mock import patch from nltk . tokenize . punkt import PunktSentenceTokenizer from cltk . corpus . utils . importer import CorpusImporter from cltk . tokenize . latin . sentence import LatinPunktSentenceTokenizer from cltk . tokenize . latin . sentence import SentenceTokenizer as LatinSentenceTokenizer from cltk . tokenize . greek . sentence import GreekPunktSentenceTokenizer , GreekRegexSentenceTokenizer from cltk . tokenize . greek . sentence import SentenceTokenizer as GreekSentenceTokenizer from cltk . tokenize . sanskrit . word import WordTokenizer as SanskritWordTokenizer from cltk . tokenize . utils import BaseSentenceTokenizerTrainer from cltk . tokenize . latin . utils import LatinSentenceTokenizerTrainer from cltk . tokenize . akkadian . word import tokenize_akkadian_words , tokenize_akkadian_signs from cltk . tokenize . arabic . word import WordTokenizer as ArabicWordTokenizer from cltk . tokenize . greek . word import WordTokenizer as GreekWordTokenizer from cltk . tokenize . latin . word import WordTokenizer as LatinWordTokenizer from cltk . tokenize . middle_english . word import WordTokenizer as MiddleEnglishWordTokenizer from cltk . tokenize . middle_high_german . word import WordTokenizer as MiddleHighGermanWordTokenizer from cltk . tokenize . old_french . word import WordTokenizer as OldFrenchWordTokenizer from cltk . tokenize . old_norse . word import WordTokenizer as OldNorseWordTokenizer from cltk . tokenize . sanskrit . word import WordTokenizer as SanskritWordTokenizer from cltk . tokenize . sentence import TokenizeSentence from cltk . tokenize . word import WordTokenizer from cltk . tokenize . line import LineTokenizer __license__ = <str> class TestSentenceTokenize ( unittest . TestCase ) :      @ classmethod def setUpClass ( self ) :          corpus_importer = CorpusImporter ( <str> ) corpus_importer . import_corpus ( <str> ) corpus_importer = CorpusImporter ( <str> ) corpus_importer . import_corpus ( <str> ) self . greek_text = self . latin_text = <str>  def test_sentence_tokenizer_latin_punkt ( self ) :          target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> ] tokenizer = LatinPunktSentenceTokenizer ( ) tokenized_sentences = tokenizer . tokenize ( self . latin_text ) self . assertEqual ( tokenized_sentences , target )  def test_sentence_tokenizer_latin_punkt_strict ( self ) :          target = [ <str> , <str> , <str> , <str> ] tokenizer = LatinPunktSentenceTokenizer ( strict = True ) tokenized_sentences = tokenizer . tokenize ( ) self . assertEqual ( tokenized_sentences , target )  def test_sentence_tokenizer_latin ( self ) :          target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> ] tokenizer = TokenizeSentence ( <str> ) tokenized_sentences = tokenizer . tokenize_sentences ( self . latin_text ) self . assertEqual ( tokenized_sentences , target )  def test_sentence_tokenizer_latin_punkt_switch ( self ) :          target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> ] tokenizer = LatinSentenceTokenizer ( tokenizer = <str> ) tokenized_sentences = tokenizer . tokenize ( self . latin_text ) self . assertEqual ( tokenized_sentences , target )  def test_sentence_tokenizer_latin_punkt_missing ( self ) :          with patch . object ( LatinPunktSentenceTokenizer , <str> , <str> ) :              with self . assertRaises ( FileNotFoundError ) :                  tokenizer = LatinPunktSentenceTokenizer ( )    def test_sentence_tokenizer_greek_regex ( self ) :          target = [ <str> , <str> , <str> ] tokenizer = GreekRegexSentenceTokenizer ( ) tokenized_sentences = tokenizer . tokenize ( self . greek_text ) self . assertEqual ( tokenized_sentences , target )  def test_sentence_tokenizer_greek_punkt ( self ) :          target = [ <str> ] tokenizer = GreekPunktSentenceTokenizer ( ) tokenized_sentences = tokenizer . tokenize ( self . greek_text ) self . assertEqual ( tokenized_sentences , target )  def test_sentence_tokenizer_greek_regex_switch ( self ) :          target = [ <str> , <str> , <str> ] tokenizer = GreekSentenceTokenizer ( tokenizer = <str> ) tokenized_sentences = tokenizer . tokenize ( self . greek_text ) self . assertEqual ( tokenized_sentences , target )  def test_sentence_tokenizer_greek_punkt_switch ( self ) :          target = [ <str> ] tokenizer = GreekSentenceTokenizer ( tokenizer = <str> ) tokenized_sentences = tokenizer . tokenize ( self . greek_text ) self . assertEqual ( tokenized_sentences , target )  def test_sentence_tokenizer_greek_punkt_missing ( self ) :          with patch . object ( GreekPunktSentenceTokenizer , <str> , <str> ) :              with self . assertRaises ( FileNotFoundError ) :                  tokenizer = GreekPunktSentenceTokenizer ( )    def test_sentence_tokenizer_sanskrit ( self ) :          text = target = [ <str> , <str> , <str> , <str> ] tokenizer = TokenizeSentence ( <str> ) tokenized_sentences = tokenizer . tokenize ( text ) self . assertEqual ( tokenized_sentences , target )   class TestWordTokenize ( unittest . TestCase ) :      @ classmethod def setUpClass ( self ) :          pass  def test_greek_word_tokenizer_base ( self ) :          word_tokenizer = WordTokenizer ( <str> ) test = <str> target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] result = word_tokenizer . tokenize ( test ) self . assertEqual ( result , target )  def test_greek_word_tokenizer ( self ) :          word_tokenizer = GreekWordTokenizer ( ) test = <str> target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] result = word_tokenizer . tokenize ( test ) self . assertEqual ( result , target )  def test_latin_word_tokenizer_base ( self ) :          word_tokenizer = WordTokenizer ( <str> ) tests = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] results = [ ] for test in tests :              result = word_tokenizer . tokenize ( test ) results . append ( result )  target = [ [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] ] self . assertEqual ( results , target )  def test_tokenize_latin_words ( self ) :          word_tokenizer = LatinWordTokenizer ( ) test = <str> tokens = word_tokenizer . tokenize ( test ) target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] self . assertEqual ( tokens , target ) test = <str> tokens = word_tokenizer . tokenize ( test ) target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] self . assertEqual ( tokens , target ) test = <str> tokens = word_tokenizer . tokenize ( test ) target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] self . assertEqual ( tokens , target ) test = <str> tokens = word_tokenizer . tokenize ( test ) target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] self . assertEqual ( tokens , target ) test = <str> tokens = word_tokenizer . tokenize ( test ) target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] self . assertEqual ( tokens , target ) test = <str> tokens = word_tokenizer . tokenize ( test ) target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] self . assertEqual ( tokens , target ) test = <str> tokens = word_tokenizer . tokenize ( test ) target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] self . assertEqual ( tokens , target ) test = <str> tokens = word_tokenizer . tokenize ( test ) target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] self . assertEqual ( tokens , target )  def test_tokenize_arabic_words_base ( self ) :          word_tokenizer = WordTokenizer ( <str> ) tests = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> ] results = [ ] for test in tests :              result = word_tokenizer . tokenize ( test ) results . append ( result )  target = [ [ <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] ] self . assertEqual ( results , target )  def test_tokenize_arabic_words ( self ) :          word_tokenizer = ArabicWordTokenizer ( ) tests = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> ] results = [ ] for test in tests :              result = word_tokenizer . tokenize ( test ) results . append ( result )  target = [ [ <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> ] , [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] ] self . assertEqual ( results , target )  def test_word_tokenizer_french_base ( self ) :          word_tokenizer = WordTokenizer ( <str> ) tests = [ <str> ] results = [ ] for test in tests :              result = word_tokenizer . tokenize ( test ) results . append ( result )  target = [ [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] ] self . assertEqual ( results , target )  def test_word_tokenizer_french ( self ) :          word_tokenizer = OldFrenchWordTokenizer ( ) tests = [ <str> ] results = [ ] for test in tests :              result = word_tokenizer . tokenize ( test ) results . append ( result )  target = [ [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] ] self . assertEqual ( results , target )  def test_old_norse_word_tokenizer_base ( self ) :          text = <str> <str> target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] word_tokenizer = WordTokenizer ( <str> ) result = word_tokenizer . tokenize ( text ) self . assertTrue ( result == target )  def test_old_norse_word_tokenizer ( self ) :          text = <str> <str> target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] word_tokenizer = OldNorseWordTokenizer ( ) result = word_tokenizer . tokenize ( text ) self . assertTrue ( result == target )  def test_middle_english_tokenizer_base ( self ) :          text = <str> target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] tokenizer = WordTokenizer ( <str> ) tokenized = tokenizer . tokenize ( text ) self . assertTrue ( tokenized == target )  def test_middle_english_tokenizer ( self ) :          text = <str> target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] tokenizer = MiddleEnglishWordTokenizer ( ) tokenized = tokenizer . tokenize ( text ) self . assertTrue ( tokenized == target )  def test_middle_high_german_tokenizer_base ( self ) :          text = <str> target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] tokenizer = WordTokenizer ( <str> ) tokenized_lines = tokenizer . tokenize ( text ) self . assertTrue ( tokenized_lines == target )  def test_middle_high_german_tokenizer ( self ) :          text = <str> target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] tokenizer = MiddleHighGermanWordTokenizer ( ) tokenized_lines = tokenizer . tokenize ( text ) self . assertTrue ( tokenized_lines == target )  def test_akkadian_word_tokenizer ( self ) :          tokenizer = WordTokenizer ( <str> ) line = <str> output = tokenizer . tokenize ( line ) goal = [ ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) ] self . assertEqual ( output , goal )  def test_akkadian_sign_tokenizer ( self ) :          tokenizer = WordTokenizer ( <str> ) word = ( <str> , <str> ) output = tokenizer . tokenize_sign ( word ) goal = [ ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) , ( <str> , <str> ) ] self . assertEqual ( output , goal )  def test_sanskrit_word_tokenizer ( self ) :          text = <str> target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] tokenizer = SanskritWordTokenizer ( ) tokens = tokenizer . tokenize ( text ) self . assertEqual ( tokens , target )   class TestLineTokenize ( unittest . TestCase ) :      def test_line_tokenizer ( self ) :          text = target = [ <str> , <str> ] tokenizer = LineTokenizer ( <str> ) tokenized_lines = tokenizer . tokenize ( text ) self . assertTrue ( tokenized_lines == target )  def test_line_tokenizer_include_blanks ( self ) :          text = target = [ <str> , <str> , <str> , <str> , <str> ] tokenizer = LineTokenizer ( <str> ) tokenized_lines = tokenizer . tokenize ( text , include_blanks = True ) self . assertTrue ( tokenized_lines == target )  def test_french_line_tokenizer ( self ) :          text = target = [ <str> , <str> , <str> , <str> ] tokenizer = LineTokenizer ( <str> ) tokenized_lines = tokenizer . tokenize ( text ) self . assertTrue ( tokenized_lines == target )  def test_french_line_tokenizer_include_blanks ( self ) :          text = target = [ <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ] tokenizer = LineTokenizer ( <str> ) tokenized_lines = tokenizer . tokenize ( text , include_blanks = True ) self . assertTrue ( tokenized_lines == target )   class TestSentenceTokenizeUtils ( unittest . TestCase ) :      @ classmethod def setUpClass ( self ) :          self . latin_text = <str>  def test_sentence_tokenizer_utils ( self ) :          trainer = BaseSentenceTokenizerTrainer ( <str> ) self . assertIsInstance ( trainer . train_sentence_tokenizer ( self . latin_text ) , PunktSentenceTokenizer )  def test_sentence_tokenizer_utils_with_punctuation ( self ) :          trainer = BaseSentenceTokenizerTrainer ( <str> , punctuation = [ <str> , <str> , <str> ] ) self . assertIsInstance ( trainer . train_sentence_tokenizer ( self . latin_text ) , PunktSentenceTokenizer )  def test_sentence_tokenizer_utils_with_abbreviations ( self ) :          trainer = BaseSentenceTokenizerTrainer ( <str> , abbreviations = [ <str> ] ) self . assertIsInstance ( trainer . train_sentence_tokenizer ( self . latin_text ) , PunktSentenceTokenizer )  def test_sentence_tokenizer_utils_with_strict ( self ) :          trainer = BaseSentenceTokenizerTrainer ( <str> , strict = True , punctuation = [ <str> , <str> , <str> ] , strict_punctuation = [ <str> ] ) self . assertIsInstance ( trainer . train_sentence_tokenizer ( self . latin_text ) , PunktSentenceTokenizer )  def test_sentence_tokenizer_trainer_pickle ( self ) :          with patch . object ( BaseSentenceTokenizerTrainer , <str> ) as mock :              trainer = BaseSentenceTokenizerTrainer ( <str> ) trainer . pickle_sentence_tokenizer ( <str> , trainer )  mock . assert_called_once_with ( <str> , trainer )  def test_sentence_tokenizer_utils_latin ( self ) :          trainer = LatinSentenceTokenizerTrainer ( ) self . assertIsInstance ( trainer . train_sentence_tokenizer ( self . latin_text ) , PunktSentenceTokenizer )   if __name__ == <str> :      unittest . main ( )   