import numpy as np import torch from torch import nn import odl from odl . contrib import torch as odl_torch from odl . util . testutils import all_almost_equal , simple_fixture dtype = simple_fixture ( <str> , [ <str> , <str> ] ) device_params = [ <str> ] if torch . cuda . is_available ( ) :      device_params . append ( <str> )  device = simple_fixture ( <str> , device_params ) shape = simple_fixture ( <str> , [ ( 3 , ) , ( 2 , 3 ) , ( 2 , 2 , 3 ) ] ) def test_autograd_function_forward ( dtype , device ) :      matrix = np . random . rand ( 2 , 3 ) . astype ( dtype ) odl_op = odl . MatrixOperator ( matrix ) x_arr = np . ones ( 3 , dtype = dtype ) x = torch . from_numpy ( x_arr ) . to ( device ) res = odl_torch . OperatorFunction . apply ( odl_op , x ) res_arr = res . detach ( ) . cpu ( ) . numpy ( ) odl_res = odl_op ( x_arr ) assert res_arr . dtype == dtype assert all_almost_equal ( res_arr , odl_res ) assert x . device . type == res . device . type == device  def test_autograd_function_backward ( dtype , device ) :      matrix = np . random . rand ( 2 , 3 ) . astype ( dtype ) odl_op = odl . MatrixOperator ( matrix ) odl_cost = odl . solvers . L2NormSquared ( odl_op . range ) odl_functional = odl_cost * odl_op x_arr = np . ones ( 3 , dtype = dtype ) x = torch . from_numpy ( x_arr ) . to ( device ) x . requires_grad_ ( True ) y = odl_torch . OperatorFunction . apply ( odl_op , x ) res = odl_torch . OperatorFunction . apply ( odl_cost , y ) res . backward ( ) grad = x . grad grad_arr = grad . detach ( ) . cpu ( ) . numpy ( ) odl_grad = odl_functional . gradient ( x_arr ) assert grad_arr . dtype == dtype assert all_almost_equal ( grad_arr , odl_grad ) assert x . device . type == grad . device . type == device  def test_module_forward ( shape , device ) :      ndim = len ( shape ) space = odl . uniform_discr ( [ 0 ] * ndim , shape , shape , dtype = <str> ) odl_op = odl . ScalingOperator ( space , 2 ) op_mod = odl_torch . OperatorModule ( odl_op ) x_arr = np . ones ( shape , dtype = <str> ) x = torch . from_numpy ( x_arr ) . to ( device ) [ None , ... ] x . requires_grad_ ( True ) res = op_mod ( x ) res_arr = res . detach ( ) . cpu ( ) . numpy ( ) assert res_arr . shape == ( 1 , ) + odl_op . range . shape assert all_almost_equal ( res_arr , np . asarray ( odl_op ( x_arr ) ) [ None , ... ] ) assert x . device . type == res . device . type == device x = torch . from_numpy ( x_arr ) . to ( device ) [ None , None , ... ] x . requires_grad_ ( True ) res = op_mod ( x ) res_arr = res . detach ( ) . cpu ( ) . numpy ( ) assert res_arr . shape == ( 1 , 1 ) + odl_op . range . shape assert all_almost_equal ( res_arr , np . asarray ( odl_op ( x_arr ) ) [ None , None , ... ] ) assert x . device . type == res . device . type == device  def test_module_forward_diff_shapes ( device ) :      matrix = np . random . rand ( 2 , 3 ) . astype ( <str> ) odl_op = odl . MatrixOperator ( matrix ) op_mod = odl_torch . OperatorModule ( odl_op ) x_arr = np . ones ( 3 , dtype = <str> ) x = torch . from_numpy ( x_arr ) . to ( device ) [ None , ... ] x . requires_grad_ ( True ) res = op_mod ( x ) res_arr = res . detach ( ) . cpu ( ) . numpy ( ) assert res_arr . shape == ( 1 , ) + odl_op . range . shape assert all_almost_equal ( res_arr , np . asarray ( odl_op ( x_arr ) ) [ None , ... ] ) assert x . device . type == res . device . type == device x = torch . from_numpy ( x_arr ) . to ( device ) [ None , None , ... ] x . requires_grad_ ( True ) res = op_mod ( x ) res_arr = res . detach ( ) . cpu ( ) . numpy ( ) assert res_arr . shape == ( 1 , 1 ) + odl_op . range . shape assert all_almost_equal ( res_arr , np . asarray ( odl_op ( x_arr ) ) [ None , None , ... ] ) assert x . device . type == res . device . type == device  def test_module_backward ( device ) :      matrix = np . random . rand ( 2 , 3 ) . astype ( <str> ) odl_op = odl . MatrixOperator ( matrix ) op_mod = odl_torch . OperatorModule ( odl_op ) loss_fn = nn . MSELoss ( ) layer_before = nn . Linear ( 3 , 3 ) layer_after = nn . Linear ( 2 , 2 ) model = nn . Sequential ( layer_before , op_mod , layer_after ) . to ( device ) x = torch . from_numpy ( np . ones ( 3 , dtype = <str> ) ) [ None , ... ] . to ( device ) x . requires_grad_ ( True ) target = torch . from_numpy ( np . zeros ( 2 , dtype = <str> ) ) [ None , ... ] . to ( device ) loss = loss_fn ( model ( x ) , target ) loss . backward ( ) assert all ( p is not None for p in model . parameters ( ) ) assert x . grad . detach ( ) . cpu ( ) . abs ( ) . sum ( ) != 0 assert x . device . type == loss . device . type == device layer_before = nn . Conv1d ( 1 , 2 , 2 ) layer_after = nn . Conv1d ( 2 , 1 , 2 ) model = nn . Sequential ( layer_before , op_mod , layer_after ) . to ( device ) x = torch . from_numpy ( np . ones ( 4 , dtype = <str> ) ) [ None , None , ... ] . to ( device ) x . requires_grad_ ( True ) target = torch . from_numpy ( np . zeros ( 1 , dtype = <str> ) ) [ None , None , ... ] . to ( device ) loss = loss_fn ( model ( x ) , target ) loss . backward ( ) assert all ( p is not None for p in model . parameters ( ) ) assert x . grad . detach ( ) . cpu ( ) . abs ( ) . sum ( ) != 0 assert x . device . type == loss . device . type == device  if __name__ == <str> :      odl . util . test_file ( __file__ )   