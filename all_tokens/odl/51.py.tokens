from __future__ import print_function , division , absolute_import import numpy as np from odl . solvers . functional . functional import Functional from odl . operator import Operator from odl . space . base_tensors import TensorSpace __all__ = ( <str> , <str> , ) class NumericalDerivative ( Operator ) :      def __init__ ( self , operator , point , method = <str> , step = None ) :          <str> if not isinstance ( operator , Operator ) :              raise TypeError ( <str> )  if not isinstance ( operator . domain , TensorSpace ) :              raise TypeError ( <str> <str> )  if not isinstance ( operator . range , TensorSpace ) :              raise TypeError ( <str> <str> )  self . operator = operator self . point = operator . domain . element ( point ) if step is None :              self . step = np . sqrt ( np . finfo ( operator . domain . dtype ) . eps )  else :              self . step = float ( step )  self . method , method_in = str ( method ) . lower ( ) , method if self . method not in ( <str> , <str> , <str> ) :              raise ValueError ( <str> ) . format ( method_in )  super ( NumericalDerivative , self ) . __init__ ( operator . domain , operator . range , linear = True )  def _call ( self , dx ) :          x = self . point dx_norm = dx . norm ( ) if dx_norm == 0 :              return 0  scaled_dx = dx * ( self . step / dx_norm ) if self . method == <str> :              dAdx = self . operator ( x ) - self . operator ( x - scaled_dx )  elif self . method == <str> :              dAdx = self . operator ( x + scaled_dx ) - self . operator ( x )  elif self . method == <str> :              dAdx = ( self . operator ( x + scaled_dx / 2 ) - self . operator ( x - scaled_dx / 2 ) )  else :              raise RuntimeError ( <str> )  return dAdx * ( dx_norm / self . step )   class NumericalGradient ( Operator ) :      def __init__ ( self , functional , method = <str> , step = None ) :          if not isinstance ( functional , Functional ) :              raise TypeError ( <str> )  if not isinstance ( functional . domain , TensorSpace ) :              raise TypeError ( <str> <str> )  self . functional = functional if step is None :              self . step = np . sqrt ( np . finfo ( functional . domain . dtype ) . eps )  else :              self . step = float ( step )  self . method , method_in = str ( method ) . lower ( ) , method if self . method not in ( <str> , <str> , <str> ) :              raise ValueError ( <str> ) . format ( method_in )  super ( NumericalGradient , self ) . __init__ ( functional . domain , functional . domain , linear = functional . is_linear )  def _call ( self , x ) :          dfdx = self . domain . zero ( ) dx = self . domain . zero ( ) if self . method == <str> :              fx = self . functional ( x ) for i in range ( self . domain . size ) :                  dx [ i - 1 ] = 0 dx [ i ] = self . step dfdx [ i ] = fx - self . functional ( x - dx )   elif self . method == <str> :              fx = self . functional ( x ) for i in range ( self . domain . size ) :                  dx [ i - 1 ] = 0 dx [ i ] = self . step dfdx [ i ] = self . functional ( x + dx ) - fx   elif self . method == <str> :              for i in range ( self . domain . size ) :                  dx [ i - 1 ] = 0 dx [ i ] = self . step / 2 dfdx [ i ] = self . functional ( x + dx ) - self . functional ( x - dx )   else :              raise RuntimeError ( <str> )  dfdx /= self . step return dfdx  def derivative ( self , point ) :          return NumericalDerivative ( self , point , method = self . method , step = np . sqrt ( self . step ) )   if __name__ == <str> :      from odl . util . testutils import run_doctests run_doctests ( )   