import numpy as np from scipy import stats from jaratoolbox import spikesanalysis from jaratoolbox import behavioranalysis AVERAGE_JITTER = { <str> : 0.0093 , <str> : 0.0094 , <str> : 0.0095 , <str> : 0.0091 , <str> : 0.0091 } def get_sound_onset_times ( ephysData , sessionType ) :      eventOnsetTimes = ephysData [ <str> ] [ <str> ] if len ( eventOnsetTimes ) == 0 :          eventOnsetTimes = ephysData [ <str> ] [ <str> ] + AVERAGE_JITTER [ sessionType ]  eventOnsetTimes = spikesanalysis . minimum_event_onset_diff ( eventOnsetTimes , minEventOnsetDiff = 0.2 ) return eventOnsetTimes  def laser_response ( ephysData , baseRange = [ - 0.05 , - 0.04 ] , responseRange = [ 0.0 , 0.01 ] ) :      fullTimeRange = [ baseRange [ 0 ] , responseRange [ 1 ] ] eventOnsetTimes = ephysData [ <str> ] [ <str> ] eventOnsetTimes = spikesanalysis . minimum_event_onset_diff ( eventOnsetTimes , minEventOnsetDiff = 0.5 ) spikeTimestamps = ephysData [ <str> ] spikeTimesFromEventOnset , trialIndexForEachSpike , indexLimitsEachTrial = spikesanalysis . eventlocked_spiketimes ( spikeTimestamps , eventOnsetTimes , fullTimeRange ) baseSpikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , baseRange ) laserSpikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , responseRange ) [ testStatistic , pVal ] = stats . ranksums ( laserSpikeCountMat , baseSpikeCountMat ) laserChangeFR = np . mean ( laserSpikeCountMat ) / ( responseRange [ 1 ] - responseRange [ 0 ] ) - np . mean ( baseSpikeCountMat ) / ( baseRange [ 1 ] - baseRange [ 0 ] ) return testStatistic , pVal , laserChangeFR  def sound_response_any_stimulus ( eventOnsetTimes , spikeTimeStamps , trialsEachCond , timeRange = [ 0.0 , 1.0 ] , baseRange = [ - 1.1 , - 0.1 ] ) :      fullTimeRange = [ min ( min ( timeRange ) , min ( baseRange ) ) , max ( max ( timeRange ) , max ( baseRange ) ) ] spikeTimesFromEventOnset , trialIndexForEachSpike , indexLimitsEachTrial = spikesanalysis . eventlocked_spiketimes ( spikeTimeStamps , eventOnsetTimes , fullTimeRange ) stimSpikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , timeRange ) baseSpikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , baseRange ) minpVal = np . inf maxzscore = - np . inf for cond in range ( trialsEachCond . shape [ 1 ] ) :          trialsThisCond = trialsEachCond [ : , cond ] if stimSpikeCountMat . shape [ 0 ] == len ( trialsThisCond ) + 1 :              stimSpikeCountMat = stimSpikeCountMat [ : - 1 , : ] baseSpikeCountMat = baseSpikeCountMat [ : - 1 , : ]  if any ( trialsThisCond ) :              thisFirstStimCounts = stimSpikeCountMat [ trialsThisCond ] . flatten ( ) thisStimBaseSpikeCouns = baseSpikeCountMat [ trialsThisCond ] . flatten ( ) thiszscore , pValThisFirst = stats . ranksums ( thisFirstStimCounts , thisStimBaseSpikeCouns ) if pValThisFirst < minpVal :                  minpVal = pValThisFirst  if thiszscore > maxzscore :                  maxzscore = thiszscore    return maxzscore , minpVal  def best_window_freq_tuning ( spikeTimesFromEventOnset , indexLimitsEachTrial , trialsEachFreq , windowsToTry = [ [ 0.0 , 0.1 ] , [ 0.0 , 0.05 ] , [ 0.1 , 0.15 ] ] ) :      zscores = np . zeros ( ( len ( windowsToTry ) , trialsEachFreq . shape [ 1 ] ) ) for ind , window in enumerate ( windowsToTry ) :          duration = window [ 1 ] - window [ 0 ] baseTimeRange = [ - 0.1 - duration , - 0.1 ] spikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , window ) baseSpikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , baseTimeRange ) for ind2 in range ( trialsEachFreq . shape [ 1 ] ) :              trialsThisFreq = trialsEachFreq [ : , ind2 ] if spikeCountMat . shape [ 0 ] == len ( trialsThisFreq ) + 1 :                  spikeCountMat = spikeCountMat [ : - 1 , : ] baseSpikeCountMat = baseSpikeCountMat [ : - 1 , : ]  spikeCountsThisFreq = spikeCountMat [ trialsThisFreq ] baseCountsThisFreq = baseSpikeCountMat [ trialsThisFreq ] zScore , pVal = stats . ranksums ( spikeCountsThisFreq , baseCountsThisFreq ) zscores [ ind , ind2 ] = zScore   maxInd = np . unravel_index ( zscores . argmax ( ) , zscores . shape ) windowToUse = windowsToTry [ maxInd [ 0 ] ] return windowToUse  def gaussian_tuning_fit ( stimArray , responseArray ) :      from scipy . optimize import curve_fit try :          maxInd = np . argmax ( responseArray ) p0 = [ stimArray [ maxInd ] , responseArray [ maxInd ] , 1. , 0. ] curveFit = curve_fit ( gaussian , stimArray , responseArray , p0 = p0 , maxfev = 10000 ) [ 0 ]  except RuntimeError :          print <str> return None , None  fitResponseArray = gaussian ( stimArray , curveFit [ 0 ] , curveFit [ 1 ] , curveFit [ 2 ] , curveFit [ 3 ] ) residuals = responseArray - fitResponseArray SSresidual = np . sum ( residuals ** 2 ) SStotal = np . sum ( ( responseArray - np . mean ( responseArray ) ) ** 2 ) Rsquared = 1 - ( SSresidual / SStotal ) return curveFit , Rsquared  def gaussian ( x , mu , amp , sigma , offset ) :      return offset + amp * np . exp ( - ( ( x - mu ) / sigma ) ** 2 )  def best_index ( cellObj , bestFreq , behavType = <str> ) :      behavIndex = cellObj . get_session_inds ( behavType ) charFreqs = [ ] for ind in behavIndex :          bdata = cellObj . load_behavior_by_index ( ind ) charFreq = np . unique ( bdata [ <str> ] ) [ 0 ] charFreqs . append ( charFreq )  if bestFreq is not None and len ( charFreqs ) > 0 :          octaveDiff = np . zeros ( len ( charFreqs ) ) for ind , charFreq in enumerate ( charFreqs ) :              octaveDiff [ ind ] = np . log2 ( bestFreq / charFreq )  octaveDiff = np . abs ( octaveDiff ) bestBehavIndex = behavIndex [ np . argmin ( octaveDiff ) ] octavesFromBest = min ( octaveDiff )  else :          bestBehavIndex = None octavesFromBest = None  return bestBehavIndex , octavesFromBest  def calculate_tuning_curve_inputs ( spikeCountEachTrial , firstSort , secondSort ) :      numFirst = np . unique ( firstSort ) numSec = np . unique ( secondSort ) trialsEachCond = behavioranalysis . find_trials_each_combination ( firstSort , numFirst , secondSort , numSec ) spikeArray = np . zeros ( ( len ( numFirst ) , len ( numSec ) ) ) errorArray = np . zeros_like ( spikeArray ) for sec in range ( len ( numSec ) ) :          trialsThisSec = trialsEachCond [ : , : , sec ] for first in range ( len ( numFirst ) ) :              trialsThisFirst = trialsThisSec [ : , first ] if spikeCountEachTrial . shape [ 0 ] != len ( trialsThisFirst ) :                  spikeCountEachTrial = spikeCountEachTrial [ : - 1 , : ]  if any ( trialsThisFirst ) :                  thisFirstCounts = spikeCountEachTrial [ trialsThisFirst ] . flatten ( ) spikeArray [ first , sec ] = np . mean ( thisFirstCounts ) errorArray [ first , sec ] = stats . sem ( thisFirstCounts )  else :                  spikeArray [ first , sec ] = np . nan errorArray [ first , sec ] = np . nan    return spikeArray , errorArray  def inactivated_cells_baselines ( spikeTimeStamps , eventOnsetTimes , laserEachTrial , baselineRange = [ - 0.05 , 0.0 ] ) :      bandSpikeTimesFromEventOnset , trialIndexForEachSpike , bandIndexLimitsEachTrial = spikesanalysis . eventlocked_spiketimes ( spikeTimeStamps , eventOnsetTimes , baselineRange ) numLaser = np . unique ( laserEachTrial ) baselineDuration = baselineRange [ 1 ] - baselineRange [ 0 ] baselineSpikeRates = np . zeros ( len ( numLaser ) ) baselineSEMs = np . zeros_like ( baselineSpikeRates ) trialsEachLaser = behavioranalysis . find_trials_each_type ( laserEachTrial , numLaser ) baselineSpikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( bandSpikeTimesFromEventOnset , bandIndexLimitsEachTrial , baselineRange ) for las in range ( len ( numLaser ) ) :          trialsThisLaser = trialsEachLaser [ : , las ] if baselineSpikeCountMat . shape [ 0 ] != len ( trialsThisLaser ) :              baselineSpikeCountMat = baselineSpikeCountMat [ : - 1 , : ]  baselineCounts = baselineSpikeCountMat [ trialsThisLaser ] . flatten ( ) baselineMean = np . mean ( baselineCounts ) / baselineDuration baselineSEM = stats . sem ( baselineCounts ) / baselineDuration baselineSpikeRates [ las ] = baselineMean baselineSEMs [ las ] = baselineSEM  return baselineSpikeRates , baselineSEMs  def bandwidth_suppression_from_peak ( spikeTimeStamps , eventOnsetTimes , firstSort , secondSort , timeRange = [ 0.2 , 1.0 ] , baseRange = [ - 1.0 , - 0.2 ] , subtractBaseline = False , zeroBWBaseline = True ) :      fullTimeRange = [ baseRange [ 0 ] , timeRange [ 1 ] ] trialsEachCond = behavioranalysis . find_trials_each_combination ( firstSort , np . unique ( firstSort ) , secondSort , np . unique ( secondSort ) ) spikeTimesFromEventOnset , trialIndexForEachSpike , indexLimitsEachTrial = spikesanalysis . eventlocked_spiketimes ( spikeTimeStamps , eventOnsetTimes , fullTimeRange ) spikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , timeRange ) baseSpikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , baseRange ) trialsEachSecondSort = behavioranalysis . find_trials_each_type ( secondSort , np . unique ( secondSort ) ) spikeArray , errorArray = calculate_tuning_curve_inputs ( spikeCountMat , firstSort , secondSort ) spikeArray = spikeArray / ( timeRange [ 1 ] - timeRange [ 0 ] ) suppressionIndex = np . zeros ( spikeArray . shape [ 1 ] ) facilitationIndex = np . zeros_like ( suppressionIndex ) peakInds = np . zeros_like ( suppressionIndex ) suppressionpVal = np . zeros_like ( suppressionIndex ) facilitationpVal = np . zeros_like ( suppressionIndex ) for ind in range ( len ( suppressionIndex ) ) :          trialsThisSecondVal = trialsEachSecondSort [ : , ind ] if spikeCountMat . shape [ 0 ] != len ( trialsThisSecondVal ) :              spikeCountMat = spikeCountMat [ : - 1 , : ] baseSpikeCountMat = baseSpikeCountMat [ : - 1 , : ]  thisCondResponse = spikeArray [ : , ind ] thisCondBaseline = np . mean ( baseSpikeCountMat [ trialsThisSecondVal ] . flatten ( ) ) / ( baseRange [ 1 ] - baseRange [ 0 ] ) if zeroBWBaseline :              thisCondResponse [ 0 ] = thisCondBaseline  if not subtractBaseline :              thisCondBaseline = 0  spikeArray [ : , ind ] = thisCondResponse suppressionIndex [ ind ] = ( max ( thisCondResponse ) - thisCondResponse [ - 1 ] ) / ( max ( thisCondResponse ) - thisCondBaseline ) facilitationIndex [ ind ] = ( max ( thisCondResponse ) - thisCondResponse [ 0 ] ) / ( max ( thisCondResponse ) - thisCondBaseline ) peakInd = np . argmax ( thisCondResponse ) peakInds [ ind ] = peakInd fullTrialsThisSecondVal = trialsEachCond [ : , : , ind ] if zeroBWBaseline :              if peakInd == 0 :                  peakSpikeCounts = baseSpikeCountMat [ trialsThisSecondVal ] . flatten ( )  else :                  peakSpikeCounts = spikeCountMat [ fullTrialsThisSecondVal [ : , peakInd ] ] . flatten ( )  zeroBWSpikeCounts = baseSpikeCountMat [ trialsThisSecondVal ] . flatten ( )  else :              peakSpikeCounts = spikeCountMat [ fullTrialsThisSecondVal [ : , peakInd ] ] . flatten ( ) zeroBWSpikeCounts = spikeCountMat [ fullTrialsThisSecondVal [ : , 0 ] ] . flatten ( )  whiteNoiseSpikeCounts = spikeCountMat [ fullTrialsThisSecondVal [ : , - 1 ] ] . flatten ( ) suppressionpVal [ ind ] = stats . ranksums ( peakSpikeCounts , whiteNoiseSpikeCounts ) [ 1 ] facilitationpVal [ ind ] = stats . ranksums ( peakSpikeCounts , zeroBWSpikeCounts ) [ 1 ]  return suppressionIndex , suppressionpVal , facilitationIndex , facilitationpVal , peakInds , spikeArray  def onset_sustained_spike_proportion ( spikeTimeStamps , eventOnsetTimes , onsetTimeRange = [ 0.0 , 0.05 ] , sustainedTimeRange = [ 0.2 , 1.0 ] ) :      fullTimeRange = [ onsetTimeRange [ 0 ] , sustainedTimeRange [ 1 ] ] spikeTimesFromEventOnset , trialIndexForEachSpike , indexLimitsEachTrial = spikesanalysis . eventlocked_spiketimes ( spikeTimeStamps , eventOnsetTimes , fullTimeRange ) onsetSpikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , onsetTimeRange ) sustainedSpikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , sustainedTimeRange ) fullSpikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , fullTimeRange ) propOnset = 1.0 * sum ( onsetSpikeCountMat ) / sum ( fullSpikeCountMat ) propSustained = 1.0 * sum ( sustainedSpikeCountMat ) / sum ( fullSpikeCountMat ) return propOnset , propSustained    