import os import numpy as np import matplotlib . pyplot as plt import matplotlib . gridspec as gridspec from jaratoolbox import settings from jaratoolbox import extraplots from jaratoolbox import behavioranalysis from jaratoolbox import spikesanalysis from jaratoolbox import ephyscore from collections import Counter from scipy import stats import pandas as pd import figparams reload ( figparams ) from sklearn import metrics FIGNAME = <str> dataDir = os . path . join ( settings . FIGURES_DATA_PATH , figparams . STUDY_NAME , FIGNAME ) def calc_MI ( x , y , bins ) :      c_xy = np . histogram2d ( x , y , bins ) [ 0 ] mi = metrics . mutual_info_score ( None , None , contingency = c_xy ) return mi  def xtab ( cols , apply_wt = False ) :      if not all ( len ( col ) == len ( cols [ 0 ] ) for col in cols [ 1 : ] ) :          raise ValueError ( <str> )  if len ( cols ) == 0 :          raise TypeError ( <str> )  fnx1 = lambda q : len ( q . squeeze ( ) . shape ) if not all ( [ fnx1 ( col ) == 1 for col in cols ] ) :          raise ValueError ( <str> )  if apply_wt :          cols , wt = cols [ : - 1 ] , cols [ - 1 ]  else :          wt = 1  uniq_vals_all_cols , idx = zip ( * ( np . unique ( col , return_inverse = True ) for col in cols ) ) shape_xt = [ uniq_vals_col . size for uniq_vals_col in uniq_vals_all_cols ] dtype_xt = <str> if apply_wt else <str> xt = np . zeros ( shape_xt , dtype = dtype_xt ) np . add . at ( xt , idx , wt ) return uniq_vals_all_cols , xt  CASE = 1 if CASE == 0 :      exampleDataPath = os . path . join ( dataDir , <str> ) exampleData = np . load ( exampleDataPath ) exampleFreqEachTrial = exampleData [ <str> ] . item ( ) exampleSpikeTimes = exampleData [ <str> ] . item ( ) exampleTrialIndexForEachSpike = exampleData [ <str> ] . item ( ) exampleIndexLimitsEachTrial = exampleData [ <str> ] . item ( ) exampleNames = [ <str> , <str> , <str> , <str> ] plt . clf ( ) for exampleName in exampleNames :          spikeTimesFromEventOnset = exampleSpikeTimes [ exampleName ] trialIndexForEachSpike = exampleTrialIndexForEachSpike [ exampleName ] indexLimitsEachTrial = exampleIndexLimitsEachTrial [ exampleName ] freqEachTrial = exampleFreqEachTrial [ exampleName ] timeRange = [ 0.1 , 0.5 ] spikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , timeRange ) spikeCountEachTrial = spikeCountMat . flatten ( ) spikeCountEachTrial = spikeCountEachTrial [ : - 1 ] uv , xt = xtab ( [ spikeCountEachTrial , freqEachTrial ] ) mi = metrics . mutual_info_score ( None , None , contingency = xt ) print exampleName print <str> . format ( mi ) print <str> . format ( mi / np . mean ( spikeCountEachTrial ) )   if CASE == 1 :      dbPath = os . path . join ( settings . FIGURES_DATA_PATH , figparams . STUDY_NAME , <str> ) dataframe = pd . read_hdf ( dbPath , key = <str> ) for indIter , ( indRow , dbRow ) in enumerate ( dataframe . iterrows ( ) ) :          if not <str> in dbRow [ <str> ] :              dataframe . loc [ indRow , <str> ] = np . nan print <str> continue  cell = ephyscore . Cell ( dbRow , useModifiedClusters = True ) try :              ephysData , bdata = cell . load ( <str> )  except ( IndexError , ValueError ) :              failed = True print <str> . format ( indRow ) dataframe . loc [ indRow , <str> ] = np . nan continue  spikeTimes = ephysData [ <str> ] if len ( spikeTimes ) < 100 :              dataframe . loc [ indRow , <str> ] = np . nan print <str> continue  numFreq = len ( np . unique ( bdata [ <str> ] ) ) allFreqVS = np . empty ( numFreq ) allFreqRal = np . empty ( numFreq ) allFreqPval = np . empty ( numFreq ) eventOnsetTimes = ephysData [ <str> ] [ <str> ] eventOnsetTimes = spikesanalysis . minimum_event_onset_diff ( eventOnsetTimes , minEventOnsetDiff = 0.7 ) baseRange = [ - 0.5 , - 0.1 ] responseRange = [ 0.1 , 0.5 ] alignmentRange = [ baseRange [ 0 ] , responseRange [ 1 ] ] ( spikeTimesFromEventOnset , trialIndexForEachSpike , indexLimitsEachTrial ) = spikesanalysis . eventlocked_spiketimes ( spikeTimes , eventOnsetTimes , alignmentRange ) nspkBase = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , baseRange ) nspkResp = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , responseRange ) [ zScore , pVal ] = stats . ranksums ( nspkResp , nspkBase ) if pVal > 0.05 :              dataframe . loc [ indRow , <str> ] = np . nan print <str> continue  timeRange = [ 0 , 0.5 ] ( spikeTimesFromEventOnset , trialIndexForEachSpike , indexLimitsEachTrial ) = spikesanalysis . eventlocked_spiketimes ( spikeTimes , eventOnsetTimes , timeRange ) freqEachTrial = bdata [ <str> ] spikeCountMat = spikesanalysis . spiketimes_to_spikecounts ( spikeTimesFromEventOnset , indexLimitsEachTrial , timeRange ) spikeCountEachTrial = spikeCountMat . flatten ( ) if len ( freqEachTrial ) == len ( spikeCountEachTrial ) - 1 :              spikeCountEachTrial = spikeCountEachTrial [ : - 1 ]  uv , xt = xtab ( [ spikeCountEachTrial , freqEachTrial ] ) mutualInfo = metrics . mutual_info_score ( None , None , contingency = xt ) randomMIs = np . empty ( 500 ) for randomIter in range ( 500 ) :              randomFreqs = np . random . permutation ( freqEachTrial ) uv , xt = xtab ( [ spikeCountEachTrial , randomFreqs ] ) mutualInfoRandom = metrics . mutual_info_score ( None , None , contingency = xt ) randomMIs [ randomIter ] = mutualInfoRandom  mutualInfoBC = mutualInfo - np . mean ( randomMIs ) mutualInfoBCBits = mutualInfoBC / np . log ( 2 ) mutualInfoPerSpike = mutualInfoBC / np . mean ( spikeCountEachTrial ) mutualInfoPerSpikeBits = mutualInfoBCBits / np . mean ( spikeCountEachTrial ) dataframe . loc [ indRow , <str> ] = mutualInfoBCBits dataframe . loc [ indRow , <str> ] = mutualInfoPerSpikeBits   dataframe . to_hdf ( dbPath , <str> ) print <str> . format ( dbPath )  