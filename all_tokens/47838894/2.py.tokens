import array import ast import bisect import time from collections import defaultdict from functools import partial import electrumx . lib . util as util from electrumx . lib . util import pack_be_uint16 , unpack_be_uint16_from from electrumx . lib . hash import hash_to_hex_str , HASHX_LEN class History ( object ) :      DB_VERSIONS = [ 0 ] def __init__ ( self ) :          self . logger = util . class_logger ( __name__ , self . __class__ . __name__ ) self . max_hist_row_entries = 12500 self . unflushed = defaultdict ( partial ( array . array , <str> ) ) self . unflushed_count = 0 self . flush_count = 0 self . comp_flush_count = - 1 self . comp_cursor = - 1 self . db_version = max ( self . DB_VERSIONS ) self . db = None  def open_db ( self , db_class , for_sync , utxo_flush_count , compacting ) :          self . db = db_class ( <str> , for_sync ) self . read_state ( ) self . clear_excess ( utxo_flush_count ) if not compacting :              self . _cancel_compaction ( )  return self . flush_count  def close_db ( self ) :          if self . db :              self . db . close ( ) self . db = None   def read_state ( self ) :          state = self . db . get ( <str> ) if state :              state = ast . literal_eval ( state . decode ( ) ) if not isinstance ( state , dict ) :                  raise RuntimeError ( <str> )  self . flush_count = state [ <str> ] self . comp_flush_count = state . get ( <str> , - 1 ) self . comp_cursor = state . get ( <str> , - 1 ) self . db_version = state . get ( <str> , 0 )  else :              self . flush_count = 0 self . comp_flush_count = - 1 self . comp_cursor = - 1 self . db_version = max ( self . DB_VERSIONS )  self . logger . info ( <str> ) if self . db_version not in self . DB_VERSIONS :              msg = <str> self . logger . error ( msg ) raise RuntimeError ( msg )  self . logger . info ( <str> )  def clear_excess ( self , utxo_flush_count ) :          if self . flush_count <= utxo_flush_count :              return  self . logger . info ( <str> <str> ) keys = [ ] for key , _hist in self . db . iterator ( prefix = <str> ) :              flush_id , = unpack_be_uint16_from ( key [ - 2 : ] ) if flush_id > utxo_flush_count :                  keys . append ( key )   self . logger . info ( <str> ) self . flush_count = utxo_flush_count with self . db . write_batch ( ) as batch :              for key in keys :                  batch . delete ( key )  self . write_state ( batch )  self . logger . info ( <str> )  def write_state ( self , batch ) :          state = { <str> : self . flush_count , <str> : self . comp_flush_count , <str> : self . comp_cursor , <str> : self . db_version , } batch . put ( <str> , repr ( state ) . encode ( ) )  def add_unflushed ( self , hashXs_by_tx , first_tx_num ) :          unflushed = self . unflushed count = 0 for tx_num , hashXs in enumerate ( hashXs_by_tx , start = first_tx_num ) :              hashXs = set ( hashXs ) for hashX in hashXs :                  unflushed [ hashX ] . append ( tx_num )  count += len ( hashXs )  self . unflushed_count += count  def unflushed_memsize ( self ) :          return len ( self . unflushed ) * 180 + self . unflushed_count * 4  def assert_flushed ( self ) :          assert not self . unflushed  def flush ( self ) :          start_time = time . time ( ) self . flush_count += 1 flush_id = pack_be_uint16 ( self . flush_count ) unflushed = self . unflushed with self . db . write_batch ( ) as batch :              for hashX in sorted ( unflushed ) :                  key = hashX + flush_id batch . put ( key , unflushed [ hashX ] . tobytes ( ) )  self . write_state ( batch )  count = len ( unflushed ) unflushed . clear ( ) self . unflushed_count = 0 if self . db . for_sync :              elapsed = time . time ( ) - start_time self . logger . info ( <str> <str> )   def backup ( self , hashXs , tx_count ) :          self . flush_count += 1 nremoves = 0 bisect_left = bisect . bisect_left with self . db . write_batch ( ) as batch :              for hashX in sorted ( hashXs ) :                  deletes = [ ] puts = { } for key , hist in self . db . iterator ( prefix = hashX , reverse = True ) :                      a = array . array ( <str> ) a . frombytes ( hist ) idx = bisect_left ( a , tx_count ) nremoves += len ( a ) - idx if idx > 0 :                          puts [ key ] = a [ : idx ] . tobytes ( ) break  deletes . append ( key )  for key in deletes :                      batch . delete ( key )  for key , value in puts . items ( ) :                      batch . put ( key , value )   self . write_state ( batch )  self . logger . info ( <str> )  def get_txnums ( self , hashX , limit = 1000 ) :          limit = util . resolve_limit ( limit ) for _key , hist in self . db . iterator ( prefix = hashX ) :              a = array . array ( <str> ) a . frombytes ( hist ) for tx_num in a :                  if limit == 0 :                      return  yield tx_num limit -= 1    def _flush_compaction ( self , cursor , write_items , keys_to_delete ) :          if cursor == 65536 :              self . flush_count = self . comp_flush_count self . comp_cursor = - 1 self . comp_flush_count = - 1  else :              self . comp_cursor = cursor  with self . db . write_batch ( ) as batch :              for key in keys_to_delete :                  batch . delete ( key )  for key , value in write_items :                  batch . put ( key , value )  self . write_state ( batch )   def _compact_hashX ( self , hashX , hist_map , hist_list , write_items , keys_to_delete ) :          max_row_size = self . max_hist_row_entries * 4 full_hist = <str> . join ( hist_list ) nrows = ( len ( full_hist ) + max_row_size - 1 ) // max_row_size if nrows > 4 :              self . logger . info ( <str> <str> . format ( hash_to_hex_str ( hashX ) , len ( full_hist ) // 4 , nrows ) )  write_size = 0 keys_to_delete . update ( hist_map ) for n , chunk in enumerate ( util . chunks ( full_hist , max_row_size ) ) :              key = hashX + pack_be_uint16 ( n ) if hist_map . get ( key ) == chunk :                  keys_to_delete . remove ( key )  else :                  write_items . append ( ( key , chunk ) ) write_size += len ( chunk )   assert n + 1 == nrows self . comp_flush_count = max ( self . comp_flush_count , n ) return write_size  def _compact_prefix ( self , prefix , write_items , keys_to_delete ) :          prior_hashX = None hist_map = { } hist_list = [ ] key_len = HASHX_LEN + 2 write_size = 0 for key , hist in self . db . iterator ( prefix = prefix ) :              if len ( key ) != key_len :                  continue  hashX = key [ : - 2 ] if hashX != prior_hashX and prior_hashX :                  write_size += self . _compact_hashX ( prior_hashX , hist_map , hist_list , write_items , keys_to_delete ) hist_map . clear ( ) hist_list . clear ( )  prior_hashX = hashX hist_map [ key ] = hist hist_list . append ( hist )  if prior_hashX :              write_size += self . _compact_hashX ( prior_hashX , hist_map , hist_list , write_items , keys_to_delete )  return write_size  def _compact_history ( self , limit ) :          keys_to_delete = set ( ) write_items = [ ] write_size = 0 cursor = self . comp_cursor while write_size < limit and cursor < 65536 :              prefix = pack_be_uint16 ( cursor ) write_size += self . _compact_prefix ( prefix , write_items , keys_to_delete ) cursor += 1  max_rows = self . comp_flush_count + 1 self . _flush_compaction ( cursor , write_items , keys_to_delete ) self . logger . info ( <str> <str> . format ( len ( write_items ) , write_size / 1000000 , len ( keys_to_delete ) , max_rows , 100 * cursor / 65536 ) ) return write_size  def _cancel_compaction ( self ) :          if self . comp_cursor != - 1 :              self . logger . warning ( <str> ) self . comp_flush_count = - 1 self . comp_cursor = - 1     