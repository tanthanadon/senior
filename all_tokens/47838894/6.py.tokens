import asyncio from struct import pack , unpack import time from aiorpcx import TaskGroup , run_in_thread import electrumx from electrumx . server . daemon import DaemonError from electrumx . lib . hash import hash_to_hex_str , HASHX_LEN from electrumx . lib . util import chunks , class_logger from electrumx . server . db import FlushData class Prefetcher ( object ) :      def __init__ ( self , daemon , coin , blocks_event ) :          self . logger = class_logger ( __name__ , self . __class__ . __name__ ) self . daemon = daemon self . coin = coin self . blocks_event = blocks_event self . blocks = [ ] self . caught_up = False self . fetched_height = None self . semaphore = asyncio . Semaphore ( ) self . refill_event = asyncio . Event ( ) self . cache_size = 0 self . min_cache_size = 10 * 1024 * 1024 self . ave_size = self . min_cache_size // 10 self . polling_delay = 5  async def main_loop ( self , bp_height ) :          await self . reset_height ( bp_height ) while True :              try :                  await self . refill_event . wait ( ) if not await self . _prefetch_blocks ( ) :                      await asyncio . sleep ( self . polling_delay )   except DaemonError as e :                  self . logger . info ( <str> )  except asyncio . CancelledError as e :                  self . logger . info ( <str> ) raise  except Exception :                  self . logger . exception ( <str> )    def get_prefetched_blocks ( self ) :          blocks = self . blocks self . blocks = [ ] self . cache_size = 0 self . refill_event . set ( ) return blocks  async def reset_height ( self , height ) :          async with self . semaphore :              self . blocks . clear ( ) self . cache_size = 0 self . fetched_height = height self . refill_event . set ( )  daemon_height = await self . daemon . height ( ) behind = daemon_height - height if behind > 0 :              self . logger . info ( <str> <str> . format ( daemon_height , behind ) )  else :              self . logger . info ( <str> . format ( daemon_height ) )   async def _prefetch_blocks ( self ) :          daemon = self . daemon daemon_height = await daemon . height ( ) async with self . semaphore :              while self . cache_size < self . min_cache_size :                  cache_room = max ( self . min_cache_size // self . ave_size , 1 ) count = min ( daemon_height - self . fetched_height , cache_room ) count = min ( 100 , max ( count , 0 ) ) if not count :                      self . caught_up = True return False  first = self . fetched_height + 1 hex_hashes = await daemon . block_hex_hashes ( first , count ) if self . caught_up :                      self . logger . info ( <str> . format ( first + count - 1 , hex_hashes [ - 1 ] ) )  blocks = await daemon . raw_blocks ( hex_hashes ) assert count == len ( blocks ) if first == 0 :                      blocks [ 0 ] = self . coin . genesis_block ( blocks [ 0 ] ) self . logger . info ( <str> . format ( hex_hashes [ 0 ] ) )  size = sum ( len ( block ) for block in blocks ) if count >= 10 :                      self . ave_size = size // count  else :                      self . ave_size = ( size + ( 10 - count ) * self . ave_size ) // 10  self . blocks . extend ( blocks ) self . cache_size += size self . fetched_height += count self . blocks_event . set ( )   self . refill_event . clear ( ) return True   class ChainError ( Exception ) :       class BlockProcessor ( object ) :      def __init__ ( self , env , db , daemon , notifications ) :          self . env = env self . db = db self . daemon = daemon self . notifications = notifications self . coin = env . coin self . blocks_event = asyncio . Event ( ) self . prefetcher = Prefetcher ( daemon , env . coin , self . blocks_event ) self . logger = class_logger ( __name__ , self . __class__ . __name__ ) self . next_cache_check = 0 self . touched = set ( ) self . reorg_count = 0 self . height = - 1 self . tip = None self . tx_count = 0 self . _caught_up_event = None self . headers = [ ] self . tx_hashes = [ ] self . undo_infos = [ ] self . utxo_cache = { } self . db_deletes = [ ] self . state_lock = asyncio . Lock ( )  async def run_in_thread_with_lock ( self , func , * args ) :          async def run_in_thread_locked ( ) :              async with self . state_lock :                  return await run_in_thread ( func , * args )   return await asyncio . shield ( run_in_thread_locked ( ) )  async def check_and_advance_blocks ( self , raw_blocks ) :          if not raw_blocks :              return  first = self . height + 1 blocks = [ self . coin . block ( raw_block , first + n ) for n , raw_block in enumerate ( raw_blocks ) ] headers = [ block . header for block in blocks ] hprevs = [ self . coin . header_prevhash ( h ) for h in headers ] chain = [ self . tip ] + [ self . coin . header_hash ( h ) for h in headers [ : - 1 ] ] if hprevs == chain :              start = time . time ( ) await self . run_in_thread_with_lock ( self . advance_blocks , blocks ) await self . _maybe_flush ( ) if not self . db . first_sync :                  s = <str> if len ( blocks ) == 1 else <str> blocks_size = sum ( len ( block ) for block in raw_blocks ) / 1_000_000 self . logger . info ( <str> <str> )  if self . _caught_up_event . is_set ( ) :                  await self . notifications . on_block ( self . touched , self . height )  self . touched = set ( )  elif hprevs [ 0 ] != chain [ 0 ] :              await self . reorg_chain ( )  else :              self . logger . warning ( <str> <str> ) await self . prefetcher . reset_height ( self . height )   async def reorg_chain ( self , count = None ) :          if count is None :              self . logger . info ( <str> )  else :              self . logger . info ( <str> )  await self . flush ( True ) async def get_raw_blocks ( last_height , hex_hashes ) :              heights = range ( last_height , last_height - len ( hex_hashes ) , - 1 ) try :                  blocks = [ self . db . read_raw_block ( height ) for height in heights ] self . logger . info ( <str> ) return blocks  except FileNotFoundError :                  return await self . daemon . raw_blocks ( hex_hashes )   def flush_backup ( ) :              self . touched . discard ( None ) self . db . flush_backup ( self . flush_data ( ) , self . touched )  _start , last , hashes = await self . reorg_hashes ( count ) hashes = [ hash_to_hex_str ( hash ) for hash in reversed ( hashes ) ] for hex_hashes in chunks ( hashes , 50 ) :              raw_blocks = await get_raw_blocks ( last , hex_hashes ) await self . run_in_thread_with_lock ( self . backup_blocks , raw_blocks ) await self . run_in_thread_with_lock ( flush_backup ) last -= len ( raw_blocks )  await self . prefetcher . reset_height ( self . height )  async def reorg_hashes ( self , count ) :          start , count = await self . calc_reorg_range ( count ) last = start + count - 1 s = <str> if count == 1 else <str> self . logger . info ( <str> <str> ) return start , last , await self . db . fs_block_hashes ( start , count )  async def calc_reorg_range ( self , count ) :          def diff_pos ( hashes1 , hashes2 ) :              for n , ( hash1 , hash2 ) in enumerate ( zip ( hashes1 , hashes2 ) ) :                  if hash1 != hash2 :                      return n   return len ( hashes )  if count is None :              start = self . height - 1 count = 1 while start > 0 :                  hashes = await self . db . fs_block_hashes ( start , count ) hex_hashes = [ hash_to_hex_str ( hash ) for hash in hashes ] d_hex_hashes = await self . daemon . block_hex_hashes ( start , count ) n = diff_pos ( hex_hashes , d_hex_hashes ) if n > 0 :                      start += n break  count = min ( count * 2 , start ) start -= count  count = ( self . height - start ) + 1  else :              start = ( self . height - count ) + 1  return start , count  def estimate_txs_remaining ( self ) :          daemon_height = self . daemon . cached_height ( ) coin = self . coin tail_count = daemon_height - max ( self . height , coin . TX_COUNT_HEIGHT ) realism = max ( 2.0 - 0.9 * self . height / coin . TX_COUNT_HEIGHT , 1.0 ) return ( tail_count * coin . TX_PER_BLOCK + max ( coin . TX_COUNT - self . tx_count , 0 ) ) * realism  def flush_data ( self ) :          assert self . state_lock . locked ( ) return FlushData ( self . height , self . tx_count , self . headers , self . tx_hashes , self . undo_infos , self . utxo_cache , self . db_deletes , self . tip )  async def flush ( self , flush_utxos ) :          def flush ( ) :              self . db . flush_dbs ( self . flush_data ( ) , flush_utxos , self . estimate_txs_remaining )  await self . run_in_thread_with_lock ( flush )  async def _maybe_flush ( self ) :          if self . _caught_up_event . is_set ( ) :              await self . flush ( True )  elif time . time ( ) > self . next_cache_check :              flush_arg = self . check_cache_size ( ) if flush_arg is not None :                  await self . flush ( flush_arg )  self . next_cache_check = time . time ( ) + 30   def check_cache_size ( self ) :          one_MB = 1000 * 1000 utxo_cache_size = len ( self . utxo_cache ) * 205 db_deletes_size = len ( self . db_deletes ) * 57 hist_cache_size = self . db . history . unflushed_memsize ( ) tx_hash_size = ( ( self . tx_count - self . db . fs_tx_count ) * 32 + ( self . height - self . db . fs_height ) * 42 ) utxo_MB = ( db_deletes_size + utxo_cache_size ) // one_MB hist_MB = ( hist_cache_size + tx_hash_size ) // one_MB self . logger . info ( <str> <str> . format ( self . height , self . daemon . cached_height ( ) , utxo_MB , hist_MB ) ) cache_MB = self . env . cache_MB if utxo_MB + hist_MB >= cache_MB or hist_MB >= cache_MB // 5 :              return utxo_MB >= cache_MB * 4 // 5  return None  def advance_blocks ( self , blocks ) :          min_height = self . db . min_undo_height ( self . daemon . cached_height ( ) ) height = self . height for block in blocks :              height += 1 undo_info = self . advance_txs ( block . transactions ) if height >= min_height :                  self . undo_infos . append ( ( undo_info , height ) ) self . db . write_raw_block ( block . raw , height )   headers = [ block . header for block in blocks ] self . height = height self . headers . extend ( headers ) self . tip = self . coin . header_hash ( headers [ - 1 ] )  def advance_txs ( self , txs ) :          self . tx_hashes . append ( <str> . join ( tx_hash for tx , tx_hash in txs ) ) undo_info = [ ] tx_num = self . tx_count script_hashX = self . coin . hashX_from_script s_pack = pack put_utxo = self . utxo_cache . __setitem__ spend_utxo = self . spend_utxo undo_info_append = undo_info . append update_touched = self . touched . update hashXs_by_tx = [ ] append_hashXs = hashXs_by_tx . append for tx , tx_hash in txs :              hashXs = [ ] append_hashX = hashXs . append tx_numb = s_pack ( <str> , tx_num ) for txin in tx . inputs :                  if txin . is_generation ( ) :                      continue  cache_value = spend_utxo ( txin . prev_hash , txin . prev_idx ) undo_info_append ( cache_value ) append_hashX ( cache_value [ : - 12 ] )  for idx , txout in enumerate ( tx . outputs ) :                  hashX = script_hashX ( txout . pk_script ) if hashX :                      append_hashX ( hashX ) put_utxo ( tx_hash + s_pack ( <str> , idx ) , hashX + tx_numb + s_pack ( <str> , txout . value ) )   append_hashXs ( hashXs ) update_touched ( hashXs ) tx_num += 1  self . db . history . add_unflushed ( hashXs_by_tx , self . tx_count ) self . tx_count = tx_num self . db . tx_counts . append ( tx_num ) return undo_info  def backup_blocks ( self , raw_blocks ) :          self . db . assert_flushed ( self . flush_data ( ) ) assert self . height >= len ( raw_blocks ) coin = self . coin for raw_block in raw_blocks :              block = coin . block ( raw_block , self . height ) header_hash = coin . header_hash ( block . header ) if header_hash != self . tip :                  raise ChainError ( <str> . format ( hash_to_hex_str ( header_hash ) , hash_to_hex_str ( self . tip ) , self . height ) )  self . tip = coin . header_prevhash ( block . header ) self . backup_txs ( block . transactions ) self . height -= 1 self . db . tx_counts . pop ( )  self . logger . info ( <str> . format ( self . height ) )  def backup_txs ( self , txs ) :          undo_info = self . db . read_undo_info ( self . height ) if undo_info is None :              raise ChainError ( <str> . format ( self . height ) )  n = len ( undo_info ) s_pack = pack put_utxo = self . utxo_cache . __setitem__ spend_utxo = self . spend_utxo script_hashX = self . coin . hashX_from_script touched = self . touched undo_entry_len = 12 + HASHX_LEN for tx , tx_hash in reversed ( txs ) :              for idx , txout in enumerate ( tx . outputs ) :                  hashX = script_hashX ( txout . pk_script ) if hashX :                      cache_value = spend_utxo ( tx_hash , idx ) touched . add ( cache_value [ : - 12 ] )   for txin in reversed ( tx . inputs ) :                  if txin . is_generation ( ) :                      continue  n -= undo_entry_len undo_item = undo_info [ n : n + undo_entry_len ] put_utxo ( txin . prev_hash + s_pack ( <str> , txin . prev_idx ) , undo_item ) touched . add ( undo_item [ : - 12 ] )   assert n == 0 self . tx_count -= len ( txs )  def spend_utxo ( self , tx_hash , tx_idx ) :          idx_packed = pack ( <str> , tx_idx ) cache_value = self . utxo_cache . pop ( tx_hash + idx_packed , None ) if cache_value :              return cache_value  prefix = <str> + tx_hash [ : 4 ] + idx_packed candidates = { db_key : hashX for db_key , hashX in self . db . utxo_db . iterator ( prefix = prefix ) } for hdb_key , hashX in candidates . items ( ) :              tx_num_packed = hdb_key [ - 4 : ] if len ( candidates ) > 1 :                  tx_num , = unpack ( <str> , tx_num_packed ) hash , height = self . db . fs_tx_hash ( tx_num ) if hash != tx_hash :                      assert hash is not None continue   udb_key = <str> + hashX + hdb_key [ - 6 : ] utxo_value_packed = self . db . utxo_db . get ( udb_key ) if utxo_value_packed :                  self . db_deletes . append ( hdb_key ) self . db_deletes . append ( udb_key ) return hashX + tx_num_packed + utxo_value_packed   raise ChainError ( <str> . format ( hash_to_hex_str ( tx_hash ) , tx_idx ) )  async def _process_prefetched_blocks ( self ) :          while True :              if self . height == self . daemon . cached_height ( ) :                  if not self . _caught_up_event . is_set ( ) :                      await self . _first_caught_up ( ) self . _caught_up_event . set ( )   await self . blocks_event . wait ( ) self . blocks_event . clear ( ) if self . reorg_count :                  await self . reorg_chain ( self . reorg_count ) self . reorg_count = 0  else :                  blocks = self . prefetcher . get_prefetched_blocks ( ) await self . check_and_advance_blocks ( blocks )    async def _first_caught_up ( self ) :          self . logger . info ( <str> ) first_sync = self . db . first_sync self . db . first_sync = False await self . flush ( True ) if first_sync :              self . logger . info ( <str> <str> )  await self . db . open_for_serving ( )  async def _first_open_dbs ( self ) :          await self . db . open_for_sync ( ) self . height = self . db . db_height self . tip = self . db . db_tip self . tx_count = self . db . db_tx_count  async def fetch_and_process_blocks ( self , caught_up_event ) :          self . _caught_up_event = caught_up_event await self . _first_open_dbs ( ) try :              async with TaskGroup ( ) as group :                  await group . spawn ( self . prefetcher . main_loop ( self . height ) ) await group . spawn ( self . _process_prefetched_blocks ( ) )   finally :              self . logger . info ( <str> ) await self . flush ( True )   def force_chain_reorg ( self , count ) :          if self . _caught_up_event . is_set ( ) :              self . reorg_count = count self . blocks_event . set ( ) return True  return False   class DecredBlockProcessor ( BlockProcessor ) :      async def calc_reorg_range ( self , count ) :          start , count = await super ( ) . calc_reorg_range ( count ) if start > 0 :              start -= 1 count += 1  return start , count   class NameIndexBlockProcessor ( BlockProcessor ) :      def advance_txs ( self , txs ) :          result = super ( ) . advance_txs ( txs ) tx_num = self . tx_count - len ( txs ) script_name_hashX = self . coin . name_hashX_from_script update_touched = self . touched . update hashXs_by_tx = [ ] append_hashXs = hashXs_by_tx . append for tx , _tx_hash in txs :              hashXs = [ ] append_hashX = hashXs . append for txout in tx . outputs :                  hashX = script_name_hashX ( txout . pk_script ) if hashX :                      append_hashX ( hashX )   append_hashXs ( hashXs ) update_touched ( hashXs ) tx_num += 1  self . db . history . add_unflushed ( hashXs_by_tx , self . tx_count - len ( txs ) ) return result   class LTORBlockProcessor ( BlockProcessor ) :      def advance_txs ( self , txs ) :          self . tx_hashes . append ( <str> . join ( tx_hash for tx , tx_hash in txs ) ) undo_info = [ ] tx_num = self . tx_count script_hashX = self . coin . hashX_from_script s_pack = pack put_utxo = self . utxo_cache . __setitem__ spend_utxo = self . spend_utxo undo_info_append = undo_info . append update_touched = self . touched . update hashXs_by_tx = [ set ( ) for _ in txs ] for ( tx , tx_hash ) , hashXs in zip ( txs , hashXs_by_tx ) :              add_hashXs = hashXs . add tx_numb = s_pack ( <str> , tx_num ) for idx , txout in enumerate ( tx . outputs ) :                  hashX = script_hashX ( txout . pk_script ) if hashX :                      add_hashXs ( hashX ) put_utxo ( tx_hash + s_pack ( <str> , idx ) , hashX + tx_numb + s_pack ( <str> , txout . value ) )   tx_num += 1  for ( tx , tx_hash ) , hashXs in zip ( txs , hashXs_by_tx ) :              add_hashXs = hashXs . add for txin in tx . inputs :                  if txin . is_generation ( ) :                      continue  cache_value = spend_utxo ( txin . prev_hash , txin . prev_idx ) undo_info_append ( cache_value ) add_hashXs ( cache_value [ : - 12 ] )   for hashXs in hashXs_by_tx :              update_touched ( hashXs )  self . db . history . add_unflushed ( hashXs_by_tx , self . tx_count ) self . tx_count = tx_num self . db . tx_counts . append ( tx_num ) return undo_info  def backup_txs ( self , txs ) :          undo_info = self . db . read_undo_info ( self . height ) if undo_info is None :              raise ChainError ( <str> . format ( self . height ) )  s_pack = pack put_utxo = self . utxo_cache . __setitem__ spend_utxo = self . spend_utxo script_hashX = self . coin . hashX_from_script add_touched = self . touched . add undo_entry_len = 12 + HASHX_LEN n = 0 for tx , tx_hash in txs :              for txin in tx . inputs :                  if txin . is_generation ( ) :                      continue  undo_item = undo_info [ n : n + undo_entry_len ] put_utxo ( txin . prev_hash + s_pack ( <str> , txin . prev_idx ) , undo_item ) add_touched ( undo_item [ : - 12 ] ) n += undo_entry_len   assert n == len ( undo_info ) for tx , tx_hash in txs :              for idx , txout in enumerate ( tx . outputs ) :                  hashX = script_hashX ( txout . pk_script ) if hashX :                      cache_value = spend_utxo ( tx_hash , idx ) add_touched ( cache_value [ : - 12 ] )    self . tx_count -= len ( txs )    