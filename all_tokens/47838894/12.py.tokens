import array import ast import os import time from bisect import bisect_right from collections import namedtuple from glob import glob from struct import Struct import attr from aiorpcx import run_in_thread , sleep import electrumx . lib . util as util from electrumx . lib . hash import hash_to_hex_str from electrumx . lib . merkle import Merkle , MerkleCache from electrumx . lib . util import ( formatted_time , pack_be_uint16 , pack_be_uint32 , pack_le_uint64 , pack_le_uint32 , unpack_le_uint32 , unpack_be_uint32 , unpack_le_uint64 ) from electrumx . server . storage import db_class from electrumx . server . history import History UTXO = namedtuple ( <str> , <str> ) @ attr . s ( slots = True ) class FlushData ( object ) :      height = attr . ib ( ) tx_count = attr . ib ( ) headers = attr . ib ( ) block_tx_hashes = attr . ib ( ) undo_infos = attr . ib ( ) adds = attr . ib ( ) deletes = attr . ib ( ) tip = attr . ib ( )  class DB ( object ) :      DB_VERSIONS = [ 6 , 7 ] class DBError ( Exception ) :           def __init__ ( self , env ) :          self . logger = util . class_logger ( __name__ , self . __class__ . __name__ ) self . env = env self . coin = env . coin if self . coin . STATIC_BLOCK_HEADERS :              self . header_offset = self . coin . static_header_offset self . header_len = self . coin . static_header_len  else :              self . header_offset = self . dynamic_header_offset self . header_len = self . dynamic_header_len  self . logger . info ( <str> ) os . chdir ( env . db_dir ) self . db_class = db_class ( self . env . db_engine ) self . history = History ( ) self . utxo_db = None self . utxo_flush_count = 0 self . fs_height = - 1 self . fs_tx_count = 0 self . db_height = - 1 self . db_tx_count = 0 self . db_tip = None self . tx_counts = None self . last_flush = time . time ( ) self . last_flush_tx_count = 0 self . wall_time = 0 self . first_sync = True self . db_version = - 1 self . logger . info ( <str> ) self . merkle = Merkle ( ) self . header_mc = MerkleCache ( self . merkle , self . fs_block_hashes ) self . headers_file = util . LogicalFile ( <str> , 2 , 16000000 ) self . tx_counts_file = util . LogicalFile ( <str> , 2 , 2000000 ) self . hashes_file = util . LogicalFile ( <str> , 4 , 16000000 ) if not self . coin . STATIC_BLOCK_HEADERS :              self . headers_offsets_file = util . LogicalFile ( <str> , 2 , 16000000 )   async def _read_tx_counts ( self ) :          if self . tx_counts is not None :              return  size = ( self . db_height + 1 ) * 4 tx_counts = self . tx_counts_file . read ( 0 , size ) assert len ( tx_counts ) == size self . tx_counts = array . array ( <str> , tx_counts ) if self . tx_counts :              assert self . db_tx_count == self . tx_counts [ - 1 ]  else :              assert self . db_tx_count == 0   async def _open_dbs ( self , for_sync , compacting ) :          assert self . utxo_db is None self . utxo_db = self . db_class ( <str> , for_sync ) if self . utxo_db . is_new :              self . logger . info ( <str> ) self . logger . info ( <str> ) os . mkdir ( <str> ) with util . open_file ( <str> , create = True ) as f :                  f . write ( <str> <str> . encode ( ) )  if not self . coin . STATIC_BLOCK_HEADERS :                  self . headers_offsets_file . write ( 0 , bytes ( 8 ) )   else :              self . logger . info ( <str> )  self . read_utxo_state ( ) self . utxo_flush_count = self . history . open_db ( self . db_class , for_sync , self . utxo_flush_count , compacting ) self . clear_excess_undo_info ( ) await self . _read_tx_counts ( )  async def open_for_compacting ( self ) :          await self . _open_dbs ( True , True )  async def open_for_sync ( self ) :          await self . _open_dbs ( True , False )  async def open_for_serving ( self ) :          if self . utxo_db :              self . logger . info ( <str> ) self . utxo_db . close ( ) self . history . close_db ( ) self . utxo_db = None  await self . _open_dbs ( False , False )  async def populate_header_merkle_cache ( self ) :          self . logger . info ( <str> ) length = max ( 1 , self . db_height - self . env . reorg_limit ) start = time . time ( ) await self . header_mc . initialize ( length ) elapsed = time . time ( ) - start self . logger . info ( <str> )  async def header_branch_and_root ( self , length , height ) :          return await self . header_mc . branch_and_root ( length , height )  def assert_flushed ( self , flush_data ) :          assert flush_data . tx_count == self . fs_tx_count == self . db_tx_count assert flush_data . height == self . fs_height == self . db_height assert flush_data . tip == self . db_tip assert not flush_data . headers assert not flush_data . block_tx_hashes assert not flush_data . adds assert not flush_data . deletes assert not flush_data . undo_infos self . history . assert_flushed ( )  def flush_dbs ( self , flush_data , flush_utxos , estimate_txs_remaining ) :          if flush_data . height == self . db_height :              self . assert_flushed ( flush_data ) return  start_time = time . time ( ) prior_flush = self . last_flush tx_delta = flush_data . tx_count - self . last_flush_tx_count self . flush_fs ( flush_data ) self . flush_history ( ) with self . utxo_db . write_batch ( ) as batch :              if flush_utxos :                  self . flush_utxo_db ( batch , flush_data )  self . flush_state ( batch )  self . flush_state ( self . utxo_db ) elapsed = self . last_flush - start_time self . logger . info ( <str> <str> <str> ) if self . utxo_db . for_sync :              flush_interval = self . last_flush - prior_flush tx_per_sec_gen = int ( flush_data . tx_count / self . wall_time ) tx_per_sec_last = 1 + int ( tx_delta / flush_interval ) eta = estimate_txs_remaining ( ) / tx_per_sec_last self . logger . info ( <str> <str> ) self . logger . info ( <str> <str> )   def flush_fs ( self , flush_data ) :          prior_tx_count = ( self . tx_counts [ self . fs_height ] if self . fs_height >= 0 else 0 ) assert len ( flush_data . block_tx_hashes ) == len ( flush_data . headers ) assert flush_data . height == self . fs_height + len ( flush_data . headers ) assert flush_data . tx_count == ( self . tx_counts [ - 1 ] if self . tx_counts else 0 ) assert len ( self . tx_counts ) == flush_data . height + 1 hashes = <str> . join ( flush_data . block_tx_hashes ) flush_data . block_tx_hashes . clear ( ) assert len ( hashes ) % 32 == 0 assert len ( hashes ) // 32 == flush_data . tx_count - prior_tx_count start_time = time . time ( ) height_start = self . fs_height + 1 offset = self . header_offset ( height_start ) self . headers_file . write ( offset , <str> . join ( flush_data . headers ) ) self . fs_update_header_offsets ( offset , height_start , flush_data . headers ) flush_data . headers . clear ( ) offset = height_start * self . tx_counts . itemsize self . tx_counts_file . write ( offset , self . tx_counts [ height_start : ] . tobytes ( ) ) offset = prior_tx_count * 32 self . hashes_file . write ( offset , hashes ) self . fs_height = flush_data . height self . fs_tx_count = flush_data . tx_count if self . utxo_db . for_sync :              elapsed = time . time ( ) - start_time self . logger . info ( <str> )   def flush_history ( self ) :          self . history . flush ( )  def flush_utxo_db ( self , batch , flush_data ) :          start_time = time . time ( ) add_count = len ( flush_data . adds ) spend_count = len ( flush_data . deletes ) // 2 batch_delete = batch . delete for key in sorted ( flush_data . deletes ) :              batch_delete ( key )  flush_data . deletes . clear ( ) batch_put = batch . put for key , value in flush_data . adds . items ( ) :              hashX = value [ : - 12 ] suffix = key [ - 4 : ] + value [ - 12 : - 8 ] batch_put ( <str> + key [ : 4 ] + suffix , hashX ) batch_put ( <str> + hashX + suffix , value [ - 8 : ] )  flush_data . adds . clear ( ) self . flush_undo_infos ( batch_put , flush_data . undo_infos ) flush_data . undo_infos . clear ( ) if self . utxo_db . for_sync :              block_count = flush_data . height - self . db_height tx_count = flush_data . tx_count - self . db_tx_count elapsed = time . time ( ) - start_time self . logger . info ( <str> <str> <str> <str> )  self . utxo_flush_count = self . history . flush_count self . db_height = flush_data . height self . db_tx_count = flush_data . tx_count self . db_tip = flush_data . tip  def flush_state ( self , batch ) :          now = time . time ( ) self . wall_time += now - self . last_flush self . last_flush = now self . last_flush_tx_count = self . fs_tx_count self . write_utxo_state ( batch )  def flush_backup ( self , flush_data , touched ) :          assert not flush_data . headers assert not flush_data . block_tx_hashes assert flush_data . height < self . db_height self . history . assert_flushed ( ) start_time = time . time ( ) tx_delta = flush_data . tx_count - self . last_flush_tx_count self . backup_fs ( flush_data . height , flush_data . tx_count ) self . history . backup ( touched , flush_data . tx_count ) with self . utxo_db . write_batch ( ) as batch :              self . flush_utxo_db ( batch , flush_data ) self . flush_state ( batch )  elapsed = self . last_flush - start_time self . logger . info ( <str> <str> <str> )  def fs_update_header_offsets ( self , offset_start , height_start , headers ) :          if self . coin . STATIC_BLOCK_HEADERS :              return  offset = offset_start offsets = [ ] for h in headers :              offset += len ( h ) offsets . append ( pack_le_uint64 ( offset ) )  pos = ( height_start + 1 ) * 8 self . headers_offsets_file . write ( pos , <str> . join ( offsets ) )  def dynamic_header_offset ( self , height ) :          assert not self . coin . STATIC_BLOCK_HEADERS offset , = unpack_le_uint64 ( self . headers_offsets_file . read ( height * 8 , 8 ) ) return offset  def dynamic_header_len ( self , height ) :          return self . dynamic_header_offset ( height + 1 ) - self . dynamic_header_offset ( height )  def backup_fs ( self , height , tx_count ) :          self . fs_height = height self . fs_tx_count = tx_count self . header_mc . truncate ( height + 1 )  async def raw_header ( self , height ) :          header , n = await self . read_headers ( height , 1 ) if n != 1 :              raise IndexError ( <str> )  return header  async def read_headers ( self , start_height , count ) :          if start_height < 0 or count < 0 :              raise self . DBError ( <str> <str> )  def read_headers ( ) :              disk_count = max ( 0 , min ( count , self . db_height + 1 - start_height ) ) if disk_count :                  offset = self . header_offset ( start_height ) size = self . header_offset ( start_height + disk_count ) - offset return self . headers_file . read ( offset , size ) , disk_count  return <str> , 0  return await run_in_thread ( read_headers )  def fs_tx_hash ( self , tx_num ) :          tx_height = bisect_right ( self . tx_counts , tx_num ) if tx_height > self . db_height :              tx_hash = None  else :              tx_hash = self . hashes_file . read ( tx_num * 32 , 32 )  return tx_hash , tx_height  def fs_tx_hashes_at_blockheight ( self , block_height ) :          if block_height > self . db_height :              raise self . DBError ( <str> )  assert block_height >= 0 if block_height > 0 :              first_tx_num = self . tx_counts [ block_height - 1 ]  else :              first_tx_num = 0  num_txs_in_block = self . tx_counts [ block_height ] - first_tx_num tx_hashes = self . hashes_file . read ( first_tx_num * 32 , num_txs_in_block * 32 ) assert num_txs_in_block == len ( tx_hashes ) // 32 return [ tx_hashes [ idx * 32 : ( idx + 1 ) * 32 ] for idx in range ( num_txs_in_block ) ]  async def tx_hashes_at_blockheight ( self , block_height ) :          return await run_in_thread ( self . fs_tx_hashes_at_blockheight , block_height )  async def fs_block_hashes ( self , height , count ) :          headers_concat , headers_count = await self . read_headers ( height , count ) if headers_count != count :              raise self . DBError ( <str> <str> . format ( headers_count , height , count ) )  offset = 0 headers = [ ] for n in range ( count ) :              hlen = self . header_len ( height + n ) headers . append ( headers_concat [ offset : offset + hlen ] ) offset += hlen  return [ self . coin . header_hash ( header ) for header in headers ]  async def limited_history ( self , hashX , * , limit = 1000 ) :          def read_history ( ) :              tx_nums = list ( self . history . get_txnums ( hashX , limit ) ) fs_tx_hash = self . fs_tx_hash return [ fs_tx_hash ( tx_num ) for tx_num in tx_nums ]  while True :              history = await run_in_thread ( read_history ) if all ( hash is not None for hash , height in history ) :                  return history  self . logger . warning ( <str> <str> ) await sleep ( 0.25 )   def min_undo_height ( self , max_height ) :          return max_height - self . env . reorg_limit + 1  def undo_key ( self , height ) :          return <str> + pack_be_uint32 ( height )  def read_undo_info ( self , height ) :          return self . utxo_db . get ( self . undo_key ( height ) )  def flush_undo_infos ( self , batch_put , undo_infos ) :          for undo_info , height in undo_infos :              batch_put ( self . undo_key ( height ) , <str> . join ( undo_info ) )   def raw_block_prefix ( self ) :          return <str>  def raw_block_path ( self , height ) :          return <str>  def read_raw_block ( self , height ) :          with util . open_file ( self . raw_block_path ( height ) ) as f :              return f . read ( - 1 )   def write_raw_block ( self , block , height ) :          with util . open_truncate ( self . raw_block_path ( height ) ) as f :              f . write ( block )  try :              del_height = self . min_undo_height ( height ) - 1 os . remove ( self . raw_block_path ( del_height ) )  except FileNotFoundError :              pass   def clear_excess_undo_info ( self ) :          prefix = <str> min_height = self . min_undo_height ( self . db_height ) keys = [ ] for key , _hist in self . utxo_db . iterator ( prefix = prefix ) :              height , = unpack_be_uint32 ( key [ - 4 : ] ) if height >= min_height :                  break  keys . append ( key )  if keys :              with self . utxo_db . write_batch ( ) as batch :                  for key in keys :                      batch . delete ( key )   self . logger . info ( <str> )  prefix = self . raw_block_prefix ( ) paths = [ path for path in glob ( <str> ) if len ( path ) > len ( prefix ) and int ( path [ len ( prefix ) : ] ) < min_height ] if paths :              for path in paths :                  try :                      os . remove ( path )  except FileNotFoundError :                      pass   self . logger . info ( <str> )   def read_utxo_state ( self ) :          state = self . utxo_db . get ( <str> ) if not state :              self . db_height = - 1 self . db_tx_count = 0 self . db_tip = <str> * 32 self . db_version = max ( self . DB_VERSIONS ) self . utxo_flush_count = 0 self . wall_time = 0 self . first_sync = True  else :              state = ast . literal_eval ( state . decode ( ) ) if not isinstance ( state , dict ) :                  raise self . DBError ( <str> )  self . db_version = state [ <str> ] if self . db_version not in self . DB_VERSIONS :                  raise self . DBError ( <str> <str> . format ( self . db_version , self . DB_VERSIONS ) )  genesis_hash = state [ <str> ] if isinstance ( genesis_hash , bytes ) :                  genesis_hash = genesis_hash . decode ( )  if genesis_hash != self . coin . GENESIS_HASH :                  raise self . DBError ( <str> . format ( genesis_hash , self . coin . GENESIS_HASH ) )  self . db_height = state [ <str> ] self . db_tx_count = state [ <str> ] self . db_tip = state [ <str> ] self . utxo_flush_count = state [ <str> ] self . wall_time = state [ <str> ] self . first_sync = state [ <str> ]  self . fs_height = self . db_height self . fs_tx_count = self . db_tx_count self . last_flush_tx_count = self . fs_tx_count if self . db_version != max ( self . DB_VERSIONS ) :              self . upgrade_db ( )  self . logger . info ( <str> . format ( self . db_version ) ) self . logger . info ( <str> . format ( self . coin . NAME ) ) self . logger . info ( <str> . format ( self . coin . NET ) ) self . logger . info ( <str> . format ( self . db_height ) ) self . logger . info ( <str> . format ( hash_to_hex_str ( self . db_tip ) ) ) self . logger . info ( <str> . format ( self . db_tx_count ) ) if self . utxo_db . for_sync :              self . logger . info ( <str> )  if self . first_sync :              self . logger . info ( <str> . format ( util . formatted_time ( self . wall_time ) ) )   def upgrade_db ( self ) :          self . logger . info ( <str> . format ( self . db_version ) ) self . logger . info ( <str> ) def upgrade_u_prefix ( prefix ) :              count = 0 with self . utxo_db . write_batch ( ) as batch :                  batch_delete = batch . delete batch_put = batch . put for db_key , db_value in self . utxo_db . iterator ( prefix = prefix ) :                      if len ( db_key ) != 18 :                          break  count += 1 batch_delete ( db_key ) batch_put ( db_key [ : 14 ] + <str> + db_key [ 14 : ] , db_value )   return count  last = time . time ( ) count = 0 for cursor in range ( 65536 ) :              prefix = <str> + pack_be_uint16 ( cursor ) count += upgrade_u_prefix ( prefix ) now = time . time ( ) if now > last + 10 :                  last = now self . logger . info ( <str> <str> )   self . logger . info ( <str> ) def upgrade_h_prefix ( prefix ) :              count = 0 with self . utxo_db . write_batch ( ) as batch :                  batch_delete = batch . delete batch_put = batch . put for db_key , db_value in self . utxo_db . iterator ( prefix = prefix ) :                      if len ( db_key ) != 11 :                          break  count += 1 batch_delete ( db_key ) batch_put ( db_key [ : 7 ] + <str> + db_key [ 7 : ] , db_value )   return count  last = time . time ( ) count = 0 for cursor in range ( 65536 ) :              prefix = <str> + pack_be_uint16 ( cursor ) count += upgrade_h_prefix ( prefix ) now = time . time ( ) if now > last + 10 :                  last = now self . logger . info ( <str> <str> )   self . db_version = max ( self . DB_VERSIONS ) with self . utxo_db . write_batch ( ) as batch :              self . write_utxo_state ( batch )  self . logger . info ( <str> )  def write_utxo_state ( self , batch ) :          state = { <str> : self . coin . GENESIS_HASH , <str> : self . db_height , <str> : self . db_tx_count , <str> : self . db_tip , <str> : self . utxo_flush_count , <str> : self . wall_time , <str> : self . first_sync , <str> : self . db_version , } batch . put ( <str> , repr ( state ) . encode ( ) )  def set_flush_count ( self , count ) :          self . utxo_flush_count = count with self . utxo_db . write_batch ( ) as batch :              self . write_utxo_state ( batch )   async def all_utxos ( self , hashX ) :          def read_utxos ( ) :              utxos = [ ] utxos_append = utxos . append unpack_2_le_uint32 = Struct ( <str> ) . unpack prefix = <str> + hashX for db_key , db_value in self . utxo_db . iterator ( prefix = prefix ) :                  tx_pos , tx_num = unpack_2_le_uint32 ( db_key [ - 8 : ] ) value , = unpack_le_uint64 ( db_value ) tx_hash , height = self . fs_tx_hash ( tx_num ) utxos_append ( UTXO ( tx_num , tx_pos , tx_hash , height , value ) )  return utxos  while True :              utxos = await run_in_thread ( read_utxos ) if all ( utxo . tx_hash is not None for utxo in utxos ) :                  return utxos  self . logger . warning ( <str> <str> ) await sleep ( 0.25 )   async def lookup_utxos ( self , prevouts ) :          def lookup_hashXs ( ) :              def lookup_hashX ( tx_hash , tx_idx ) :                  idx_packed = pack_le_uint32 ( tx_idx ) prefix = <str> + tx_hash [ : 4 ] + idx_packed for db_key , hashX in self . utxo_db . iterator ( prefix = prefix ) :                      tx_num_packed = db_key [ - 4 : ] tx_num , = unpack_le_uint32 ( tx_num_packed ) hash , _height = self . fs_tx_hash ( tx_num ) if hash == tx_hash :                          return hashX , idx_packed + tx_num_packed   return None , None  return [ lookup_hashX ( * prevout ) for prevout in prevouts ]  def lookup_utxos ( hashX_pairs ) :              def lookup_utxo ( hashX , suffix ) :                  if not hashX :                      return None  key = <str> + hashX + suffix db_value = self . utxo_db . get ( key ) if not db_value :                      return None  value , = unpack_le_uint64 ( db_value ) return hashX , value  return [ lookup_utxo ( * hashX_pair ) for hashX_pair in hashX_pairs ]  hashX_pairs = await run_in_thread ( lookup_hashXs ) return await run_in_thread ( lookup_utxos , hashX_pairs )    