from __future__ import division import warnings import numpy as np import torch from packaging . version import parse as parse_version from odl import Operator if parse_version ( torch . __version__ ) < parse_version ( <str> ) :      warnings . warn ( <str> , RuntimeWarning , stacklevel = 2 )  __all__ = ( <str> , <str> ) class OperatorFunction ( torch . autograd . Function ) :      @ staticmethod def forward ( ctx , operator , input ) :          if not isinstance ( operator , Operator ) :              raise TypeError ( <str> <str> . format ( operator ) )  ctx . operator = operator if not operator . is_linear :              ctx . save_for_backward ( input )  input_arr = copy_if_zero_strides ( input . cpu ( ) . detach ( ) . numpy ( ) ) in_shape = input_arr . shape op_in_shape = operator . domain . shape if operator . is_functional :              op_out_shape = ( ) op_out_dtype = operator . domain . dtype  else :              op_out_shape = operator . range . shape op_out_dtype = operator . range . dtype  extra_shape = in_shape [ : - len ( op_in_shape ) ] if in_shape [ - len ( op_in_shape ) : ] != op_in_shape :              shp_str = str ( op_in_shape ) . strip ( <str> ) raise ValueError ( <str> <str> . format ( shp_str , in_shape ) )  ctx . op_in_shape = op_in_shape ctx . op_out_shape = op_out_shape ctx . extra_shape = extra_shape ctx . op_in_dtype = operator . domain . dtype ctx . op_out_dtype = op_out_dtype if extra_shape :              input_arr_flat_extra = input_arr . reshape ( ( - 1 , ) + op_in_shape ) results = [ ] for inp in input_arr_flat_extra :                  results . append ( operator ( inp ) )  result_arr = np . stack ( results ) . astype ( op_out_dtype , copy = False ) result_arr = result_arr . reshape ( extra_shape + op_out_shape )  else :              result_arr = np . asarray ( operator ( input_arr ) ) . astype ( op_out_dtype , copy = False )  tensor = torch . from_numpy ( result_arr ) . to ( input . device ) return tensor  @ staticmethod def backward ( ctx , grad_output ) :          <str> if not ctx . needs_input_grad [ 1 ] :              return None , None  operator = ctx . operator if not operator . is_linear :              input_arr = copy_if_zero_strides ( ctx . saved_tensors [ 0 ] . detach ( ) . cpu ( ) . numpy ( ) )  try :              dom_weight = operator . domain . weighting . const  except AttributeError :              dom_weight = 1.0  try :              ran_weight = operator . range . weighting . const  except AttributeError :              ran_weight = 1.0  scaling = dom_weight / ran_weight grad_output_arr = copy_if_zero_strides ( grad_output . detach ( ) . cpu ( ) . numpy ( ) ) op_in_shape = ctx . op_in_shape op_out_shape = ctx . op_out_shape extra_shape = ctx . extra_shape op_in_dtype = ctx . op_in_dtype if grad_output_arr . shape != extra_shape + op_out_shape :              raise ValueError ( <str> <str> . format ( extra_shape + op_out_shape , grad_output_arr . shape ) )  if extra_shape :              grad_output_arr_flat_extra = grad_output_arr . reshape ( ( - 1 , ) + op_out_shape ) results = [ ] if operator . is_linear :                  for ograd in grad_output_arr_flat_extra :                      results . append ( np . asarray ( operator . adjoint ( ograd ) ) )   else :                  input_arr_flat_extra = input_arr . reshape ( ( - 1 , ) + op_in_shape ) for ograd , inp in zip ( grad_output_arr_flat_extra , input_arr_flat_extra ) :                      results . append ( np . asarray ( operator . derivative ( inp ) . adjoint ( ograd ) ) )   result_arr = np . stack ( results ) . astype ( op_in_dtype , copy = False ) result_arr = result_arr . reshape ( extra_shape + op_in_shape )  else :              if operator . is_linear :                  result_arr = np . asarray ( operator . adjoint ( grad_output_arr ) ) . astype ( op_in_dtype , copy = False )  else :                  result_arr = np . asarray ( operator . derivative ( input_arr ) . adjoint ( grad_output_arr ) ) . astype ( op_in_dtype , copy = False )   if scaling != 1.0 :              result_arr *= scaling  grad_input = torch . from_numpy ( result_arr ) . to ( grad_output . device ) return None , grad_input   class OperatorModule ( torch . nn . Module ) :      def __init__ ( self , operator ) :          super ( OperatorModule , self ) . __init__ ( ) self . operator = operator  def forward ( self , x ) :          in_shape = tuple ( x . shape ) in_ndim = len ( in_shape ) op_in_shape = self . operator . domain . shape op_in_ndim = len ( op_in_shape ) if in_ndim <= op_in_ndim or in_shape [ - op_in_ndim : ] != op_in_shape :              shp_str = str ( op_in_shape ) . strip ( <str> ) raise ValueError ( <str> <str> . format ( shp_str , in_shape ) )  return OperatorFunction . apply ( self . operator , x )  def __repr__ ( self ) :          op_name = self . operator . __class__ . __name__ op_in_shape = self . operator . domain . shape if len ( op_in_shape ) == 1 :              op_in_shape = op_in_shape [ 0 ]  op_out_shape = self . operator . range . shape if len ( op_out_shape ) == 1 :              op_out_shape = op_out_shape [ 0 ]  return <str> . format ( self . __class__ . __name__ , op_name , op_in_shape , op_out_shape )   def copy_if_zero_strides ( arr ) :      assert isinstance ( arr , np . ndarray ) return arr . copy ( ) if 0 in arr . strides else arr  if __name__ == <str> :      from odl . util . testutils import run_doctests import odl from torch import autograd , nn run_doctests ( extraglobs = { <str> : np , <str> : odl , <str> : torch , <str> : nn , <str> : autograd } )   