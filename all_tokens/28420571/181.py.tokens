from __future__ import division from packaging . version import parse as parse_version import warnings import numpy as np import torch if parse_version ( torch . __version__ ) < parse_version ( <str> ) :      warnings . warn ( <str> , RuntimeWarning )  __all__ = ( <str> , <str> ) class OperatorAsAutogradFunction ( torch . autograd . Function ) :      def __init__ ( self , operator ) :          super ( OperatorAsAutogradFunction , self ) . __init__ ( ) self . operator = operator  def forward ( self , input ) :          if not self . operator . is_linear :              self . save_for_backward ( input )  input_arr = input . cpu ( ) . detach ( ) . numpy ( ) if any ( s == 0 for s in input_arr . strides ) :              input_arr = input_arr . copy ( )  op_result = self . operator ( input_arr ) if np . isscalar ( op_result ) :              op_result = np . array ( op_result , ndmin = 1 , dtype = self . operator . domain . dtype )  tensor = torch . from_numpy ( np . array ( op_result , copy = False , ndmin = 1 ) ) if input . is_cuda :              tensor = tensor . cuda ( )  return tensor  def backward ( self , grad_output ) :          if not self . operator . is_linear :              input_arr = self . saved_variables [ 0 ] . data . cpu ( ) . numpy ( ) if any ( s == 0 for s in input_arr . strides ) :                  input_arr = input_arr . copy ( )   grad = None try :              dom_weight = self . operator . domain . weighting . const  except AttributeError :              dom_weight = 1.0  try :              ran_weight = self . operator . range . weighting . const  except AttributeError :              ran_weight = 1.0  scaling = dom_weight / ran_weight if self . needs_input_grad [ 0 ] :              grad_output_arr = grad_output . cpu ( ) . numpy ( ) if any ( s == 0 for s in grad_output_arr . strides ) :                  grad_output_arr = grad_output_arr . copy ( )  if self . operator . is_linear :                  adjoint = self . operator . adjoint  else :                  adjoint = self . operator . derivative ( input_arr ) . adjoint  grad_odl = adjoint ( grad_output_arr ) if scaling != 1.0 :                  grad_odl *= scaling  grad = torch . from_numpy ( np . array ( grad_odl , copy = False , ndmin = 1 ) ) if grad_output . is_cuda :                  grad = grad . cuda ( )   return grad  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . operator )   class OperatorAsModule ( torch . nn . Module ) :      def __init__ ( self , operator ) :          super ( OperatorAsModule , self ) . __init__ ( ) self . op_func = OperatorAsAutogradFunction ( operator )  @ property def operator ( self ) :          return self . op_func . operator  def forward ( self , x ) :          in_shape = x . data . shape op_in_shape = self . op_func . operator . domain . shape op_out_shape = self . op_func . operator . range . shape extra_shape = in_shape [ : - len ( op_in_shape ) ] if in_shape [ - len ( op_in_shape ) : ] != op_in_shape or not extra_shape :              shp_str = str ( op_in_shape ) . strip ( <str> ) raise ValueError ( <str> <str> . format ( shp_str , in_shape ) )  newshape = ( int ( np . prod ( extra_shape ) ) , ) + op_in_shape x_flat_xtra = x . reshape ( * newshape ) results = [ ] for i in range ( x_flat_xtra . data . shape [ 0 ] ) :              results . append ( self . op_func ( x_flat_xtra [ i ] ) )  stack_flat_xtra = torch . stack ( results ) return stack_flat_xtra . view ( extra_shape + op_out_shape )  def __repr__ ( self ) :          op_name = self . op_func . operator . __class__ . __name__ op_dom_shape = self . op_func . operator . domain . shape if len ( op_dom_shape ) == 1 :              op_dom_shape = op_dom_shape [ 0 ]  op_ran_shape = self . op_func . operator . range . shape if len ( op_ran_shape ) == 1 :              op_ran_shape = op_ran_shape [ 0 ]  return <str> . format ( self . __class__ . __name__ , op_name , op_dom_shape , op_ran_shape )   if __name__ == <str> :      from odl . util . testutils import run_doctests import odl from torch import autograd , nn run_doctests ( extraglobs = { <str> : np , <str> : odl , <str> : torch , <str> : nn , <str> : autograd } )   