import numpy as np import torch from torch import autograd , nn import odl from odl . contrib import torch as odl_torch from odl . util . testutils import all_almost_equal , simple_fixture dtype = simple_fixture ( <str> , [ <str> , <str> ] ) use_cuda_params = [ False ] if torch . cuda . is_available ( ) :      use_cuda_params . append ( True )  use_cuda = simple_fixture ( <str> , use_cuda_params ) shape = simple_fixture ( <str> , [ ( 3 , ) , ( 2 , 3 ) , ( 2 , 2 , 3 ) ] ) def test_autograd_function_forward ( dtype , use_cuda ) :      matrix = np . random . rand ( 2 , 3 ) . astype ( dtype ) odl_op = odl . MatrixOperator ( matrix ) torch_op = odl_torch . OperatorAsAutogradFunction ( odl_op ) x = torch . from_numpy ( np . ones ( 3 , dtype = dtype ) ) if use_cuda :          x = x . cuda ( )  x_var = autograd . Variable ( x ) res_var = torch_op ( x_var ) odl_res = odl_op ( x . cpu ( ) . numpy ( ) ) assert res_var . data . cpu ( ) . numpy ( ) . dtype == dtype assert all_almost_equal ( res_var . data . cpu ( ) . numpy ( ) , odl_res ) if use_cuda :          assert res_var . is_cuda   def test_autograd_function_backward ( dtype , use_cuda ) :      matrix = np . random . rand ( 2 , 3 ) . astype ( dtype ) odl_op = odl . MatrixOperator ( matrix ) odl_cost = odl . solvers . L2NormSquared ( odl_op . range ) odl_functional = odl_cost * odl_op torch_op = odl_torch . OperatorAsAutogradFunction ( odl_op ) torch_cost = odl_torch . OperatorAsAutogradFunction ( odl_cost ) x = torch . from_numpy ( np . ones ( 3 , dtype = dtype ) ) if use_cuda :          x = x . cuda ( )  x_var = autograd . Variable ( x , requires_grad = True ) y_var = torch_op ( x_var ) res_var = torch_cost ( y_var ) res_var . backward ( ) torch_grad = x_var . grad odl_grad = odl_functional . gradient ( x . cpu ( ) . numpy ( ) ) assert torch_grad . data . cpu ( ) . numpy ( ) . dtype == dtype assert all_almost_equal ( torch_grad . data . cpu ( ) . numpy ( ) , odl_grad ) if use_cuda :          assert torch_grad . is_cuda   def test_module_forward ( shape , use_cuda ) :      ndim = len ( shape ) space = odl . uniform_discr ( [ 0 ] * ndim , shape , shape ) odl_op = odl . ScalingOperator ( space , 2 ) op_mod = odl_torch . OperatorAsModule ( odl_op ) x = torch . from_numpy ( np . ones ( shape ) ) if use_cuda :          x = x . cuda ( )  x_var = autograd . Variable ( x , requires_grad = True ) [ None , ... ] y_var = op_mod ( x_var ) assert y_var . data . shape == ( 1 , ) + odl_op . range . shape assert all_almost_equal ( y_var . data . cpu ( ) . numpy ( ) , 2 * np . ones ( ( 1 , ) + shape ) ) x_var = autograd . Variable ( x , requires_grad = True ) [ None , None , ... ] y_var = op_mod ( x_var ) assert y_var . data . shape == ( 1 , 1 ) + odl_op . range . shape assert all_almost_equal ( y_var . data . cpu ( ) . numpy ( ) , 2 * np . ones ( ( 1 , 1 ) + shape ) ) if use_cuda :          assert y_var . is_cuda   def test_module_forward_diff_shapes ( use_cuda ) :      matrix = np . random . rand ( 2 , 3 ) odl_op = odl . MatrixOperator ( matrix ) op_mod = odl_torch . OperatorAsModule ( odl_op ) x = torch . from_numpy ( np . ones ( 3 ) ) if use_cuda :          x = x . cuda ( )  x_var = autograd . Variable ( x , requires_grad = True ) [ None , ... ] y_var = op_mod ( x_var ) assert y_var . data . shape == ( 1 , ) + odl_op . range . shape assert all_almost_equal ( y_var . data . cpu ( ) . numpy ( ) , odl_op ( np . ones ( 3 ) ) . asarray ( ) . reshape ( ( 1 , 2 ) ) ) x_var = autograd . Variable ( x , requires_grad = True ) [ None , None , ... ] y_var = op_mod ( x_var ) assert y_var . data . shape == ( 1 , 1 ) + odl_op . range . shape assert all_almost_equal ( y_var . data . cpu ( ) . numpy ( ) , odl_op ( np . ones ( 3 ) ) . asarray ( ) . reshape ( ( 1 , 1 , 2 ) ) )  def test_module_backward ( use_cuda ) :      matrix = np . random . rand ( 2 , 3 ) . astype ( <str> ) odl_op = odl . MatrixOperator ( matrix ) op_mod = odl_torch . OperatorAsModule ( odl_op ) loss_fun = nn . MSELoss ( ) layer_before = nn . Linear ( 3 , 3 ) layer_after = nn . Linear ( 2 , 2 ) model = nn . Sequential ( layer_before , op_mod , layer_after ) x = torch . from_numpy ( np . ones ( 3 , dtype = <str> ) ) target = torch . from_numpy ( np . zeros ( 2 , dtype = <str> ) ) if use_cuda :          x = x . cuda ( ) target = target . cuda ( ) model = model . cuda ( )  x_var = autograd . Variable ( x , requires_grad = True ) [ None , ... ] target_var = autograd . Variable ( target ) [ None , ... ] loss = loss_fun ( model ( x_var ) , target_var ) loss . backward ( ) assert all ( p is not None for p in model . parameters ( ) ) layer_before = nn . Conv1d ( 1 , 2 , 2 ) layer_after = nn . Conv1d ( 2 , 1 , 2 ) model = nn . Sequential ( layer_before , op_mod , layer_after ) x = torch . from_numpy ( np . ones ( 4 , dtype = <str> ) ) target = torch . from_numpy ( np . zeros ( 1 , dtype = <str> ) ) if use_cuda :          x = x . cuda ( ) target = target . cuda ( ) model = model . cuda ( )  x_var = autograd . Variable ( x , requires_grad = True ) [ None , None , ... ] target_var = autograd . Variable ( target ) [ None , None , ... ] loss = loss_fun ( model ( x_var ) , target_var ) loss . backward ( ) assert all ( p is not None for p in model . parameters ( ) ) if use_cuda :          assert x_var . is_cuda   if __name__ == <str> :      odl . util . test_file ( __file__ )   