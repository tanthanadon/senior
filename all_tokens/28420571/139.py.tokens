from __future__ import print_function , division , absolute_import import numpy as np from odl . operator . operator import ( Operator , OperatorComp , OperatorLeftScalarMult , OperatorRightScalarMult , OperatorRightVectorMult , OperatorSum , OperatorPointwiseProduct ) from odl . operator . default_ops import ( IdentityOperator , ConstantOperator ) from odl . solvers . nonsmooth import ( proximal_arg_scaling , proximal_translation , proximal_quadratic_perturbation , proximal_const_func , proximal_convex_conj ) from odl . util import signature_string , indent __all__ = ( <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ) class Functional ( Operator ) :      def __init__ ( self , space , linear = False , grad_lipschitz = np . nan ) :          Operator . __init__ ( self , domain = space , range = space . field , linear = linear ) self . __grad_lipschitz = float ( grad_lipschitz )  @ property def grad_lipschitz ( self ) :          return self . __grad_lipschitz  @ grad_lipschitz . setter def grad_lipschitz ( self , value ) :          self . __grad_lipschitz = float ( value )  @ property def gradient ( self ) :          <str> raise NotImplementedError ( <str> <str> . format ( self ) )  @ property def proximal ( self ) :          <str> raise NotImplementedError ( <str> <str> . format ( self ) )  @ property def convex_conj ( self ) :          <str> return FunctionalDefaultConvexConjugate ( self )  def derivative ( self , point ) :          return self . gradient ( point ) . T  def translated ( self , shift ) :          return FunctionalTranslation ( self , shift )  def bregman ( self , point , subgrad ) :          <str> return BregmanDistance ( self , point , subgrad )  def __mul__ ( self , other ) :          if isinstance ( other , Operator ) :              return FunctionalComp ( self , other )  elif other in self . range :              if other == 0 :                  from odl . solvers . functional . default_functionals import ( ConstantFunctional ) return ConstantFunctional ( self . domain , self ( self . domain . zero ( ) ) )  elif self . is_linear :                  return FunctionalLeftScalarMult ( self , other )  else :                  return FunctionalRightScalarMult ( self , other )   elif other in self . domain :              return FunctionalRightVectorMult ( self , other )  else :              return super ( Functional , self ) . __mul__ ( other )   def __rmul__ ( self , other ) :          if other in self . range :              if other == 0 :                  from odl . solvers . functional . default_functionals import ( ZeroFunctional ) return ZeroFunctional ( self . domain )  else :                  return FunctionalLeftScalarMult ( self , other )   else :              return super ( Functional , self ) . __rmul__ ( other )   def __add__ ( self , other ) :          if other in self . domain . field :              return FunctionalScalarSum ( self , other )  elif isinstance ( other , Functional ) :              return FunctionalSum ( self , other )  else :              return super ( Functional , self ) . __add__ ( other )   __radd__ = __add__ def __sub__ ( self , other ) :          return self + ( - 1 ) * other   class FunctionalLeftScalarMult ( Functional , OperatorLeftScalarMult ) :      def __init__ ( self , func , scalar ) :          if not isinstance ( func , Functional ) :              raise TypeError ( <str> <str> . format ( func ) )  Functional . __init__ ( self , space = func . domain , linear = func . is_linear , grad_lipschitz = np . abs ( scalar ) * func . grad_lipschitz ) OperatorLeftScalarMult . __init__ ( self , operator = func , scalar = scalar )  @ property def functional ( self ) :          return self . operator  @ property def gradient ( self ) :          return self . scalar * self . functional . gradient  @ property def convex_conj ( self ) :          if self . scalar <= 0 :              raise ValueError ( <str> <str> <str> . format ( self . scalar ) )  return self . scalar * self . functional . convex_conj * ( 1.0 / self . scalar )  @ property def proximal ( self ) :          if self . scalar < 0 :              raise ValueError ( <str> <str> <str> . format ( self . scalar ) )  elif self . scalar == 0 :              return proximal_const_func ( self . domain )  else :              def proximal_left_scalar_mult ( sigma = 1.0 ) :                  return self . functional . proximal ( sigma * self . scalar )  return proximal_left_scalar_mult    class FunctionalRightScalarMult ( Functional , OperatorRightScalarMult ) :      def __init__ ( self , func , scalar ) :          if not isinstance ( func , Functional ) :              raise TypeError ( <str> <str> . format ( func ) )  scalar = func . domain . field . element ( scalar ) Functional . __init__ ( self , space = func . domain , linear = func . is_linear , grad_lipschitz = np . abs ( scalar ) * func . grad_lipschitz ) OperatorRightScalarMult . __init__ ( self , operator = func , scalar = scalar )  @ property def functional ( self ) :          return self . operator  @ property def gradient ( self ) :          return self . scalar * self . functional . gradient * self . scalar  @ property def convex_conj ( self ) :          return self . functional . convex_conj * ( 1 / self . scalar )  @ property def proximal ( self ) :          return proximal_arg_scaling ( self . functional . proximal , self . scalar )   class FunctionalComp ( Functional , OperatorComp ) :      def __init__ ( self , func , op ) :          if not isinstance ( func , Functional ) :              raise TypeError ( <str> <str> . format ( func ) )  OperatorComp . __init__ ( self , left = func , right = op ) Functional . __init__ ( self , space = op . domain , linear = ( func . is_linear and op . is_linear ) , grad_lipschitz = np . nan )  @ property def gradient ( self ) :          func = self . left op = self . right class FunctionalCompositionGradient ( Operator ) :              def __init__ ( self ) :                  super ( FunctionalCompositionGradient , self ) . __init__ ( op . domain , op . domain , linear = False )  def _call ( self , x ) :                  return op . derivative ( x ) . adjoint ( func . gradient ( op ( x ) ) )  def derivative ( self , x ) :                  if not op . is_linear :                      raise NotImplementedError ( <str> <str> )  else :                      return ( op . adjoint * func . gradient * op ) . derivative ( x )    return FunctionalCompositionGradient ( )   class FunctionalRightVectorMult ( Functional , OperatorRightVectorMult ) :      def __init__ ( self , func , vector ) :          if not isinstance ( func , Functional ) :              raise TypeError ( <str> <str> . format ( func ) )  OperatorRightVectorMult . __init__ ( self , operator = func , vector = vector ) Functional . __init__ ( self , space = func . domain )  @ property def functional ( self ) :          return self . operator  @ property def gradient ( self ) :          return self . vector * self . operator . gradient * self . vector  @ property def convex_conj ( self ) :          return self . functional . convex_conj * ( 1.0 / self . vector )   class FunctionalSum ( Functional , OperatorSum ) :      def __init__ ( self , left , right ) :          if not isinstance ( left , Functional ) :              raise TypeError ( <str> <str> . format ( left ) )  if not isinstance ( right , Functional ) :              raise TypeError ( <str> <str> . format ( right ) )  Functional . __init__ ( self , space = left . domain , linear = ( left . is_linear and right . is_linear ) , grad_lipschitz = left . grad_lipschitz + right . grad_lipschitz ) OperatorSum . __init__ ( self , left , right )  @ property def gradient ( self ) :          return self . left . gradient + self . right . gradient   class FunctionalScalarSum ( FunctionalSum ) :      def __init__ ( self , func , scalar ) :          from odl . solvers . functional . default_functionals import ( ConstantFunctional ) if not isinstance ( func , Functional ) :              raise TypeError ( <str> <str> . format ( func ) )  if scalar not in func . range :              raise TypeError ( <str> <str> . format ( scalar , func ) )  super ( FunctionalScalarSum , self ) . __init__ ( left = func , right = ConstantFunctional ( space = func . domain , constant = scalar ) )  @ property def scalar ( self ) :          return self . right . constant  @ property def proximal ( self ) :          return self . left . proximal  @ property def convex_conj ( self ) :          return self . left . convex_conj - self . scalar   class FunctionalTranslation ( Functional ) :      def __init__ ( self , func , translation ) :          if not isinstance ( func , Functional ) :              raise TypeError ( <str> <str> . format ( func ) )  translation = func . domain . element ( translation ) super ( FunctionalTranslation , self ) . __init__ ( space = func . domain , linear = False , grad_lipschitz = func . grad_lipschitz ) if isinstance ( func , FunctionalTranslation ) :              self . __functional = func . functional self . __translation = func . translation + translation  else :              self . __functional = func self . __translation = translation   @ property def functional ( self ) :          return self . __functional  @ property def translation ( self ) :          return self . __translation  def _call ( self , x ) :          return self . functional ( x - self . translation )  @ property def gradient ( self ) :          return ( self . functional . gradient * ( IdentityOperator ( self . domain ) - self . translation ) )  @ property def proximal ( self ) :          return proximal_translation ( self . functional . proximal , self . translation )  @ property def convex_conj ( self ) :          return FunctionalQuadraticPerturb ( self . functional . convex_conj , linear_term = self . translation )  def __repr__ ( self ) :          return <str> . format ( self . functional , self . translation )  def __str__ ( self ) :          return <str> . format ( self . functional , self . translation )   class InfimalConvolution ( Functional ) :      def __init__ ( self , left , right ) :          if not isinstance ( left , Functional ) :              raise TypeError ( <str> <str> . format ( left ) )  if not isinstance ( right , Functional ) :              raise TypeError ( <str> <str> . format ( right ) )  super ( InfimalConvolution , self ) . __init__ ( space = left . domain , linear = False , grad_lipschitz = np . nan ) self . __left = left self . __right = right  @ property def left ( self ) :          return self . __left  @ property def right ( self ) :          return self . __right  @ property def convex_conj ( self ) :          return self . left . convex_conj + self . right . convex_conj  def __repr__ ( self ) :          posargs = [ self . left , self . right ] inner_str = signature_string ( posargs , [ ] , sep = <str> ) return <str> . format ( self . __class__ . __name__ , indent ( inner_str ) )  def __str__ ( self ) :          return repr ( self )   class FunctionalQuadraticPerturb ( Functional ) :      def __init__ ( self , func , quadratic_coeff = 0 , linear_term = None , constant = 0 ) :          if not isinstance ( func , Functional ) :              raise TypeError ( <str> <str> . format ( func ) )  self . __functional = func quadratic_coeff = func . domain . field . element ( quadratic_coeff ) if quadratic_coeff . imag != 0 :              raise ValueError ( <str> )  self . __quadratic_coeff = quadratic_coeff . real if linear_term is not None :              self . __linear_term = func . domain . element ( linear_term )  else :              self . __linear_term = func . domain . zero ( )  if linear_term is None :              grad_lipschitz = func . grad_lipschitz  else :              grad_lipschitz = ( func . grad_lipschitz + self . linear_term . norm ( ) )  constant = func . domain . field . element ( constant ) if constant . imag != 0 :              raise ValueError ( <str> )  self . __constant = constant . real super ( FunctionalQuadraticPerturb , self ) . __init__ ( space = func . domain , linear = func . is_linear and ( quadratic_coeff == 0 ) , grad_lipschitz = grad_lipschitz )  @ property def functional ( self ) :          return self . __functional  @ property def quadratic_coeff ( self ) :          return self . __quadratic_coeff  @ property def linear_term ( self ) :          return self . __linear_term  @ property def constant ( self ) :          return self . __constant  def _call ( self , x ) :          return ( self . functional ( x ) + self . quadratic_coeff * x . inner ( x ) + x . inner ( self . linear_term ) + self . constant )  @ property def gradient ( self ) :          return ( self . functional . gradient + ( 2 * self . quadratic_coeff ) * IdentityOperator ( self . domain ) + ConstantOperator ( self . linear_term ) )  @ property def proximal ( self ) :          if self . quadratic_coeff < 0 :              raise TypeError ( <str> <str> . format ( self . quadratic_coeff ) )  return proximal_quadratic_perturbation ( self . functional . proximal , a = self . quadratic_coeff , u = self . linear_term )  @ property def convex_conj ( self ) :          <str> if self . quadratic_coeff == 0 :              cconj = self . functional . convex_conj . translated ( self . linear_term ) if self . constant != 0 :                  cconj = cconj - self . constant  return cconj  else :              return super ( FunctionalQuadraticPerturb , self ) . convex_conj   def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . functional , self . quadratic_coeff , self . linear_term , self . constant )  def __str__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . functional , self . quadratic_coeff , self . linear_term , self . constant )   class FunctionalProduct ( Functional , OperatorPointwiseProduct ) :      def __init__ ( self , left , right ) :          if not isinstance ( left , Functional ) :              raise TypeError ( <str> <str> . format ( left ) )  if not isinstance ( right , Functional ) :              raise TypeError ( <str> <str> . format ( right ) )  OperatorPointwiseProduct . __init__ ( self , left , right ) Functional . __init__ ( self , left . domain , linear = False , grad_lipschitz = np . nan )  @ property def gradient ( self ) :          <str> func = self class FunctionalProductGradient ( Operator ) :              def _call ( self , x ) :                  return ( func . right ( x ) * func . left . gradient ( x ) + func . left ( x ) * func . right . gradient ( x ) )   return FunctionalProductGradient ( self . domain , self . domain , linear = False )   class FunctionalQuotient ( Functional ) :      def __init__ ( self , dividend , divisor ) :          if not isinstance ( dividend , Functional ) :              raise TypeError ( <str> <str> . format ( dividend ) )  if not isinstance ( divisor , Functional ) :              raise TypeError ( <str> <str> . format ( divisor ) )  if dividend . domain != divisor . domain :              raise ValueError ( <str> )  self . __dividend = dividend self . __divisor = divisor super ( FunctionalQuotient , self ) . __init__ ( dividend . domain , linear = False , grad_lipschitz = np . nan )  @ property def dividend ( self ) :          return self . __dividend  @ property def divisor ( self ) :          return self . __divisor  def _call ( self , x ) :          return self . dividend ( x ) / self . divisor ( x )  @ property def gradient ( self ) :          <str> func = self class FunctionalQuotientGradient ( Operator ) :              def _call ( self , x ) :                  dividendx = func . dividend ( x ) divisorx = func . divisor ( x ) return ( ( 1 / divisorx ) * func . dividend . gradient ( x ) + ( - dividendx / divisorx ** 2 ) * func . divisor . gradient ( x ) )   return FunctionalQuotientGradient ( self . domain , self . domain , linear = False )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . dividend , self . divisor )  def __str__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . dividend , self . divisor )   class FunctionalDefaultConvexConjugate ( Functional ) :      <str> def __init__ ( self , func ) :          if not isinstance ( func , Functional ) :              raise TypeError ( <str> <str> . format ( func ) )  super ( FunctionalDefaultConvexConjugate , self ) . __init__ ( space = func . domain , linear = func . is_linear ) self . __convex_conj = func  @ property def convex_conj ( self ) :          return self . __convex_conj  @ property def proximal ( self ) :          return proximal_convex_conj ( self . convex_conj . proximal )  def __repr__ ( self ) :          return <str> . format ( self . convex_conj )  def __str__ ( self ) :          return <str> . format ( self . convex_conj )   class BregmanDistance ( Functional ) :      <str> def __init__ ( self , functional , point , subgrad ) :          if not isinstance ( functional , Functional ) :              raise TypeError ( <str> <str> . format ( functional ) )  self . __functional = functional if point not in functional . domain :              raise ValueError ( <str> <str> . format ( point , functional . domain ) )  self . __point = point if subgrad not in functional . domain :              raise TypeError ( <str> <str> . format ( subgrad ) )  self . __subgrad = subgrad self . __constant = - functional ( point ) + subgrad . inner ( point ) self . __bregman_dist = FunctionalQuadraticPerturb ( functional , linear_term = - subgrad , constant = self . __constant ) grad_lipschitz = functional . grad_lipschitz + subgrad . norm ( ) super ( BregmanDistance , self ) . __init__ ( space = functional . domain , linear = False , grad_lipschitz = grad_lipschitz )  @ property def functional ( self ) :          return self . __functional  @ property def point ( self ) :          return self . __point  @ property def subgrad ( self ) :          return self . __subgrad  def _call ( self , x ) :          return self . __bregman_dist ( x )  @ property def convex_conj ( self ) :          return self . __bregman_dist . convex_conj  @ property def proximal ( self ) :          return self . __bregman_dist . proximal  @ property def gradient ( self ) :          try :              op_to_return = self . functional . gradient  except NotImplementedError :              raise NotImplementedError ( <str> <str> . format ( self . functional ) )  op_to_return = op_to_return - ConstantOperator ( self . subgrad ) return op_to_return  def __repr__ ( self ) :          posargs = [ self . functional , self . point , self . subgrad ] optargs = [ ] inner_str = signature_string ( posargs , optargs , sep = <str> ) return <str> . format ( self . __class__ . __name__ , indent ( inner_str ) )   def simple_functional ( space , fcall = None , grad = None , prox = None , grad_lip = np . nan , convex_conj_fcall = None , convex_conj_grad = None , convex_conj_prox = None , convex_conj_grad_lip = np . nan , linear = False ) :      if grad is not None and not isinstance ( grad , Operator ) :          grad_in = grad class SimpleFunctionalGradient ( Operator ) :              def _call ( self , x ) :                  return grad_in ( x )   grad = SimpleFunctionalGradient ( space , space , linear = False )  if ( convex_conj_grad is not None and not isinstance ( convex_conj_grad , Operator ) ) :          convex_conj_grad_in = convex_conj_grad class SimpleFunctionalConvexConjGradient ( Operator ) :              def _call ( self , x ) :                  return convex_conj_grad_in ( x )   convex_conj_grad = SimpleFunctionalConvexConjGradient ( space , space , linear = False )  class SimpleFunctional ( Functional ) :          def __init__ ( self ) :              super ( SimpleFunctional , self ) . __init__ ( space , linear = linear , grad_lipschitz = grad_lip )  def _call ( self , x ) :              if fcall is None :                  raise NotImplementedError ( <str> )  else :                  return fcall ( x )   @ property def proximal ( self ) :              if prox is None :                  raise NotImplementedError ( <str> )  else :                  return prox   @ property def gradient ( self ) :              if grad is None :                  raise NotImplementedError ( <str> )  else :                  return grad   @ property def convex_conj ( self ) :              return simple_functional ( space , fcall = convex_conj_fcall , grad = convex_conj_grad , prox = convex_conj_prox , grad_lip = convex_conj_grad_lip , convex_conj_fcall = fcall , convex_conj_grad = grad , convex_conj_prox = prox , convex_conj_grad_lip = grad_lip , linear = linear )   return SimpleFunctional ( )  if __name__ == <str> :      from odl . util . testutils import run_doctests run_doctests ( )   