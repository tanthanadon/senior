from __future__ import absolute_import , division , print_function from numbers import Integral import numpy as np from odl . operator import ( ConstantOperator , DiagonalOperator , Operator , PointwiseNorm , ScalingOperator , ZeroOperator ) from odl . solvers . functional . functional import ( Functional , FunctionalQuadraticPerturb ) from odl . solvers . nonsmooth . proximal_operators import ( combine_proximals , proj_simplex , proximal_box_constraint , proximal_const_func , proximal_convex_conj , proximal_convex_conj_kl , proximal_convex_conj_kl_cross_entropy , proximal_convex_conj_l1 , proximal_convex_conj_l1_l2 , proximal_convex_conj_l2 , proximal_convex_conj_linfty , proximal_huber , proximal_l1 , proximal_l1_l2 , proximal_l2 , proximal_l2_squared , proximal_linfty ) from odl . space import ProductSpace from odl . util import conj_exponent __all__ = ( <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ) class LpNorm ( Functional ) :      <str> def __init__ ( self , space , exponent ) :          super ( LpNorm , self ) . __init__ ( space = space , linear = False , grad_lipschitz = np . nan ) self . exponent = float ( exponent )  def _call ( self , x ) :          if self . exponent == 0 :              return self . domain . one ( ) . inner ( np . not_equal ( x , 0 ) )  elif self . exponent == 1 :              return x . ufuncs . absolute ( ) . inner ( self . domain . one ( ) )  elif self . exponent == 2 :              return np . sqrt ( x . inner ( x ) )  elif np . isfinite ( self . exponent ) :              tmp = x . ufuncs . absolute ( ) tmp . ufuncs . power ( self . exponent , out = tmp ) return np . power ( tmp . inner ( self . domain . one ( ) ) , 1 / self . exponent )  elif self . exponent == np . inf :              return x . ufuncs . absolute ( ) . ufuncs . max ( )  elif self . exponent == - np . inf :              return x . ufuncs . absolute ( ) . ufuncs . min ( )  else :              raise RuntimeError ( <str> )   @ property def convex_conj ( self ) :          return IndicatorLpUnitBall ( self . domain , exponent = conj_exponent ( self . exponent ) )  @ property def proximal ( self ) :          if self . exponent == 1 :              return proximal_l1 ( space = self . domain )  elif self . exponent == 2 :              return proximal_l2 ( space = self . domain )  elif self . exponent == np . inf :              return proximal_linfty ( space = self . domain )  else :              raise NotImplementedError ( <str> <str> )   @ property def gradient ( self ) :          functional = self if self . exponent == 1 :              class L1Gradient ( Operator ) :                  def __init__ ( self ) :                      super ( L1Gradient , self ) . __init__ ( functional . domain , functional . domain , linear = False )  def _call ( self , x ) :                      return x . ufuncs . sign ( )  def derivative ( self , x ) :                      return ZeroOperator ( self . domain )   return L1Gradient ( )  elif self . exponent == 2 :              class L2Gradient ( Operator ) :                  def __init__ ( self ) :                      super ( L2Gradient , self ) . __init__ ( functional . domain , functional . domain , linear = False )  def _call ( self , x ) :                      norm_of_x = x . norm ( ) if norm_of_x == 0 :                          return self . domain . zero ( )  else :                          return x / norm_of_x    return L2Gradient ( )  else :              raise NotImplementedError ( <str> <str> )   def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . exponent )   class GroupL1Norm ( Functional ) :      <str> def __init__ ( self , vfspace , exponent = None ) :          if not isinstance ( vfspace , ProductSpace ) :              raise TypeError ( <str> )  if not vfspace . is_power_space :              raise TypeError ( <str> )  super ( GroupL1Norm , self ) . __init__ ( space = vfspace , linear = False , grad_lipschitz = np . nan ) self . pointwise_norm = PointwiseNorm ( vfspace , exponent )  def _call ( self , x ) :          pointwise_norm = self . pointwise_norm ( x ) return pointwise_norm . inner ( pointwise_norm . space . one ( ) )  @ property def gradient ( self ) :          <str> functional = self class GroupL1Gradient ( Operator ) :              def __init__ ( self ) :                  super ( GroupL1Gradient , self ) . __init__ ( functional . domain , functional . domain , linear = False )  def _call ( self , x , out ) :                  pwnorm_x = functional . pointwise_norm ( x ) pwnorm_x . ufuncs . sign ( out = pwnorm_x ) functional . pointwise_norm . derivative ( x ) . adjoint ( pwnorm_x , out = out ) return out   return GroupL1Gradient ( )  @ property def proximal ( self ) :          if self . pointwise_norm . exponent == 1 :              return proximal_l1 ( space = self . domain )  elif self . pointwise_norm . exponent == 2 :              return proximal_l1_l2 ( space = self . domain )  else :              raise NotImplementedError ( <str> <str> )   @ property def convex_conj ( self ) :          conj_exp = conj_exponent ( self . pointwise_norm . exponent ) return IndicatorGroupL1UnitBall ( self . domain , exponent = conj_exp )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . pointwise_norm . exponent )   class IndicatorGroupL1UnitBall ( Functional ) :      def __init__ ( self , vfspace , exponent = None ) :          if not isinstance ( vfspace , ProductSpace ) :              raise TypeError ( <str> )  if not vfspace . is_power_space :              raise TypeError ( <str> )  super ( IndicatorGroupL1UnitBall , self ) . __init__ ( space = vfspace , linear = False , grad_lipschitz = np . nan ) self . pointwise_norm = PointwiseNorm ( vfspace , exponent )  def _call ( self , x ) :          x_norm = self . pointwise_norm ( x ) . ufuncs . max ( ) if x_norm > 1 :              return np . inf  else :              return 0   @ property def proximal ( self ) :          if self . pointwise_norm . exponent == np . inf :              return proximal_convex_conj_l1 ( space = self . domain )  elif self . pointwise_norm . exponent == 2 :              return proximal_convex_conj_l1_l2 ( space = self . domain )  else :              raise NotImplementedError ( <str> <str> )   @ property def convex_conj ( self ) :          conj_exp = conj_exponent ( self . pointwise_norm . exponent ) return GroupL1Norm ( self . domain , exponent = conj_exp )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . pointwise_norm . exponent )   class IndicatorLpUnitBall ( Functional ) :      <str> def __init__ ( self , space , exponent ) :          super ( IndicatorLpUnitBall , self ) . __init__ ( space = space , linear = False ) self . __norm = LpNorm ( space , exponent ) self . __exponent = float ( exponent )  @ property def exponent ( self ) :          return self . __exponent  def _call ( self , x ) :          x_norm = self . __norm ( x ) if x_norm > 1 :              return np . inf  else :              return 0   @ property def convex_conj ( self ) :          if self . exponent == np . inf :              return L1Norm ( self . domain )  elif self . exponent == 2 :              return L2Norm ( self . domain )  else :              return LpNorm ( self . domain , exponent = conj_exponent ( self . exponent ) )   @ property def proximal ( self ) :          if self . exponent == np . inf :              return proximal_convex_conj_l1 ( space = self . domain )  elif self . exponent == 2 :              return proximal_convex_conj_l2 ( space = self . domain )  elif self . exponent == 1 :              return proximal_convex_conj_linfty ( space = self . domain )  else :              raise NotImplementedError ( <str> <str> )   def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . exponent )   class L1Norm ( LpNorm ) :      <str> def __init__ ( self , space ) :          super ( L1Norm , self ) . __init__ ( space = space , exponent = 1 )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain )   class L2Norm ( LpNorm ) :      <str> def __init__ ( self , space ) :          super ( L2Norm , self ) . __init__ ( space = space , exponent = 2 )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain )   class L2NormSquared ( Functional ) :      <str> def __init__ ( self , space ) :          super ( L2NormSquared , self ) . __init__ ( space = space , linear = False , grad_lipschitz = 2 )  def _call ( self , x ) :          return x . inner ( x )  @ property def gradient ( self ) :          return ScalingOperator ( self . domain , 2.0 )  @ property def proximal ( self ) :          return proximal_l2_squared ( space = self . domain )  @ property def convex_conj ( self ) :          <str> return ( 1.0 / 4 ) * L2NormSquared ( self . domain )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain )   class ConstantFunctional ( Functional ) :      def __init__ ( self , space , constant ) :          super ( ConstantFunctional , self ) . __init__ ( space = space , linear = ( constant == 0 ) , grad_lipschitz = 0 ) self . __constant = self . range . element ( constant )  @ property def constant ( self ) :          return self . __constant  def _call ( self , x ) :          return self . constant  @ property def gradient ( self ) :          return ZeroOperator ( self . domain )  @ property def proximal ( self ) :          return proximal_const_func ( self . domain )  @ property def convex_conj ( self ) :          <str> return IndicatorZero ( self . domain , - self . constant )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . constant )   class ZeroFunctional ( ConstantFunctional ) :      def __init__ ( self , space ) :          super ( ZeroFunctional , self ) . __init__ ( space = space , constant = 0 )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain )   class ScalingFunctional ( Functional , ScalingOperator ) :      def __init__ ( self , field , scale ) :          Functional . __init__ ( self , space = field , linear = True , grad_lipschitz = 0 ) ScalingOperator . __init__ ( self , field , scale )  @ property def gradient ( self ) :          return ConstantFunctional ( self . domain , self . scalar )   class IdentityFunctional ( ScalingFunctional ) :      def __init__ ( self , field ) :          super ( IdentityFunctional , self ) . __init__ ( field , 1.0 )   class IndicatorBox ( Functional ) :      <str> def __init__ ( self , space , lower = None , upper = None ) :          super ( IndicatorBox , self ) . __init__ ( space , linear = False ) self . lower = lower self . upper = upper  def _call ( self , x ) :          proj = self . proximal ( 1 ) ( x ) return np . inf if x . dist ( proj ) > 0 else 0  @ property def proximal ( self ) :          return proximal_box_constraint ( self . domain , self . lower , self . upper )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . lower , self . upper )   class IndicatorNonnegativity ( IndicatorBox ) :      <str> def __init__ ( self , space ) :          super ( IndicatorNonnegativity , self ) . __init__ ( space , lower = 0 , upper = None )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain )   class IndicatorZero ( Functional ) :      def __init__ ( self , space , constant = 0 ) :          super ( IndicatorZero , self ) . __init__ ( space , linear = False ) self . __constant = constant  @ property def constant ( self ) :          return self . __constant  def _call ( self , x ) :          if x . norm ( ) == 0 :              return self . constant  else :              return np . inf   @ property def convex_conj ( self ) :          return ConstantFunctional ( self . domain , - self . constant )  @ property def proximal ( self ) :          def zero_proximal ( sigma = 1.0 ) :              return ZeroOperator ( self . domain )  return zero_proximal  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . constant )   class KullbackLeibler ( Functional ) :      <str> def __init__ ( self , space , prior = None ) :          super ( KullbackLeibler , self ) . __init__ ( space = space , linear = False , grad_lipschitz = np . nan ) if prior is not None and prior not in self . domain :              raise ValueError ( <str> <str> . format ( prior , self . domain ) )  self . __prior = prior  @ property def prior ( self ) :          return self . __prior  def _call ( self , x ) :          import scipy . special with np . errstate ( invalid = <str> , divide = <str> ) :              if self . prior is None :                  res = ( x - 1 - np . log ( x ) ) . inner ( self . domain . one ( ) )  else :                  xlogy = scipy . special . xlogy ( self . prior , self . prior / x ) res = ( x - self . prior + xlogy ) . inner ( self . domain . one ( ) )   if not np . isfinite ( res ) :              return np . inf  else :              return res   @ property def gradient ( self ) :          <str> functional = self class KLGradient ( Operator ) :              def __init__ ( self ) :                  super ( KLGradient , self ) . __init__ ( functional . domain , functional . domain , linear = False )  def _call ( self , x ) :                  if functional . prior is None :                      return ( - 1.0 ) / x + 1  else :                      return ( - functional . prior ) / x + 1    return KLGradient ( )  @ property def proximal ( self ) :          return proximal_convex_conj ( proximal_convex_conj_kl ( space = self . domain , g = self . prior ) )  @ property def convex_conj ( self ) :          return KullbackLeiblerConvexConj ( self . domain , self . prior )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . prior )   class KullbackLeiblerConvexConj ( Functional ) :      <str> def __init__ ( self , space , prior = None ) :          super ( KullbackLeiblerConvexConj , self ) . __init__ ( space = space , linear = False , grad_lipschitz = np . nan ) if prior is not None and prior not in self . domain :              raise ValueError ( <str> <str> . format ( prior , self . domain ) )  self . __prior = prior  @ property def prior ( self ) :          return self . __prior  def _call ( self , x ) :          import scipy . special with np . errstate ( invalid = <str> ) :              if self . prior is None :                  res = - ( np . log ( 1 - x ) ) . inner ( self . domain . one ( ) )  else :                  xlogy = scipy . special . xlogy ( self . prior , 1 - x ) res = - self . domain . element ( xlogy ) . inner ( self . domain . one ( ) )   if not np . isfinite ( res ) :              return np . inf  else :              return res   @ property def gradient ( self ) :          functional = self class KLCCGradient ( Operator ) :              def __init__ ( self ) :                  super ( KLCCGradient , self ) . __init__ ( functional . domain , functional . domain , linear = False )  def _call ( self , x ) :                  if functional . prior is None :                      return 1.0 / ( 1 - x )  else :                      return functional . prior / ( 1 - x )    return KLCCGradient ( )  @ property def proximal ( self ) :          return proximal_convex_conj_kl ( space = self . domain , g = self . prior )  @ property def convex_conj ( self ) :          return KullbackLeibler ( self . domain , self . prior )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . prior )   class KullbackLeiblerCrossEntropy ( Functional ) :      <str> def __init__ ( self , space , prior = None ) :          super ( KullbackLeiblerCrossEntropy , self ) . __init__ ( space = space , linear = False , grad_lipschitz = np . nan ) if prior is not None and prior not in self . domain :              raise ValueError ( <str> <str> . format ( prior , self . domain ) )  self . __prior = prior  @ property def prior ( self ) :          return self . __prior  def _call ( self , x ) :          import scipy . special with np . errstate ( invalid = <str> , divide = <str> ) :              if self . prior is None :                  xlogx = scipy . special . xlogy ( x , x ) res = ( 1 - x + xlogx ) . inner ( self . domain . one ( ) )  else :                  xlogy = scipy . special . xlogy ( x , x / self . prior ) res = ( self . prior - x + xlogy ) . inner ( self . domain . one ( ) )   if not np . isfinite ( res ) :              return np . inf  else :              return res   @ property def gradient ( self ) :          functional = self class KLCrossEntropyGradient ( Operator ) :              def __init__ ( self ) :                  super ( KLCrossEntropyGradient , self ) . __init__ ( functional . domain , functional . domain , linear = False )  def _call ( self , x ) :                  if functional . prior is None :                      tmp = np . log ( x )  else :                      tmp = np . log ( x / functional . prior )  if np . all ( np . isfinite ( tmp ) ) :                      return tmp  else :                      raise ValueError ( <str> <str> <str> <str> . format ( x ) )    return KLCrossEntropyGradient ( )  @ property def proximal ( self ) :          return proximal_convex_conj ( proximal_convex_conj_kl_cross_entropy ( space = self . domain , g = self . prior ) )  @ property def convex_conj ( self ) :          return KullbackLeiblerCrossEntropyConvexConj ( self . domain , self . prior )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . prior )   class KullbackLeiblerCrossEntropyConvexConj ( Functional ) :      <str> def __init__ ( self , space , prior = None ) :          super ( KullbackLeiblerCrossEntropyConvexConj , self ) . __init__ ( space = space , linear = False , grad_lipschitz = np . nan ) if prior is not None and prior not in self . domain :              raise ValueError ( <str> <str> . format ( prior , self . domain ) )  self . __prior = prior  @ property def prior ( self ) :          return self . __prior  def _call ( self , x ) :          if self . prior is None :              tmp = self . domain . element ( ( np . exp ( x ) - 1 ) ) . inner ( self . domain . one ( ) )  else :              tmp = ( self . prior * ( np . exp ( x ) - 1 ) ) . inner ( self . domain . one ( ) )  return tmp  @ property def gradient ( self ) :          functional = self class KLCrossEntCCGradient ( Operator ) :              def __init__ ( self ) :                  super ( KLCrossEntCCGradient , self ) . __init__ ( functional . domain , functional . domain , linear = False )  def _call ( self , x ) :                  if functional . prior is None :                      return self . domain . element ( np . exp ( x ) )  else :                      return functional . prior * np . exp ( x )    return KLCrossEntCCGradient ( )  @ property def proximal ( self ) :          return proximal_convex_conj_kl_cross_entropy ( space = self . domain , g = self . prior )  @ property def convex_conj ( self ) :          return KullbackLeiblerCrossEntropy ( self . domain , self . prior )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . prior )   class SeparableSum ( Functional ) :      <str> def __init__ ( self , * functionals ) :          <str> if ( len ( functionals ) == 2 and isinstance ( functionals [ 1 ] , Integral ) ) :              functionals = [ functionals [ 0 ] ] * functionals [ 1 ]  if not all ( isinstance ( op , Functional ) for op in functionals ) :              raise TypeError ( <str> )  domains = [ func . domain for func in functionals ] domain = ProductSpace ( * domains ) linear = all ( func . is_linear for func in functionals ) super ( SeparableSum , self ) . __init__ ( space = domain , linear = linear ) self . __functionals = tuple ( functionals )  def _call ( self , x ) :          return sum ( fi ( xi ) for xi , fi in zip ( x , self . functionals ) )  @ property def functionals ( self ) :          return self . __functionals  def __getitem__ ( self , indices ) :          result = self . functionals [ indices ] if isinstance ( result , tuple ) :              return SeparableSum ( * result )  else :              return result   @ property def gradient ( self ) :          gradients = [ func . gradient for func in self . functionals ] return DiagonalOperator ( * gradients )  @ property def proximal ( self ) :          proximals = [ func . proximal for func in self . functionals ] return combine_proximals ( * proximals )  @ property def convex_conj ( self ) :          convex_conjs = [ func . convex_conj for func in self . functionals ] return SeparableSum ( * convex_conjs )  def __repr__ ( self ) :          func_repr = <str> . join ( repr ( func ) for func in self . functionals ) return <str> . format ( self . __class__ . __name__ , func_repr )   class QuadraticForm ( Functional ) :      def __init__ ( self , operator = None , vector = None , constant = 0 ) :          if operator is None and vector is None :              raise ValueError ( <str> <str> )  if operator is not None :              domain = operator . domain  elif vector is not None :              domain = vector . space  if ( operator is not None and vector is not None and vector not in operator . domain ) :              raise ValueError ( <str> <str> )  super ( QuadraticForm , self ) . __init__ ( space = domain , linear = ( operator is None and constant == 0 ) ) self . __operator = operator self . __vector = vector self . __constant = constant if self . constant not in self . range :              raise ValueError ( <str> <str> )   @ property def operator ( self ) :          return self . __operator  @ property def vector ( self ) :          return self . __vector  @ property def constant ( self ) :          return self . __constant  def _call ( self , x ) :          if self . operator is None :              return self . vector . inner ( x ) + self . constant  elif self . vector is None :              return x . inner ( self . operator ( x ) ) + self . constant  else :              tmp = self . operator ( x ) tmp += self . vector return x . inner ( tmp ) + self . constant   @ property def gradient ( self ) :          if self . operator is None :              return ConstantOperator ( self . vector , self . domain )  else :              if not self . operator . is_linear :                  raise NotImplementedError ( <str> )  opadjoint = self . operator . adjoint if opadjoint == self . operator :                  gradient = 2 * self . operator  else :                  gradient = self . operator + opadjoint  if self . vector is None :                  return gradient  else :                  return gradient + self . vector    @ property def convex_conj ( self ) :          <str> if self . operator is None :              tmp = IndicatorZero ( space = self . domain , constant = - self . constant ) if self . vector is None :                  return tmp  else :                  return tmp . translated ( self . vector )   if self . vector is None :              return QuadraticForm ( operator = self . operator . inverse , constant = - self . constant )  else :              opinv = self . operator . inverse vector = - opinv . adjoint ( self . vector ) - opinv ( self . vector ) constant = self . vector . inner ( opinv ( self . vector ) ) - self . constant return QuadraticForm ( operator = opinv , vector = vector , constant = constant )    class NuclearNorm ( Functional ) :      <str> def __init__ ( self , space , outer_exp = 1 , singular_vector_exp = 2 ) :          if ( not isinstance ( space , ProductSpace ) or not isinstance ( space [ 0 ] , ProductSpace ) ) :              raise TypeError ( <str> <str> )  if ( not space . is_power_space or not space [ 0 ] . is_power_space ) :              raise TypeError ( <str> )  super ( NuclearNorm , self ) . __init__ ( space = space , linear = False , grad_lipschitz = np . nan ) self . outernorm = LpNorm ( self . domain [ 0 , 0 ] , exponent = outer_exp ) self . pwisenorm = PointwiseNorm ( self . domain [ 0 ] , exponent = singular_vector_exp ) self . pshape = ( len ( self . domain ) , len ( self . domain [ 0 ] ) )  def _asarray ( self , vec ) :          shape = self . domain [ 0 , 0 ] . shape + self . pshape arr = np . empty ( shape , dtype = self . domain . dtype ) for i , xi in enumerate ( vec ) :              for j , xij in enumerate ( xi ) :                  arr [ ... , i , j ] = xij . asarray ( )   return arr  def _asvector ( self , arr ) :          result = np . moveaxis ( arr , [ - 2 , - 1 ] , [ 0 , 1 ] ) return self . domain . element ( result )  def _call ( self , x ) :          arr = self . _asarray ( x ) svd_diag = np . linalg . svd ( arr , compute_uv = False ) s_reordered = np . moveaxis ( svd_diag , - 1 , 0 ) return self . outernorm ( self . pwisenorm ( s_reordered ) )  @ property def proximal ( self ) :          if self . outernorm . exponent != 1 :              raise NotImplementedError ( <str> <str> )  if self . pwisenorm . exponent not in [ 1 , 2 , np . inf ] :              raise NotImplementedError ( <str> <str> )  def nddot ( a , b ) :              return np . einsum ( <str> , a , b )  func = self dtype = getattr ( self . domain , <str> , float ) eps = np . finfo ( dtype ) . resolution * 10 class NuclearNormProximal ( Operator ) :              def __init__ ( self , sigma ) :                  self . sigma = float ( sigma ) super ( NuclearNormProximal , self ) . __init__ ( func . domain , func . domain , linear = False )  def _call ( self , x ) :                  arr = func . _asarray ( x ) U , s , Vt = np . linalg . svd ( arr , full_matrices = False ) V = Vt . swapaxes ( - 1 , - 2 ) sinv = s . copy ( ) sinv [ sinv != 0 ] = 1 / sinv [ sinv != 0 ] if func . pwisenorm . exponent == 1 :                      abss = np . abs ( s ) - ( self . sigma - eps ) sprox = np . sign ( s ) * np . maximum ( abss , 0 )  elif func . pwisenorm . exponent == 2 :                      s_reordered = np . moveaxis ( s , - 1 , 0 ) snorm = func . pwisenorm ( s_reordered ) . asarray ( ) snorm = np . maximum ( self . sigma , snorm , out = snorm ) sprox = ( ( 1 - eps ) - self . sigma / snorm ) [ ... , None ] * s  elif func . pwisenorm . exponent == np . inf :                      snorm = np . sum ( np . abs ( s ) , axis = - 1 ) snorm = np . maximum ( self . sigma , snorm , out = snorm ) sprox = ( ( 1 - eps ) - self . sigma / snorm ) [ ... , None ] * s  else :                      raise RuntimeError  sproxsinv = ( sprox * sinv ) [ ... , : , None ] result = nddot ( nddot ( arr , V ) , sproxsinv * Vt ) return func . _asvector ( result )  def __repr__ ( self ) :                  return <str> . format ( func , self . sigma )   return NuclearNormProximal  @ property def convex_conj ( self ) :          return IndicatorNuclearNormUnitBall ( self . domain , conj_exponent ( self . outernorm . exponent ) , conj_exponent ( self . pwisenorm . exponent ) )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . outernorm . exponent , self . pwisenorm . exponent )   class IndicatorNuclearNormUnitBall ( Functional ) :      <str> def __init__ ( self , space , outer_exp = 1 , singular_vector_exp = 2 ) :          super ( IndicatorNuclearNormUnitBall , self ) . __init__ ( space = space , linear = False , grad_lipschitz = np . nan ) self . __norm = NuclearNorm ( space , outer_exp , singular_vector_exp )  def _call ( self , x ) :          x_norm = self . __norm ( x ) if x_norm > 1 :              return np . inf  else :              return 0   @ property def proximal ( self ) :          return proximal_convex_conj ( self . convex_conj . proximal )  @ property def convex_conj ( self ) :          return NuclearNorm ( self . domain , conj_exponent ( self . __norm . outernorm . exponent ) , conj_exponent ( self . __norm . pwisenorm . exponent ) )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . __norm . outernorm . exponent , self . __norm . pwisenorm . exponent )   class IndicatorSimplex ( Functional ) :      <str> def __init__ ( self , space , diameter = 1 , sum_rtol = None ) :          super ( IndicatorSimplex , self ) . __init__ ( space = space , linear = False , grad_lipschitz = np . nan ) self . diameter = float ( diameter ) if sum_rtol is None :              if space . dtype == <str> :                  sum_rtol = 1e-10 * self . domain . size  else :                  sum_rtol = 1e-6 * self . domain . size   self . sum_rtol = sum_rtol  def _call ( self , x ) :          sum_constr = abs ( x . ufuncs . sum ( ) / self . diameter - 1 ) <= self . sum_rtol nonneq_constr = x . ufuncs . greater_equal ( 0 ) . asarray ( ) . all ( ) if sum_constr and nonneq_constr :              return 0  else :              return np . inf   @ property def gradient ( self ) :          raise NotImplementedError ( <str> )  @ property def proximal ( self ) :          domain = self . domain diameter = self . diameter class ProximalSimplex ( Operator ) :              def __init__ ( self , sigma ) :                  self . sigma = sigma super ( ProximalSimplex , self ) . __init__ ( domain = domain , range = domain , linear = False )  def _call ( self , x , out ) :                  proj_simplex ( x , diameter , out )   return ProximalSimplex  @ property def convex_conj ( self ) :          raise NotImplementedError ( <str> )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain )   class IndicatorSumConstraint ( Functional ) :      <str> def __init__ ( self , space , sum_value = 1 , sum_rtol = None ) :          super ( IndicatorSumConstraint , self ) . __init__ ( space = space , linear = False , grad_lipschitz = np . nan ) if sum_rtol is None :              if space . dtype == <str> :                  sum_rtol = 1e-10 * self . domain . size  else :                  sum_rtol = 1e-6 * self . domain . size   self . sum_rtol = float ( sum_rtol ) self . sum_value = float ( sum_value )  def _call ( self , x ) :          if abs ( x . ufuncs . sum ( ) / self . sum_value - 1 ) <= self . sum_rtol :              return 0  else :              return np . inf   @ property def gradient ( self ) :          raise NotImplementedError ( <str> )  @ property def proximal ( self ) :          domain = self . domain class ProximalSum ( Operator ) :              def __init__ ( self , sigma ) :                  self . sigma = sigma super ( ProximalSum , self ) . __init__ ( domain = domain , range = domain , linear = False )  def _call ( self , x , out ) :                  offset = 1 / x . size * ( self . sum_value - x . ufuncs . sum ( ) ) out . assign ( x ) out += offset   return ProximalSum  @ property def convex_conj ( self ) :          raise NotImplementedError ( <str> )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain )   class MoreauEnvelope ( Functional ) :      <str> def __init__ ( self , functional , sigma = 1.0 ) :          super ( MoreauEnvelope , self ) . __init__ ( space = functional . domain , linear = False ) self . __functional = functional self . __sigma = sigma  @ property def functional ( self ) :          return self . __functional  @ property def sigma ( self ) :          return self . __sigma  @ property def gradient ( self ) :          return ( ScalingOperator ( self . domain , 1 / self . sigma ) - ( 1 / self . sigma ) * self . functional . proximal ( self . sigma ) )   class Huber ( Functional ) :      <str> def __init__ ( self , space , gamma ) :          self . __gamma = float ( gamma ) if self . gamma > 0 :              grad_lipschitz = 1 / self . gamma  else :              grad_lipschitz = np . inf  super ( Huber , self ) . __init__ ( space = space , linear = False , grad_lipschitz = grad_lipschitz )  @ property def gamma ( self ) :          return self . __gamma  def _call ( self , x ) :          if isinstance ( self . domain , ProductSpace ) :              norm = PointwiseNorm ( self . domain , 2 ) ( x )  else :              norm = x . ufuncs . absolute ( )  if self . gamma > 0 :              tmp = norm . ufuncs . square ( ) tmp *= 1 / ( 2 * self . gamma ) index = norm . ufuncs . greater_equal ( self . gamma ) tmp [ index ] = norm [ index ] - self . gamma / 2  else :              tmp = norm  return tmp . inner ( tmp . space . one ( ) )  @ property def convex_conj ( self ) :          if isinstance ( self . domain , ProductSpace ) :              norm = GroupL1Norm ( self . domain , 2 )  else :              norm = L1Norm ( self . domain )  return FunctionalQuadraticPerturb ( norm . convex_conj , quadratic_coeff = self . gamma / 2 )  @ property def proximal ( self ) :          return proximal_huber ( space = self . domain , gamma = self . gamma )  @ property def gradient ( self ) :          <str> functional = self class HuberGradient ( Operator ) :              def __init__ ( self ) :                  super ( HuberGradient , self ) . __init__ ( functional . domain , functional . domain , linear = False )  def _call ( self , x ) :                  if isinstance ( self . domain , ProductSpace ) :                      norm = PointwiseNorm ( self . domain , 2 ) ( x )  else :                      norm = x . ufuncs . absolute ( )  grad = x / functional . gamma index = norm . ufuncs . greater_equal ( functional . gamma ) if isinstance ( self . domain , ProductSpace ) :                      for xi , gi in zip ( x , grad ) :                          gi [ index ] = xi [ index ] / norm [ index ]   else :                      grad [ index ] = x [ index ] / norm [ index ]  return grad   return HuberGradient ( )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . gamma )   if __name__ == <str> :      from odl . util . testutils import run_doctests run_doctests ( )   