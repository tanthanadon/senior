from __future__ import print_function from builtins import super import numpy as np import odl import scipy . signal import matplotlib import matplotlib . pyplot as plt from skimage . io import imsave __all__ = ( <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ) def save_image ( image , name , folder , fignum , cmap = <str> , clim = None ) :      matplotlib . rc ( <str> , usetex = False ) fig = plt . figure ( fignum ) plt . clf ( ) image . show ( name , cmap = cmap , fig = fig ) fig . savefig ( <str> . format ( folder , name ) , bbox_inches = <str> ) if clim is None :          x = image - np . min ( image ) if np . max ( x ) > 1e-4 :              x /= np . max ( x )   else :          x = ( image - clim [ 0 ] ) / ( clim [ 1 ] - clim [ 0 ] )  x = np . minimum ( np . maximum ( x , 0 ) , 1 ) imsave ( <str> . format ( folder , name ) , np . rot90 ( x , 1 ) )  def save_signal ( signal , name , folder , fignum ) :      matplotlib . rc ( <str> , usetex = False ) fig = plt . figure ( fignum ) plt . clf ( ) signal . show ( name , fig = fig ) fig . savefig ( <str> . format ( folder , name ) , bbox_inches = <str> )  def bregman ( f , v , subgrad ) :      return ( odl . solvers . FunctionalQuadraticPerturb ( f , linear_term = - subgrad ) - f ( v ) + subgrad . inner ( v ) )  def partition_1d ( arr , slices ) :      return tuple ( arr [ slc ] for slc in slices )  def partition_equally_1d ( arr , nparts , order = <str> ) :      if order == <str> :          stride = int ( np . ceil ( arr . size / nparts ) ) slices = [ slice ( i * stride , ( i + 1 ) * stride ) for i in range ( nparts ) ]  elif order == <str> :          slices = [ slice ( i , len ( arr ) , nparts ) for i in range ( nparts ) ]  else :          raise ValueError  return partition_1d ( arr , tuple ( slices ) )  def divide_1Darray_equally ( ind , nsub ) :      n_ind = len ( ind ) sub2ind = partition_equally_1d ( ind , nsub , order = <str> ) ind2sub = [ ] for i in range ( n_ind ) :          ind2sub . append ( [ ] )  for i in range ( nsub ) :          for j in sub2ind [ i ] :              ind2sub [ j ] . append ( i )   return ( sub2ind , ind2sub )  def total_variation ( domain , grad = None ) :      if grad is None :          grad = odl . Gradient ( domain , method = <str> , pad_mode = <str> ) grad . norm = 2 * np . sqrt ( sum ( 1 / grad . domain . cell_sides ** 2 ) )  else :          grad = grad  f = odl . solvers . GroupL1Norm ( grad . range , exponent = 2 ) return f * grad  class TotalVariationNonNegative ( odl . solvers . Functional ) :      def __init__ ( self , domain , alpha = 1 , prox_options = { } , grad = None , strong_convexity = 0 ) :          self . strong_convexity = strong_convexity if <str> not in prox_options :              prox_options [ <str> ] = <str>  if <str> not in prox_options :              prox_options [ <str> ] = True  if <str> not in prox_options :              prox_options [ <str> ] = 5  if <str> not in prox_options :              prox_options [ <str> ] = None  if <str> not in prox_options :              prox_options [ <str> ] = None  self . prox_options = prox_options self . alpha = alpha self . tv = total_variation ( domain , grad = grad ) self . grad = self . tv . right self . nn = odl . solvers . IndicatorBox ( domain , 0 , np . inf ) self . l2 = 0.5 * odl . solvers . L2NormSquared ( domain ) self . proj_P = self . tv . left . convex_conj . proximal ( 0 ) self . proj_C = self . nn . proximal ( 1 ) super ( ) . __init__ ( space = domain , linear = False , grad_lipschitz = 0 )  def __call__ ( self , x ) :          nn = self . nn ( x ) if nn is np . inf :              return nn  else :              out = self . alpha * self . tv ( x ) + nn if self . strong_convexity > 0 :                  out += self . strong_convexity * self . l2 ( x )  return out   def proximal ( self , sigma ) :          if sigma == 0 :              return odl . IdentityOperator ( self . domain )  else :              def tv_prox ( z , out = None ) :                  if out is None :                      out = z . space . zero ( )  opts = self . prox_options sigma_ = np . copy ( sigma ) z_ = z . copy ( ) if self . strong_convexity > 0 :                      sigma_ /= ( 1 + sigma * self . strong_convexity ) z_ /= ( 1 + sigma * self . strong_convexity )  if opts [ <str> ] == <str> :                      if opts [ <str> ] :                          if opts [ <str> ] is None :                              opts [ <str> ] = self . grad . range . zero ( )  p = opts [ <str> ]  else :                          p = self . grad . range . zero ( )  sigma_sqrt = np . sqrt ( sigma_ ) z_ /= sigma_sqrt grad = sigma_sqrt * self . grad grad . norm = sigma_sqrt * self . grad . norm niter = opts [ <str> ] alpha = self . alpha out [ : ] = fgp_dual ( p , z_ , alpha , niter , grad , self . proj_C , self . proj_P , tol = opts [ <str> ] ) out *= sigma_sqrt return out  else :                      raise NotImplementedError ( <str> )   return tv_prox    def fgp_dual ( p , data , alpha , niter , grad , proj_C , proj_P , tol = None , ** kwargs ) :      callback = kwargs . pop ( <str> , None ) if callback is not None and not callable ( callback ) :          raise TypeError ( <str> . format ( callback ) )  factr = 1 / ( grad . norm ** 2 * alpha ) q = p . copy ( ) x = data . space . zero ( ) t = 1. if tol is None :          def convergence_eval ( p1 , p2 ) :              return False   else :          def convergence_eval ( p1 , p2 ) :              return ( p1 - p2 ) . norm ( ) / p1 . norm ( ) < tol   pnew = p . copy ( ) if callback is not None :          callback ( p )  for k in range ( niter ) :          t0 = t grad . adjoint ( q , out = x ) proj_C ( data - alpha * x , out = x ) grad ( x , out = pnew ) pnew *= factr pnew += q proj_P ( pnew , out = pnew ) converged = convergence_eval ( p , pnew ) if not converged :              t = ( 1 + np . sqrt ( 1 + 4 * t0 ** 2 ) ) / 2. q [ : ] = pnew + ( t0 - 1 ) / t * ( pnew - p )  p [ : ] = pnew if converged :              t = None break  if callback is not None :              callback ( p )   x = proj_C ( data - alpha * grad . adjoint ( p ) ) return x  class Blur2D ( odl . Operator ) :      def __init__ ( self , domain , kernel , boundary_condition = <str> ) :          super ( ) . __init__ ( domain = domain , range = domain , linear = True ) self . __kernel = kernel self . __boundary_condition = boundary_condition  @ property def kernel ( self ) :          return self . __kernel  @ property def boundary_condition ( self ) :          return self . __boundary_condition  def _call ( self , x , out ) :          out [ : ] = scipy . signal . convolve2d ( x , self . kernel , mode = <str> , boundary = <str> )  @ property def gradient ( self ) :          raise NotImplementedError ( <str> )  @ property def adjoint ( self ) :          adjoint_kernel = self . kernel . copy ( ) . conj ( ) adjoint_kernel = np . fliplr ( np . flipud ( adjoint_kernel ) ) return Blur2D ( self . domain , adjoint_kernel , self . boundary_condition )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . kernel , self . boundary_condition )   class KullbackLeiblerSmooth ( odl . solvers . Functional ) :      <str> def __init__ ( self , space , data , background ) :          self . strong_convexity = 0 if background . ufuncs . less_equal ( 0 ) . ufuncs . sum ( ) > 0 :              raise NotImplementedError ( <str> )  super ( ) . __init__ ( space = space , linear = False , grad_lipschitz = np . max ( data / background ** 2 ) ) if data not in self . domain :              raise ValueError ( <str> <str> . format ( data , self . domain ) )  self . __data = data self . __background = background  @ property def data ( self ) :          return self . __data  @ property def background ( self ) :          return self . __background  def _call ( self , x ) :          y = self . data r = self . background obj = self . domain . zero ( ) i = x . ufuncs . greater_equal ( 0 ) obj [ i ] = x [ i ] + r [ i ] - y [ i ] j = y . ufuncs . greater ( 0 ) k = i . ufuncs . logical_and ( j ) obj [ k ] += y [ k ] * ( y [ k ] / ( x [ k ] + r [ k ] ) ) . ufuncs . log ( ) i = i . ufuncs . logical_not ( ) obj [ i ] += ( y [ i ] / ( 2 * r [ i ] ** 2 ) * x [ i ] ** 2 + ( 1 - y [ i ] / r [ i ] ) * x [ i ] + r [ i ] - y [ i ] ) k = i . ufuncs . logical_and ( j ) obj [ k ] += y [ k ] * ( y [ k ] / r [ k ] ) . ufuncs . log ( ) return obj . inner ( self . domain . one ( ) )  @ property def gradient ( self ) :          raise NotImplementedError ( <str> )  @ property def proximal ( self ) :          raise NotImplementedError ( <str> )  @ property def convex_conj ( self ) :          return KullbackLeiblerSmoothConvexConj ( self . domain , self . data , self . background )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . data , self . background )   class KullbackLeiblerSmoothConvexConj ( odl . solvers . Functional ) :      <str> def __init__ ( self , space , data , background ) :          if background . ufuncs . less_equal ( 0 ) . ufuncs . sum ( ) > 0 :              raise NotImplementedError ( <str> )  super ( ) . __init__ ( space = space , linear = False , grad_lipschitz = np . inf ) if data is not None and data not in self . domain :              raise ValueError ( <str> <str> . format ( data , self . domain ) )  self . __data = data self . __background = background if np . min ( self . data ) == 0 :              self . strong_convexity = np . inf  else :              self . strong_convexity = np . min ( self . background ** 2 / self . data )   @ property def data ( self ) :          return self . __data  @ property def background ( self ) :          return self . __background  def _call ( self , x ) :          y = self . data r = self . background if x . ufuncs . greater_equal ( 1 ) . ufuncs . sum ( ) > 0 :              return np . inf  obj = self . domain . zero ( ) i = x . ufuncs . less ( 1 - y / r ) ry = r [ i ] ** 2 / y [ i ] obj [ i ] += ( ry / 2 * x [ i ] ** 2 + ( r [ i ] - ry ) * x [ i ] + ry / 2 + 3 / 2 * y [ i ] - 2 * r [ i ] ) j = y . ufuncs . greater ( 0 ) k = i . ufuncs . logical_and ( j ) obj [ k ] -= y [ k ] * ( y [ k ] / r [ k ] ) . ufuncs . log ( ) i = i . ufuncs . logical_not ( ) obj [ i ] -= r [ i ] * x [ i ] k = i . ufuncs . logical_and ( j ) obj [ k ] -= y [ k ] * ( 1 - x [ k ] ) . ufuncs . log ( ) return obj . inner ( self . domain . one ( ) )  @ property def gradient ( self ) :          raise NotImplementedError ( <str> )  @ property def proximal ( self ) :          space = self . domain y = self . data r = self . background class ProxKullbackLeiblerSmoothConvexConj ( odl . Operator ) :              def __init__ ( self , sigma ) :                  self . sigma = float ( sigma ) self . background = r self . data = y super ( ) . __init__ ( domain = space , range = space , linear = False )  def _call ( self , x , out ) :                  s = self . sigma y = self . data r = self . background sr = s * r sy = s * y i = x . ufuncs . less ( 1 - y / r ) out [ i ] = ( ( y [ i ] * x [ i ] - sr [ i ] * y [ i ] + sr [ i ] * r [ i ] ) / ( y [ i ] + sr [ i ] * r [ i ] ) ) i . ufuncs . logical_not ( out = i ) out [ i ] = ( x [ i ] + sr [ i ] + 1 - ( ( x [ i ] + sr [ i ] - 1 ) ** 2 + 4 * sy [ i ] ) . ufuncs . sqrt ( ) ) out [ i ] /= 2 return out   return ProxKullbackLeiblerSmoothConvexConj  @ property def convex_conj ( self ) :          return KullbackLeiblerSmooth ( self . domain , self . data , self . background )  def __repr__ ( self ) :          return <str> . format ( self . __class__ . __name__ , self . domain , self . data , self . background )    