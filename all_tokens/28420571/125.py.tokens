from __future__ import print_function , division , absolute_import import numpy as np from odl . solvers . util import ConstantLineSearch __all__ = ( <str> , <str> ) def steepest_descent ( f , x , line_search = 1.0 , maxiter = 1000 , tol = 1e-16 , projection = None , callback = None ) :      grad = f . gradient if x not in grad . domain :          raise TypeError ( <str> <str> . format ( x , grad . domain ) )  if not callable ( line_search ) :          line_search = ConstantLineSearch ( line_search )  grad_x = grad . range . element ( ) for _ in range ( maxiter ) :          grad ( x , out = grad_x ) dir_derivative = - grad_x . norm ( ) ** 2 if np . abs ( dir_derivative ) < tol :              return  step = line_search ( x , - grad_x , dir_derivative ) x . lincomb ( 1 , x , - step , grad_x ) if projection is not None :              projection ( x )  if callback is not None :              callback ( x )    def adam ( f , x , learning_rate = 1e-3 , beta1 = 0.9 , beta2 = 0.999 , eps = 1e-8 , maxiter = 1000 , tol = 1e-16 , callback = None ) :      grad = f . gradient if x not in grad . domain :          raise TypeError ( <str> <str> . format ( x , grad . domain ) )  m = grad . domain . zero ( ) v = grad . domain . zero ( ) grad_x = grad . range . element ( ) for _ in range ( maxiter ) :          grad ( x , out = grad_x ) if grad_x . norm ( ) < tol :              return  m . lincomb ( beta1 , m , 1 - beta1 , grad_x ) v . lincomb ( beta2 , v , 1 - beta2 , grad_x ** 2 ) step = learning_rate * np . sqrt ( 1 - beta2 ) / ( 1 - beta1 ) x . lincomb ( 1 , x , - step , m / ( np . sqrt ( v ) + eps ) ) if callback is not None :              callback ( x )    if __name__ == <str> :      from odl . util . testutils import run_doctests run_doctests ( )   