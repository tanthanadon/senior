from __future__ import print_function import numpy as np import torch import odl from odl . contrib import torch as odl_torch matrix = np . array ( [ [ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] ] , dtype = float ) odl_op = odl . MatrixOperator ( matrix ) op_func = odl_torch . OperatorAsAutogradFunction ( odl_op ) x = torch . DoubleTensor ( [ 1 , 1 , 1 ] ) x_var = torch . autograd . Variable ( x , requires_grad = True ) res_var = op_func ( x_var ) odl_res = odl_op ( x . numpy ( ) ) print ( <str> , res_var . data . numpy ( ) ) print ( <str> , odl_res . asarray ( ) ) odl_cost = odl . solvers . L2NormSquared ( odl_op . range ) functional_cost = odl_torch . OperatorAsAutogradFunction ( odl_cost ) res_var = functional_cost ( op_func ( x_var ) ) res_var . backward ( ) odl_grad = ( odl_cost * odl_op ) . gradient ( x . numpy ( ) ) print ( <str> , x_var . grad . data . numpy ( ) ) print ( <str> , odl_grad . asarray ( ) ) dom = odl . uniform_discr ( 0 , 6 , 3 ) odl_op = odl . MatrixOperator ( matrix , domain = dom ) odl_cost = odl . solvers . L2NormSquared ( odl_op . range ) odl_functional = odl_cost * odl_op op_func = odl_torch . OperatorAsAutogradFunction ( odl_op ) functional_cost = odl_torch . OperatorAsAutogradFunction ( odl_cost ) x = torch . ones ( ( 3 , ) ) . type ( torch . DoubleTensor ) x_var = torch . autograd . Variable ( x , requires_grad = True ) y_var = op_func ( x_var ) res_var = functional_cost ( y_var ) res_var . backward ( ) print ( <str> , x_var . grad . data . numpy ( ) ) print ( <str> , odl_functional . gradient ( x . numpy ( ) ) . asarray ( ) )  