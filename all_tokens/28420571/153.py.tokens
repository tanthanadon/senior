from __future__ import print_function , division , absolute_import import numpy as np from odl . discr . lp_discr import DiscreteLp from odl . operator . tensor_ops import PointwiseTensorFieldOperator from odl . space import ProductSpace from odl . util import writable_array , signature_string , indent __all__ = ( <str> , <str> , <str> , <str> ) _SUPPORTED_DIFF_METHODS = ( <str> , <str> , <str> ) _SUPPORTED_PAD_MODES = ( <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> , <str> ) _ADJ_METHOD = { <str> : <str> , <str> : <str> , <str> : <str> } _ADJ_PADDING = { <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> , <str> : <str> } class PartialDerivative ( PointwiseTensorFieldOperator ) :      def __init__ ( self , domain , axis , range = None , method = <str> , pad_mode = <str> , pad_const = 0 ) :          if not isinstance ( domain , DiscreteLp ) :              raise TypeError ( <str> <str> . format ( domain ) )  if range is None :              range = domain  linear = not ( pad_mode == <str> and pad_const != 0 ) super ( PartialDerivative , self ) . __init__ ( domain , range , base_space = domain , linear = linear ) self . axis = int ( axis ) self . dx = self . domain . cell_sides [ axis ] self . method , method_in = str ( method ) . lower ( ) , method if method not in _SUPPORTED_DIFF_METHODS :              raise ValueError ( <str> <str> . format ( method_in ) )  self . pad_mode , pad_mode_in = str ( pad_mode ) . lower ( ) , pad_mode if pad_mode not in _SUPPORTED_PAD_MODES :              raise ValueError ( <str> <str> . format ( pad_mode_in ) )  self . pad_const = self . domain . field . element ( pad_const )  def _call ( self , x , out = None ) :          if out is None :              out = self . range . element ( )  with writable_array ( out ) as out_arr :              finite_diff ( x . asarray ( ) , axis = self . axis , dx = self . dx , method = self . method , pad_mode = self . pad_mode , pad_const = self . pad_const , out = out_arr )  return out  def derivative ( self , point = None ) :          if self . pad_mode == <str> and self . pad_const != 0 :              return PartialDerivative ( self . domain , self . axis , self . range , self . method , self . pad_mode , 0 )  else :              return self   @ property def adjoint ( self ) :          if not self . is_linear :              raise ValueError ( <str> <str> <str> . format ( self . pad_const ) )  return - PartialDerivative ( self . range , self . axis , self . domain , _ADJ_METHOD [ self . method ] , _ADJ_PADDING [ self . pad_mode ] , self . pad_const )  def __repr__ ( self ) :          posargs = [ self . domain ] optargs = [ ( <str> , self . axis , None ) , ( <str> , self . range , self . domain ) , ( <str> , self . method , <str> ) , ( <str> , self . pad_mode , <str> ) , ( <str> , self . pad_const , 0 ) ] inner_str = signature_string ( posargs , optargs , sep = <str> , mod = [ <str> , <str> ] ) return <str> . format ( self . __class__ . __name__ , indent ( inner_str ) )  def __str__ ( self ) :          dom_ran_str = <str> . join ( [ repr ( self . domain ) , repr ( self . range ) ] ) return <str> . format ( self . __class__ . __name__ , indent ( dom_ran_str ) )   class Gradient ( PointwiseTensorFieldOperator ) :      def __init__ ( self , domain = None , range = None , method = <str> , pad_mode = <str> , pad_const = 0 ) :          if domain is None and range is None :              raise ValueError ( <str> )  if domain is None :              try :                  domain = range [ 0 ]  except TypeError :                  pass   if range is None :              range = ProductSpace ( domain , domain . ndim )  if not isinstance ( range , ProductSpace ) :              raise TypeError ( <str> <str> . format ( range ) )  elif not range . is_power_space :              raise ValueError ( <str> <str> . format ( range ) )  if not isinstance ( domain , DiscreteLp ) :              raise TypeError ( <str> <str> . format ( domain ) )  if len ( range ) != domain . ndim :              raise ValueError ( <str> <str> <str> . format ( domain . ndim , len ( range ) ) )  linear = not ( pad_mode == <str> and pad_const != 0 ) super ( Gradient , self ) . __init__ ( domain , range , base_space = domain , linear = linear ) self . method , method_in = str ( method ) . lower ( ) , method if method not in _SUPPORTED_DIFF_METHODS :              raise ValueError ( <str> <str> . format ( method_in ) )  self . pad_mode , pad_mode_in = str ( pad_mode ) . lower ( ) , pad_mode if pad_mode not in _SUPPORTED_PAD_MODES :              raise ValueError ( <str> <str> . format ( pad_mode_in ) )  self . pad_const = domain . field . element ( pad_const )  def _call ( self , x , out = None ) :          if out is None :              out = self . range . element ( )  x_arr = x . asarray ( ) ndim = self . domain . ndim dx = self . domain . cell_sides for axis in range ( ndim ) :              with writable_array ( out [ axis ] ) as out_arr :                  finite_diff ( x_arr , axis = axis , dx = dx [ axis ] , method = self . method , pad_mode = self . pad_mode , pad_const = self . pad_const , out = out_arr )   return out  def derivative ( self , point = None ) :          if self . pad_mode == <str> and self . pad_const != 0 :              return Gradient ( self . domain , self . range , self . method , pad_mode = self . pad_mode , pad_const = 0 )  else :              return self   @ property def adjoint ( self ) :          if not self . is_linear :              raise ValueError ( <str> <str> <str> . format ( self . pad_const ) )  return - Divergence ( domain = self . range , range = self . domain , method = _ADJ_METHOD [ self . method ] , pad_mode = _ADJ_PADDING [ self . pad_mode ] , pad_const = self . pad_const )  def __repr__ ( self ) :          posargs = [ self . domain ] optargs = [ ( <str> , self . range , self . domain ** self . domain . ndim ) , ( <str> , self . method , <str> ) , ( <str> , self . pad_mode , <str> ) , ( <str> , self . pad_const , 0 ) ] inner_str = signature_string ( posargs , optargs , sep = [ <str> , <str> , <str> ] , mod = [ <str> , <str> ] ) return <str> . format ( self . __class__ . __name__ , indent ( inner_str ) )  def __str__ ( self ) :          dom_ran_str = <str> . join ( [ repr ( self . domain ) , repr ( self . range ) ] ) return <str> . format ( self . __class__ . __name__ , indent ( dom_ran_str ) )   class Divergence ( PointwiseTensorFieldOperator ) :      def __init__ ( self , domain = None , range = None , method = <str> , pad_mode = <str> , pad_const = 0 ) :          if domain is None and range is None :              raise ValueError ( <str> )  if domain is None :              domain = ProductSpace ( range , range . ndim )  if range is None :              try :                  range = domain [ 0 ]  except TypeError :                  pass   if not isinstance ( domain , ProductSpace ) :              raise TypeError ( <str> <str> . format ( domain ) )  elif not domain . is_power_space :              raise ValueError ( <str> <str> . format ( domain ) )  if not isinstance ( range , DiscreteLp ) :              raise TypeError ( <str> <str> . format ( range ) )  if len ( domain ) != range . ndim :              raise ValueError ( <str> <str> <str> . format ( range . ndim , len ( domain ) ) )  linear = not ( pad_mode == <str> and pad_const != 0 ) super ( Divergence , self ) . __init__ ( domain , range , base_space = range , linear = linear ) self . method , method_in = str ( method ) . lower ( ) , method if method not in _SUPPORTED_DIFF_METHODS :              raise ValueError ( <str> <str> . format ( method_in ) )  self . pad_mode , pad_mode_in = str ( pad_mode ) . lower ( ) , pad_mode if pad_mode not in _SUPPORTED_PAD_MODES :              raise ValueError ( <str> <str> . format ( pad_mode_in ) )  self . pad_const = range . field . element ( pad_const )  def _call ( self , x , out = None ) :          if out is None :              out = self . range . element ( )  ndim = self . range . ndim dx = self . range . cell_sides tmp = np . empty ( out . shape , out . dtype , order = out . space . default_order ) with writable_array ( out ) as out_arr :              for axis in range ( ndim ) :                  finite_diff ( x [ axis ] , axis = axis , dx = dx [ axis ] , method = self . method , pad_mode = self . pad_mode , pad_const = self . pad_const , out = tmp ) if axis == 0 :                      out_arr [ : ] = tmp  else :                      out_arr += tmp    return out  def derivative ( self , point = None ) :          if self . pad_mode == <str> and self . pad_const != 0 :              return Divergence ( self . domain , self . range , self . method , pad_mode = self . pad_mode , pad_const = 0 )  else :              return self   @ property def adjoint ( self ) :          if not self . is_linear :              raise ValueError ( <str> <str> <str> . format ( self . pad_const ) )  return - Gradient ( self . range , self . domain , method = _ADJ_METHOD [ self . method ] , pad_mode = _ADJ_PADDING [ self . pad_mode ] )  def __repr__ ( self ) :          posargs = [ self . domain ] optargs = [ ( <str> , self . range , self . domain [ 0 ] ) , ( <str> , self . method , <str> ) , ( <str> , self . pad_mode , <str> ) , ( <str> , self . pad_const , 0 ) ] inner_str = signature_string ( posargs , optargs , sep = [ <str> , <str> , <str> ] , mod = [ <str> , <str> ] ) return <str> . format ( self . __class__ . __name__ , indent ( inner_str ) )  def __str__ ( self ) :          dom_ran_str = <str> . join ( [ repr ( self . domain ) , repr ( self . range ) ] ) return <str> . format ( self . __class__ . __name__ , indent ( dom_ran_str ) )   class Laplacian ( PointwiseTensorFieldOperator ) :      def __init__ ( self , domain , range = None , pad_mode = <str> , pad_const = 0 ) :          if not isinstance ( domain , DiscreteLp ) :              raise TypeError ( <str> <str> . format ( domain ) )  if range is None :              range = domain  super ( Laplacian , self ) . __init__ ( domain , range , base_space = domain , linear = True ) self . pad_mode , pad_mode_in = str ( pad_mode ) . lower ( ) , pad_mode if pad_mode not in _SUPPORTED_PAD_MODES :              raise ValueError ( <str> <str> . format ( pad_mode_in ) )  if pad_mode in ( <str> , <str> , <str> , <str> ) :              raise ValueError ( <str> <str> . format ( pad_mode_in ) )  self . pad_const = self . domain . field . element ( pad_const )  def _call ( self , x , out = None ) :          if out is None :              out = self . range . zero ( )  else :              out . set_zero ( )  x_arr = x . asarray ( ) out_arr = out . asarray ( ) tmp = np . empty ( out . shape , out . dtype , order = out . space . default_order ) ndim = self . domain . ndim dx = self . domain . cell_sides with writable_array ( out ) as out_arr :              for axis in range ( ndim ) :                  finite_diff ( x_arr , axis = axis , dx = dx [ axis ] ** 2 , method = <str> , pad_mode = self . pad_mode , pad_const = self . pad_const , out = tmp ) out_arr += tmp finite_diff ( x_arr , axis = axis , dx = dx [ axis ] ** 2 , method = <str> , pad_mode = self . pad_mode , pad_const = self . pad_const , out = tmp ) out_arr -= tmp   return out  def derivative ( self , point = None ) :          if self . pad_mode == <str> and self . pad_const != 0 :              return Laplacian ( self . domain , self . range , pad_mode = self . pad_mode , pad_const = 0 )  else :              return self   @ property def adjoint ( self ) :          return Laplacian ( self . range , self . domain , pad_mode = self . pad_mode , pad_const = 0 )  def __repr__ ( self ) :          posargs = [ self . domain ] optargs = [ ( <str> , self . range , self . domain ** self . domain . ndim ) , ( <str> , self . pad_mode , <str> ) , ( <str> , self . pad_const , 0 ) ] inner_str = signature_string ( posargs , optargs , sep = [ <str> , <str> , <str> ] , mod = [ <str> , <str> ] ) return <str> . format ( self . __class__ . __name__ , indent ( inner_str ) )  def __str__ ( self ) :          dom_ran_str = <str> . join ( [ repr ( self . domain ) , repr ( self . range ) ] ) return <str> . format ( self . __class__ . __name__ , indent ( dom_ran_str ) )   def finite_diff ( f , axis , dx = 1.0 , method = <str> , out = None , ** kwargs ) :      f_arr = np . asarray ( f ) ndim = f_arr . ndim if f_arr . shape [ axis ] < 2 :          raise ValueError ( <str> <str> . format ( axis , f_arr . shape [ axis ] ) )  if axis < 0 :          axis += ndim  if not ( 0 <= axis < ndim ) :          raise IndexError ( <str> <str> . format ( axis , ndim - 1 ) )  dx , dx_in = float ( dx ) , dx if dx <= 0 or not np . isfinite ( dx ) :          raise ValueError ( <str> . format ( dx_in ) )  method , method_in = str ( method ) . lower ( ) , method if method not in _SUPPORTED_DIFF_METHODS :          raise ValueError ( <str> . format ( method_in ) )  pad_mode = kwargs . pop ( <str> , <str> ) if pad_mode not in _SUPPORTED_PAD_MODES :          raise ValueError ( <str> <str> . format ( pad_mode ) )  pad_const = kwargs . pop ( <str> , 0 ) pad_const = f . dtype . type ( pad_const ) if out is None :          out = np . empty_like ( f_arr )  else :          if out . shape != f . shape :              raise ValueError ( <str> <str> . format ( f . shape , out . shape ) )   if f_arr . shape [ axis ] < 2 and pad_mode == <str> :          raise ValueError ( <str> <str> . format ( axis ) )  if f_arr . shape [ axis ] < 3 and pad_mode == <str> :          raise ValueError ( <str> <str> . format ( axis ) )  if kwargs :          raise ValueError ( <str> . format ( kwargs ) )  out , out_in = np . swapaxes ( out , 0 , axis ) , out f_arr = np . swapaxes ( f_arr , 0 , axis ) if method == <str> :          np . subtract ( f_arr [ 2 : ] , f_arr [ : - 2 ] , out = out [ 1 : - 1 ] ) out [ 1 : - 1 ] /= 2.0  elif method == <str> :          np . subtract ( f_arr [ 2 : ] , f_arr [ 1 : - 1 ] , out = out [ 1 : - 1 ] )  elif method == <str> :          np . subtract ( f_arr [ 1 : - 1 ] , f_arr [ : - 2 ] , out = out [ 1 : - 1 ] )  if pad_mode == <str> :          if method == <str> :              out [ 0 ] = ( f_arr [ 1 ] - pad_const ) / 2.0 out [ - 1 ] = ( pad_const - f_arr [ - 2 ] ) / 2.0  elif method == <str> :              out [ 0 ] = f_arr [ 1 ] - f_arr [ 0 ] out [ - 1 ] = pad_const - f_arr [ - 1 ]  elif method == <str> :              out [ 0 ] = f_arr [ 0 ] - pad_const out [ - 1 ] = f_arr [ - 1 ] - f_arr [ - 2 ]   elif pad_mode == <str> :          if method == <str> :              out [ 0 ] = ( f_arr [ 1 ] - f_arr [ 0 ] ) / 2.0 out [ - 1 ] = ( f_arr [ - 1 ] - f_arr [ - 2 ] ) / 2.0  elif method == <str> :              out [ 0 ] = f_arr [ 1 ] - f_arr [ 0 ] out [ - 1 ] = 0  elif method == <str> :              out [ 0 ] = 0 out [ - 1 ] = f_arr [ - 1 ] - f_arr [ - 2 ]   elif pad_mode == <str> :          if method == <str> :              out [ 0 ] = ( f_arr [ 1 ] + f_arr [ 0 ] ) / 2.0 out [ - 1 ] = ( - f_arr [ - 1 ] - f_arr [ - 2 ] ) / 2.0  elif method == <str> :              out [ 0 ] = f_arr [ 1 ] out [ - 1 ] = - f_arr [ - 1 ]  elif method == <str> :              out [ 0 ] = f_arr [ 0 ] out [ - 1 ] = - f_arr [ - 2 ]   elif pad_mode == <str> :          if method == <str> :              out [ 0 ] = ( f_arr [ 1 ] - f_arr [ - 1 ] ) / 2.0 out [ - 1 ] = ( f_arr [ 0 ] - f_arr [ - 2 ] ) / 2.0  elif method == <str> :              out [ 0 ] = f_arr [ 1 ] - f_arr [ 0 ] out [ - 1 ] = f_arr [ 0 ] - f_arr [ - 1 ]  elif method == <str> :              out [ 0 ] = f_arr [ 0 ] - f_arr [ - 1 ] out [ - 1 ] = f_arr [ - 1 ] - f_arr [ - 2 ]   elif pad_mode == <str> :          if method == <str> :              out [ 0 ] = ( f_arr [ 1 ] - f_arr [ 0 ] ) / 2.0 out [ - 1 ] = ( f_arr [ - 1 ] - f_arr [ - 2 ] ) / 2.0  elif method == <str> :              out [ 0 ] = f_arr [ 1 ] - f_arr [ 0 ] out [ - 1 ] = 0  elif method == <str> :              out [ 0 ] = 0 out [ - 1 ] = f_arr [ - 1 ] - f_arr [ - 2 ]   elif pad_mode == <str> :          if method == <str> :              out [ 0 ] = ( f_arr [ 0 ] + f_arr [ 1 ] ) / 2.0 out [ - 1 ] = - ( f_arr [ - 1 ] + f_arr [ - 2 ] ) / 2.0  elif method == <str> :              out [ 0 ] = f_arr [ 1 ] out [ - 1 ] = - f_arr [ - 1 ]  elif method == <str> :              out [ 0 ] = f_arr [ 0 ] out [ - 1 ] = - f_arr [ - 2 ]   elif pad_mode == <str> :          out [ 0 ] = f_arr [ 1 ] - f_arr [ 0 ] out [ - 1 ] = f_arr [ - 1 ] - f_arr [ - 2 ]  elif pad_mode == <str> :          if method == <str> :              out [ 0 ] = f_arr [ 0 ] + f_arr [ 1 ] / 2.0 out [ - 1 ] = - f_arr [ - 1 ] - f_arr [ - 2 ] / 2.0 out [ 1 ] -= f_arr [ 0 ] / 2.0 out [ - 2 ] += f_arr [ - 1 ] / 2.0  elif method == <str> :              out [ 0 ] = f_arr [ 0 ] + f_arr [ 1 ] out [ - 1 ] = - f_arr [ - 1 ] out [ 1 ] -= f_arr [ 0 ]  elif method == <str> :              out [ 0 ] = f_arr [ 0 ] out [ - 1 ] = - f_arr [ - 1 ] - f_arr [ - 2 ] out [ - 2 ] += f_arr [ - 1 ]   elif pad_mode == <str> :          out [ 0 ] = - ( 3.0 * f_arr [ 0 ] - 4.0 * f_arr [ 1 ] + f_arr [ 2 ] ) / 2.0 out [ - 1 ] = ( 3.0 * f_arr [ - 1 ] - 4.0 * f_arr [ - 2 ] + f_arr [ - 3 ] ) / 2.0  elif pad_mode == <str> :          if method == <str> :              out [ 0 ] = 1.5 * f_arr [ 0 ] + 0.5 * f_arr [ 1 ] out [ - 1 ] = - 1.5 * f_arr [ - 1 ] - 0.5 * f_arr [ - 2 ] out [ 1 ] -= 1.5 * f_arr [ 0 ] out [ 2 ] += 0.5 * f_arr [ 0 ] out [ - 3 ] -= 0.5 * f_arr [ - 1 ] out [ - 2 ] += 1.5 * f_arr [ - 1 ]  elif method == <str> :              out [ 0 ] = 1.5 * f_arr [ 0 ] + 1.0 * f_arr [ 1 ] out [ - 1 ] = - 1.5 * f_arr [ - 1 ] out [ 1 ] -= 2.0 * f_arr [ 0 ] out [ 2 ] += 0.5 * f_arr [ 0 ] out [ - 3 ] -= 0.5 * f_arr [ - 1 ] out [ - 2 ] += 1.0 * f_arr [ - 1 ]  elif method == <str> :              out [ 0 ] = 1.5 * f_arr [ 0 ] out [ - 1 ] = - 1.0 * f_arr [ - 2 ] - 1.5 * f_arr [ - 1 ] out [ 1 ] -= 1.0 * f_arr [ 0 ] out [ 2 ] += 0.5 * f_arr [ 0 ] out [ - 3 ] -= 0.5 * f_arr [ - 1 ] out [ - 2 ] += 2.0 * f_arr [ - 1 ]   else :          raise NotImplementedError ( <str> )  out /= dx return out_in  if __name__ == <str> :      from odl . util . testutils import run_doctests run_doctests ( )   